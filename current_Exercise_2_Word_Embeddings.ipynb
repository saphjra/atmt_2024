{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saphjra/atmt_2024/blob/main/current_Exercise_2_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gALCagMSOnRz"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start with this cell to load data and skip training\n",
        "[link text](https://)\n"
      ],
      "metadata": {
        "id": "uZT6c2e-W7wL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Source: [link](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)\n",
        "\n",
        "# Word Embeddings: Encoding Lexical Semantics\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your\n",
        "vocabulary. In NLP, it is almost always the case that your features are\n",
        "words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what\n",
        "the word *is*, it doesn't say much about what it *means* (you might be\n",
        "able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you\n",
        "combine these representations? We often want dense outputs from our\n",
        "neural networks, where the inputs are $|V|$ dimensional, where\n",
        "$V$ is our vocabulary, but often the outputs are only a few\n",
        "dimensional (if we are only predicting a handful of labels, for\n",
        "instance). How do we get from a massive dimensional space to a smaller\n",
        "dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding?\n",
        "That is, we represent the word $w$ by\n",
        "\n",
        "\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n",
        "\n",
        "where the 1 is in a location unique to $w$. Any other word will\n",
        "have a 1 in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how\n",
        "huge it is. It basically treats all words as independent entities with\n",
        "no relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let's see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "* The mathematician ran to the store.\n",
        "* The physicist ran to the store.\n",
        "* The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before\n",
        "seen in our training data:\n",
        "\n",
        "* The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn't it be much\n",
        "better if we could use the following two facts:\n",
        "\n",
        "* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n",
        "  have a semantic relation.\n",
        "* We have seen mathematician in the same role  in this new unseen sentence\n",
        "  as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen\n",
        "sentence? This is what we mean by a notion of similarity: we mean\n",
        "*semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic\n",
        "data, by connecting the dots between what we have seen and what we\n",
        "haven't. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each\n",
        "other semantically. This is called the `distributional\n",
        "hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n",
        "\n",
        "\n",
        "# Getting Dense Word Embeddings\n",
        "\n",
        "How can we solve this problem? That is, how could we actually encode\n",
        "semantic similarity in words? Maybe we think up some semantic\n",
        "attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \"is able to\n",
        "run\" semantic attribute. Think of some other attributes, and imagine\n",
        "what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector,\n",
        "like this:\n",
        "\n",
        "\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
        "   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
        "   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        "\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
        "   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors. That way,\n",
        "extremely similar words (words whose embeddings point in the same\n",
        "direction) will have similarity 1. Extremely dissimilar words should\n",
        "have similarity -1.\n",
        "\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this\n",
        "section as a special case of these new vectors we have defined, where\n",
        "each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their\n",
        "entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of\n",
        "different semantic attributes that might be relevant to determining\n",
        "similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural\n",
        "network learns representations of the features, rather than requiring\n",
        "the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during\n",
        "training? This is exactly what we will do. We will have some *latent\n",
        "semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is,\n",
        "although with our hand-crafted vectors above we can see that\n",
        "mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both\n",
        "mathematicians and physicists have a large value in the second\n",
        "dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to\n",
        "us.\n",
        "\n",
        "\n",
        "In summary, **word embeddings are a representation of the *semantics* of\n",
        "a word, efficiently encoding semantic information that might be relevant\n",
        "to the task at hand**. You can embed other things too: part of speech\n",
        "tags, parse trees, anything! The idea of feature embeddings is central\n",
        "to the field.\n",
        "\n",
        "\n",
        "# Word Embeddings in Pytorch\n",
        "\n",
        "Before we get to a worked example and an exercise, a few quick notes\n",
        "about how to use embeddings in Pytorch and in deep learning programming\n",
        "in general. Similar to how we defined a unique index for each word when\n",
        "making one-hot vectors, we also need to define an index for each word\n",
        "when using embeddings. These will be keys into a lookup table. That is,\n",
        "embeddings are stored as a $|V| \\times D$ matrix, where $D$\n",
        "is the dimensionality of the embeddings, such that the word assigned\n",
        "index $i$ has its embedding stored in the $i$'th row of the\n",
        "matrix. In all of my code, the mapping from words to indices is a\n",
        "dictionary named word\\_to\\_ix.\n",
        "\n",
        "The module that allows you to use embeddings is torch.nn.Embedding,\n",
        "which takes two arguments: the vocabulary size, and the dimensionality\n",
        "of the embeddings.\n",
        "\n",
        "To index into this table, you must use torch.LongTensor (since the\n",
        "indices are integers, not floats).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jgHDDrKlO35U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Q2GVH0OwvH",
        "outputId": "567aa46a-d697-4a51-8cfc-ad945b7aa3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d1333fe4e90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An Example: N-Gram Language Modeling\n",
        "\n",
        "Recall that in an n-gram language model, given a sequence of words\n",
        "$w$, we want to compute\n",
        "\n",
        "\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n",
        "\n",
        "Where $w_i$ is the ith word of the sequence.\n",
        "\n",
        "In this example, we will compute the loss function on some training\n",
        "examples and update the parameters with backpropagation."
      ],
      "metadata": {
        "id": "l3ecRemdPstX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typcially, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an\n",
        "$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n",
        "and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n",
        "collectively as $C$, CBOW tries to minimize\n",
        "\n",
        "\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n"
      ],
      "metadata": {
        "id": "7fV7zUnlz7xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise Layout\n",
        "### 1. <u>Training CBOW Embeddings</u>\n",
        "1.1) Implement a CBOW Model by completing ```class CBOW(nn.Module)``` and train it on ```raw_text```.    \n",
        "\n",
        "1.2) Load Datasets ```tripadvisor_hotel_reviews_reduced.csv``` and ```scifi_reduced.txt```.     \n",
        "\n",
        "1.3) Decide preprocessing steps by completing the function ```def custom_preprocess()```. Describe your decisions. Note that it's your choice to create different preprocessing functions for hotel reviews and scifi datasets or use the same preprocessing function.             \n",
        "\n",
        "1.4) Train CBOW2 with a context width of 2 (in both directions) for the Hotel Reviews dataset.   \n",
        "\n",
        "1.5) Train CBOW5 with a context width of 5 (in both directions) for the Hotel Reviews dataset. Are predictions made by the model sensitive towards the context size?\n",
        "     \n",
        "1.6) Train CBOW2 with a context width of 2 (in both directions) for the Sci-Fi story dataset.  \n",
        "\n",
        "\n",
        "### 2. <u>Test your Embeddings</u>\n",
        "Note - Do the following for CBOW2, and optionally for CBOW5\n",
        "\n",
        "2.1) For the hotel reviews dataset, choose 3 nouns, 3 verbs, and 3 adjectives. Make sure that some nouns/verbs/adjectives occur frequently in the corpus and that others are rare. For each of the 9 chosen words, retrieve the 5 closest words according to your trained CBOW2 model. List them in your report and comment on the performance of your model: do the neighbours the model provides make sense? Discuss.   \n",
        "\n",
        "2.2) Do the same for Sci-Fi dataset.   \n",
        "\n",
        "2.3) How does the quality of the hotel review-based embeddings compare with the Sci-fi-based embeddings? Elaborate.   \n",
        "\n",
        "2.4) Choose 2 words and retrieve their 5 closest neighbours according to hotel review-based embeddings and the Sci-fi-based embeddings. Do they have different neighbours? If yes, can you reason why?    \n",
        "\n",
        "2.5) What are the differences between CBOW2 and CBOW5 ? Can you \"describe\" them?   \n"
      ],
      "metadata": {
        "id": "Pj_2_qqM7Md_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tips\n",
        "\n",
        "1. Switch from CPU to a GPU instance after you have confirmed that your training procedure is working correctly.\n",
        "2. You can always save your intermediate results (embeddings, preprocessed dataset, model, etc.) in your google drive via colab\n"
      ],
      "metadata": {
        "id": "Xvo1QvEf-iT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.1 Create a CBOW Model by completing ```class CBOW(nn.Module)``` and test it on ```raw_text```\n",
        "Implement CBOW in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "* Think about which parameters you need to define.\n",
        "* Make sure you know what shape each operation expects. Use .view() if you need to\n",
        "  reshape."
      ],
      "metadata": {
        "id": "vfG8j2JiRMly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load Datasets"
      ],
      "metadata": {
        "id": "9NbVpFkR77uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Load Datasets tripadvisor_hotel_reviews_reduced.csv and scifi_reduced.txt\n",
        "\n",
        "!gdown 1foE1JuZJeu5E_4qVge9kExzhvF32teuF # For Hotel Reviews\n",
        "!gdown 13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75 # For Scifi-Text"
      ],
      "metadata": {
        "id": "i4XpJnxV79rK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e04c213-999b-4e9b-8261-e7afcdfe10f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1foE1JuZJeu5E_4qVge9kExzhvF32teuF\n",
            "To: /content/tripadvisor_hotel_reviews_reduced.csv\n",
            "100% 7.36M/7.36M [00:00<00:00, 45.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75\n",
            "To: /content/scifi_reduced.txt\n",
            "100% 43.1M/43.1M [00:00<00:00, 95.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Preprocess Datasets\n",
        "### ðŸ—’â“ Describe your decisions for preprocessing the datasets"
      ],
      "metadata": {
        "id": "F9AUsLd78JVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('tripadvisor_hotel_reviews_reduced.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q8fBwCdGAvrd",
        "outputId": "cc073464-646c-4751-f4bb-ab60d6cce64f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Rating\n",
              "0  fantastic service large hotel caters business ...       5\n",
              "1  great hotel modern hotel good location, locate...       4\n",
              "2  3 star plus glasgowjust got 30th november 4 da...       4\n",
              "3  nice stayed hotel nov 19-23. great little bout...       4\n",
              "4  great place wonderful hotel ideally located me...       5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d88ab8a-33ba-48a5-88af-260b996a7c2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fantastic service large hotel caters business ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>great hotel modern hotel good location, locate...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 star plus glasgowjust got 30th november 4 da...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nice stayed hotel nov 19-23. great little bout...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great place wonderful hotel ideally located me...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d88ab8a-33ba-48a5-88af-260b996a7c2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d88ab8a-33ba-48a5-88af-260b996a7c2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d88ab8a-33ba-48a5-88af-260b996a7c2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96ddea61-6f78-4cce-bcb7-4b5365374960\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96ddea61-6f78-4cce-bcb7-4b5365374960')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96ddea61-6f78-4cce-bcb7-4b5365374960 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"enjoyed stay punta cana princess, stayed punta cana princess july 18-25. husband 30 27 went couple late twenties/early thirties, husband inclusive adults resort, really great time things disappointing, 1 desserts- dessert tried buffet ala carte restaurants not good, 2 drinks- reviews read site heard people say drinks not consistently cold agree, order nice tropical drink like pina colada example nice cold slushy like like drinking pina colada juice liquid not cold/icy, think pina colada daiquiri think nice icy cool refreshing drink did not, not happy things actually return punta cana princess, positives resort check in- no problem check, nice greeted cool towel glass yummy tropical punch, rooms got luggage, room- room great stayed 5107. perfect location second floor loved building close pool, room big queen beds little sitting area tv fridge, bathroom nice size loved double sinks great cause husband space ready, ac worked really really actually got pretty cold room day rest time stayed plenty cool turning ac just having ceiling fan, food- thought food really good, caribe buffet twice, night wednesday asian buffet definitely eat liked, ate domnican buffet night monday really good, music playing ate just nice little taste dominican culture, rest nights ate ala carte restaurants, tried il pilon domincan food il bacio italian food la cava fancy retaurant tex mex restaurant forget enjoyed food, la cava nice lobster, came lobster tails really good second overcooked, tex mex restaurant got super hot restaurant sat right near kitchen, husband went talk guest services right away manager came thermostat checking ac, pretty happy took care quickly, meal really good loved margaritas gave guacamole chips, staff- staff incredibly friendly, anamacion staff hostess buffet bartenders met greeted ola, happy help, noticed people reviewed resort said staff unfriendly, wondering just language barrier, not really able speak spanish really tried did feel n't connected staff guests speak fluent spanish, n't think not friendly just think harder connect people not share language, feel like tried talk staff tried talk, ask, beach- beautiful actually did not spend time, did activities beach like ping pong pilates really just liked pool, pool- loved pool, huge nice pool bar day nice palapa sit spot near pool, lots activities pool did aerobicos times little silly fun played pool volleyball, cleanliness- resort maintained daily people work clean debris beach working maintaining gardens resort, overall really enjoyed stay punta cana princess, not happy consistency cold drinks average desserts wonderful nice spacious room good food friendly staff beautiful beach pool,  \",\n          \"captivating stayed dauphine march 2003 mardi gras passed delighted quiet close action french quarter, absolutely gorgeous room street main hotel overlooking captivating courtyard, room just beautiful loved brick walls huge comfortable bed french doors jet tub, felt like princess room clean inviting, hotel exceeded expectations online stay heartbeat, fact fully intend just spring,  \",\n          \"great location hotel calzaiuoli good hotel prime florence location, easy walking distance main attractions blocks duomo uffizi gallery.the breakfast room spacious food offerings average, decor nice, lift relatively large, beds comfortable.there downside hotel, door locks old-fashioned keys fine, inside room no way prevent hotel staff key entering room, no not disturb signs provided, hotel chambermaid entered room twice 4 night stay room,  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Complete the preprocessing function and apply it to the datasets\n",
        "import re\n",
        "def custom_preprocess(df, col):\n",
        "    def tokenize(text):\n",
        "    # Convert to lowercase and split by non-alphabetic characters, very minor proprocessing steps, we could add more. However the dataset, seemed to be already preprocessed\n",
        "      tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "      return tokens\n",
        "\n",
        "\n",
        "    df_preproccessed = df[col].apply(tokenize)\n",
        "\n",
        "    vocab_set = set(token for tokens in df_preproccessed for token in tokens)\n",
        "    return df_preproccessed, vocab_set\n",
        "\n"
      ],
      "metadata": {
        "id": "9AEF16-v9Erc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pre, vocab = custom_preprocess(df, 'Review')\n",
        "df_pre.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-dbrnpWnkUm",
        "outputId": "6687bdf8-ec33-4a23-c342-eb835d436df4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Series name: Review\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "10000 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 78.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab_and_data(df, col, context_size=2):\n",
        "    # By deriving a set from `raw_text`, we deduplicate the array\n",
        "    df, vocab = custom_preprocess(df, col)\n",
        "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "    data = []\n",
        "    for j in range(len(df)):\n",
        "      raw_text = df[j]\n",
        "      # print(raw_text)\n",
        "      for i in range(context_size, len(raw_text) - context_size):\n",
        "          context = raw_text[i - context_size:i] + raw_text[i + 1:i + context_size + 1]\n",
        "          target = raw_text[i]\n",
        "          data.append((context, target))\n",
        "    return data, word_to_ix, vocab\n"
      ],
      "metadata": {
        "id": "TlNwOJRyCJm6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, word_to_ix, vocab = create_vocab_and_data(df, 'Review')\n",
        "print(data[0:5])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3HxW1icrm6R",
        "outputId": "fc6e7643-1a8e-4d65-f4f8-8c3105e263db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['fantastic', 'service', 'hotel', 'caters'], 'large'), (['service', 'large', 'caters', 'business'], 'hotel'), (['large', 'hotel', 'business', 'corporates'], 'caters'), (['hotel', 'caters', 'corporates', 'serve'], 'business'), (['caters', 'business', 'serve', 'provided'], 'corporates')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(int(len(data)*0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqFQFG-34wdk",
        "outputId": "1b273c4f-b0fc-4741-c7e4-1be557b8098d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "946944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, word_to_ix, vocab = create_vocab_and_data(df, 'Review', context_size=5)\n",
        "print(data[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RKZV9Y2zgcD",
        "outputId": "32a4f2fa-bcf7-4e4f-dd4c-b40e0557c198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['fantastic', 'service', 'large', 'hotel', 'caters', 'corporates', 'serve', 'provided', 'better', 'wife'], 'business'), (['service', 'large', 'hotel', 'caters', 'business', 'serve', 'provided', 'better', 'wife', 'experienced'], 'corporates'), (['large', 'hotel', 'caters', 'business', 'corporates', 'provided', 'better', 'wife', 'experienced', 'nothing'], 'serve'), (['hotel', 'caters', 'business', 'corporates', 'serve', 'better', 'wife', 'experienced', 'nothing', 'short'], 'provided'), (['caters', 'business', 'corporates', 'serve', 'provided', 'wife', 'experienced', 'nothing', 'short', 'world'], 'better')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "\n",
        "    idxs = [word_to_ix[w] for w in context] if type(context)==list  else [word_to_ix[context]]\n",
        "\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "print(make_context_vector(data[0][0], word_to_ix))  # example\n",
        "print(make_context_vector(data[0][1], word_to_ix))  # example\n",
        "data[0][1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "QZt2SqCOUqH_",
        "outputId": "1974b6d7-91ff-4826-ec93-c677abaee4b7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14162, 14319, 19780, 30734])\n",
            "tensor([2100])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'large'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_to_tensor(data, word_to_ix):\n",
        "    data_tensor_context = []\n",
        "    data_tensor_target  = []\n",
        "    for context, target in data:\n",
        "        context_idxs = make_context_vector(context, word_to_ix)\n",
        "        target_idx = word_to_ix[target]\n",
        "        data_tensor_context.append(context_idxs)\n",
        "        data_tensor_target.append(target_idx)\n",
        "    data_tensor_context = torch.stack(data_tensor_context)\n",
        "    data_tensor_target = torch.tensor(data_tensor_target, dtype=torch.long)\n",
        "    return data_tensor_context, data_tensor_target.unsqueeze(1)\n"
      ],
      "metadata": {
        "id": "wK_EZmA4WR5b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_to_tensor(data[:10], word_to_ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIw6NA-MWw8Q",
        "outputId": "bb65d748-e56f-41cc-8ae8-8bc3d3090123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[17667, 16493, 18955,  4637],\n",
              "         [16493, 21993,  4637, 20623],\n",
              "         [21993, 18955, 20623, 29567],\n",
              "         [18955,  4637, 29567, 28517],\n",
              "         [ 4637, 20623, 28517, 33122],\n",
              "         [20623, 29567, 33122, 13291],\n",
              "         [29567, 28517, 13291, 21399],\n",
              "         [28517, 33122, 21399, 33994],\n",
              "         [33122, 13291, 33994, 36446],\n",
              "         [13291, 21399, 36446, 32184]]),\n",
              " tensor([[21993],\n",
              "         [18955],\n",
              "         [ 4637],\n",
              "         [20623],\n",
              "         [29567],\n",
              "         [28517],\n",
              "         [33122],\n",
              "         [13291],\n",
              "         [21399],\n",
              "         [33994]]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_index= int(len(data)*0.9)\n",
        "\n",
        "train_X, train_Y = data_to_tensor(data[:split_index], word_to_ix)\n",
        "test_X, test_Y = data_to_tensor(data[split_index:], word_to_ix)\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQxqHaDqZCrq",
        "outputId": "48de27b8-9714-43bc-832e-2410ce85c42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([946944, 4])\n",
            "torch.Size([946944, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_X.shape)\n",
        "print(test_Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyFreSWh3z4T",
        "outputId": "264e52b0-bdd8-43bb-b038-ff1dfbdf27b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([105216, 4])\n",
            "torch.Size([105216, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X4-L2sY38cy",
        "outputId": "fb7eb78a-9402-4ddb-cd9e-283bce177b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1052160"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "class MyDataset(IterableDataset):\n",
        "    def __init__(self, data_X, data_y):\n",
        "        assert len(data_X) == len(data_y)\n",
        "        self.data_X = data_X.to(device)\n",
        "        self.data_y = data_y.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_X)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(len(self.data_X)):\n",
        "            yield (self.data_X[i], self.data_y[i])"
      ],
      "metadata": {
        "id": "Pf9EbocHZwXE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N8At7UQnanrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "train_set = MyDataset(train_X, train_Y)\n",
        "test_set = MyDataset(test_X, test_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xHPAULfZxxd",
        "outputId": "830a58aa-304b-4213-cf37-81f6c418b692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * 2* embedding_dim, hidden_dim)     # multipy with two because you have a left anfd a right ontext\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=-1)\n",
        "        return log_probs\n",
        "\n",
        "# create your model and train.  here are some functions to help you make\n",
        "# the data ready for use by your module\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMOoKGjMkxKX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval(model, test_loader)\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "  model.eval()  # without dropout and alike not really necessary\n",
        "  with torch.no_grad():  # disable gradient computation, since it is only needed when backward() is called\n",
        "      for test_X, test_y in test_loader:\n",
        "          pred_y = model(test_X)\n",
        "          #print(pred_y, test_y)\n",
        "          batch_preds = [x.item() for x in torch.argmax(pred_y, dim=-1)]\n",
        "          predictions.extend(batch_preds)\n",
        "          true_labels.extend([y.item() for y in test_y.squeeze()])\n",
        "\n",
        "\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  acc = accuracy_score(true_labels, predictions)\n",
        "  acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u0CNZkd0hfYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "f973dada-baa0-48e1-e3ac-a775b14ca5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-17.2027, -17.1444, -19.4640,  ..., -18.6364, -18.2556, -19.3207],\n",
            "        [-18.8358, -23.2521, -20.8849,  ..., -22.8250, -22.3099, -19.3059],\n",
            "        [-20.1133, -22.4892, -19.2174,  ..., -21.8385, -20.6431, -20.5586],\n",
            "        ...,\n",
            "        [-17.9041, -19.0858, -16.3741,  ..., -17.3103, -13.8947, -16.8426],\n",
            "        [-23.5336, -19.5310, -20.1699,  ..., -22.0300, -19.6643, -21.8681],\n",
            "        [-20.8341, -16.8419, -18.0029,  ..., -20.2550, -16.8591, -16.8116]]) tensor([[  561],\n",
            "        [ 2606],\n",
            "        [ 9803],\n",
            "        [11225],\n",
            "        [12235],\n",
            "        [28621],\n",
            "        [16350],\n",
            "        [35104],\n",
            "        [34938],\n",
            "        [ 5735],\n",
            "        [35104],\n",
            "        [32594],\n",
            "        [ 9859],\n",
            "        [29580],\n",
            "        [ 9859],\n",
            "        [18670]])\n",
            "tensor([[-17.2301, -18.9866, -15.7950,  ..., -19.0685, -18.4993, -17.7286],\n",
            "        [-21.3038, -20.3481, -20.2360,  ..., -21.5439, -18.8984, -18.5081],\n",
            "        [-21.5107, -23.6949, -20.3636,  ..., -25.1956, -20.6841, -20.8851],\n",
            "        ...,\n",
            "        [-26.5692, -25.8688, -21.7860,  ..., -26.7425, -21.5749, -21.2530],\n",
            "        [-33.1820, -35.9453, -34.7395,  ..., -35.8579, -32.7851, -34.5458],\n",
            "        [-23.7943, -24.2835, -23.9005,  ..., -21.8426, -21.2141, -23.6045]]) tensor([[35918],\n",
            "        [ 5859],\n",
            "        [26952],\n",
            "        [28485],\n",
            "        [ 1806],\n",
            "        [31776],\n",
            "        [23065],\n",
            "        [23300],\n",
            "        [29454],\n",
            "        [14262],\n",
            "        [23546],\n",
            "        [15383],\n",
            "        [26856],\n",
            "        [13425],\n",
            "        [ 4908],\n",
            "        [ 3286]])\n",
            "tensor([[-26.3324, -24.3146, -26.4658,  ..., -26.1656, -26.8559, -22.7325],\n",
            "        [-15.8239, -18.7373, -16.2180,  ..., -20.6841, -19.2474, -16.4867],\n",
            "        [-17.2423, -19.5032, -18.5884,  ..., -19.5073, -18.9972, -19.5036],\n",
            "        ...,\n",
            "        [-17.1554, -18.9040, -17.8423,  ..., -18.9666, -18.2105, -17.2363],\n",
            "        [-19.1561, -21.7216, -18.5702,  ..., -20.1031, -18.9158, -18.2353],\n",
            "        [-19.4818, -19.5138, -20.1333,  ..., -22.6088, -18.5941, -17.9258]]) tensor([[19196],\n",
            "        [26748],\n",
            "        [20868],\n",
            "        [14740],\n",
            "        [25154],\n",
            "        [13096],\n",
            "        [23261],\n",
            "        [ 9816],\n",
            "        [23594],\n",
            "        [ 2434],\n",
            "        [30245],\n",
            "        [ 9471],\n",
            "        [  562],\n",
            "        [23243],\n",
            "        [12199],\n",
            "        [15383]])\n",
            "tensor([[-17.6229, -22.2909, -20.4034,  ..., -21.9857, -17.7484, -17.8712],\n",
            "        [-20.6250, -24.4340, -20.7801,  ..., -26.4907, -23.9264, -22.4730],\n",
            "        [-22.3925, -18.7103, -18.5082,  ..., -19.5571, -19.9310, -19.2794],\n",
            "        ...,\n",
            "        [-25.8892, -25.6040, -23.0853,  ..., -26.1454, -24.8168, -24.5513],\n",
            "        [-23.5950, -24.2492, -24.5875,  ..., -26.8538, -23.0532, -23.9477],\n",
            "        [-24.1877, -26.1196, -25.1068,  ..., -23.5659, -23.9634, -23.3518]]) tensor([[25076],\n",
            "        [ 9440],\n",
            "        [32885],\n",
            "        [20951],\n",
            "        [13783],\n",
            "        [15383],\n",
            "        [ 3655],\n",
            "        [35233],\n",
            "        [ 9551],\n",
            "        [ 2046],\n",
            "        [21089],\n",
            "        [ 9803],\n",
            "        [17746],\n",
            "        [21230],\n",
            "        [ 2410],\n",
            "        [ 2255]])\n",
            "tensor([[-18.2791, -20.7484, -20.9402,  ..., -20.7918, -21.0266, -19.5807],\n",
            "        [-18.1281, -21.4334, -19.5208,  ..., -20.2072, -18.2716, -18.5839],\n",
            "        [-22.8532, -26.6140, -23.8438,  ..., -26.8147, -24.5092, -27.6584],\n",
            "        ...,\n",
            "        [-19.8750, -20.9109, -17.5037,  ..., -19.7200, -18.3764, -21.0418],\n",
            "        [-20.5969, -21.4564, -20.9720,  ..., -24.4619, -21.2328, -22.5991],\n",
            "        [-24.7654, -23.7948, -21.2778,  ..., -24.6817, -19.9780, -22.0027]]) tensor([[33090],\n",
            "        [19927],\n",
            "        [10361],\n",
            "        [35031],\n",
            "        [ 2827],\n",
            "        [16274],\n",
            "        [27786],\n",
            "        [23963],\n",
            "        [26905],\n",
            "        [ 9471],\n",
            "        [12842],\n",
            "        [27515],\n",
            "        [11247],\n",
            "        [10176],\n",
            "        [ 8414],\n",
            "        [29661]])\n",
            "tensor([[-19.1004, -19.9702, -22.6558,  ..., -24.0056, -20.5574, -21.1588],\n",
            "        [-24.2892, -28.6417, -24.2548,  ..., -24.3560, -23.1840, -25.9547],\n",
            "        [-34.4345, -38.0835, -34.8052,  ..., -37.8405, -33.2842, -35.5124],\n",
            "        ...,\n",
            "        [-22.5835, -24.3736, -21.7203,  ..., -24.9726, -20.3513, -22.3681],\n",
            "        [-22.2567, -23.0296, -21.3775,  ..., -25.7455, -22.2453, -20.4405],\n",
            "        [-35.1640, -33.5413, -30.5835,  ..., -36.0650, -34.0554, -31.2722]]) tensor([[33816],\n",
            "        [14259],\n",
            "        [ 4703],\n",
            "        [11857],\n",
            "        [27786],\n",
            "        [ 8124],\n",
            "        [   75],\n",
            "        [20268],\n",
            "        [27515],\n",
            "        [ 8229],\n",
            "        [ 1324],\n",
            "        [20756],\n",
            "        [33694],\n",
            "        [27515],\n",
            "        [31495],\n",
            "        [11004]])\n",
            "tensor([[-19.2575, -19.4003, -21.9340,  ..., -19.9610, -20.9049, -18.1513],\n",
            "        [-22.1195, -22.4237, -21.4756,  ..., -24.1917, -19.4647, -21.2553],\n",
            "        [-35.3997, -36.8291, -32.0793,  ..., -33.9368, -29.3538, -32.2476],\n",
            "        ...,\n",
            "        [-22.9521, -24.4296, -23.5247,  ..., -22.7075, -22.9782, -25.5614],\n",
            "        [-23.3445, -22.2535, -22.9153,  ..., -24.0140, -23.4482, -26.4604],\n",
            "        [-19.5069, -18.0547, -19.4563,  ..., -17.5510, -16.9853, -18.0565]]) tensor([[33971],\n",
            "        [29892],\n",
            "        [24323],\n",
            "        [   45],\n",
            "        [ 4583],\n",
            "        [21230],\n",
            "        [ 7969],\n",
            "        [22789],\n",
            "        [ 8463],\n",
            "        [  995],\n",
            "        [35554],\n",
            "        [31644],\n",
            "        [22704],\n",
            "        [28581],\n",
            "        [ 7026],\n",
            "        [28283]])\n",
            "tensor([[-22.6297, -23.4612, -18.9985,  ..., -24.9793, -20.0165, -20.3425],\n",
            "        [-21.2680, -22.1392, -18.2241,  ..., -21.3942, -19.2890, -18.0914],\n",
            "        [-19.2474, -17.8526, -19.3188,  ..., -20.9212, -20.8945, -20.6722],\n",
            "        ...,\n",
            "        [-21.7609, -21.8010, -24.4927,  ..., -20.6186, -20.4338, -21.0443],\n",
            "        [-18.0430, -18.9382, -17.0625,  ..., -20.4671, -17.6026, -17.1926],\n",
            "        [-26.3262, -25.7117, -24.0248,  ..., -27.7685, -26.0929, -24.5195]]) tensor([[ 9803],\n",
            "        [ 4160],\n",
            "        [21334],\n",
            "        [ 5534],\n",
            "        [23458],\n",
            "        [21904],\n",
            "        [23300],\n",
            "        [24452],\n",
            "        [  484],\n",
            "        [35554],\n",
            "        [ 3669],\n",
            "        [15178],\n",
            "        [31946],\n",
            "        [ 5446],\n",
            "        [ 7187],\n",
            "        [33729]])\n",
            "tensor([[-19.6976, -19.8681, -18.3489,  ..., -18.5800, -16.9173, -18.0310],\n",
            "        [-15.6909, -17.4618, -19.9834,  ..., -19.1071, -16.6782, -18.6847],\n",
            "        [-27.6356, -27.8013, -26.1408,  ..., -27.3137, -25.8566, -25.2905],\n",
            "        ...,\n",
            "        [-25.3042, -26.4333, -26.8866,  ..., -27.6223, -25.8957, -27.5812],\n",
            "        [-18.8226, -16.0940, -15.8509,  ..., -17.3994, -17.5279, -16.7772],\n",
            "        [-19.9279, -22.4050, -20.0987,  ..., -24.1204, -22.7545, -24.5169]]) tensor([[17953],\n",
            "        [27697],\n",
            "        [32587],\n",
            "        [36732],\n",
            "        [32940],\n",
            "        [22038],\n",
            "        [ 1052],\n",
            "        [  949],\n",
            "        [29060],\n",
            "        [17211],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 1731],\n",
            "        [24258],\n",
            "        [  216],\n",
            "        [  736]])\n",
            "tensor([[-18.6300, -19.2143, -17.8496,  ..., -22.3397, -18.2935, -19.7696],\n",
            "        [-19.2529, -21.8235, -23.6906,  ..., -23.6824, -21.2629, -21.8452],\n",
            "        [-22.2109, -24.3573, -23.5004,  ..., -24.5496, -24.6149, -24.1953],\n",
            "        ...,\n",
            "        [-16.5606, -18.8035, -17.9506,  ..., -19.3476, -16.6629, -16.9378],\n",
            "        [-23.0386, -23.7827, -23.9934,  ..., -27.7572, -23.4595, -25.4978],\n",
            "        [-19.0266, -20.5732, -18.3756,  ..., -21.7862, -18.4939, -17.8912]]) tensor([[29580],\n",
            "        [35319],\n",
            "        [18406],\n",
            "        [13830],\n",
            "        [ 1887],\n",
            "        [30766],\n",
            "        [33522],\n",
            "        [ 8148],\n",
            "        [10746],\n",
            "        [ 2305],\n",
            "        [23546],\n",
            "        [13497],\n",
            "        [11902],\n",
            "        [ 7721],\n",
            "        [35161],\n",
            "        [25737]])\n",
            "tensor([[-24.9138, -24.2540, -26.4514,  ..., -28.3147, -25.4941, -24.8454],\n",
            "        [-19.8875, -20.1024, -19.9799,  ..., -21.9443, -22.0993, -21.6311],\n",
            "        [-21.4829, -23.0297, -22.5749,  ..., -22.8978, -20.4090, -23.9752],\n",
            "        ...,\n",
            "        [-14.1964, -18.2073, -14.4081,  ..., -17.8622, -14.9854, -15.6980],\n",
            "        [-22.0607, -23.7851, -25.1094,  ..., -25.6024, -24.2698, -22.8733],\n",
            "        [-22.0120, -21.9693, -21.1386,  ..., -21.5756, -19.0801, -20.5959]]) tensor([[22997],\n",
            "        [23546],\n",
            "        [31352],\n",
            "        [14401],\n",
            "        [31248],\n",
            "        [28607],\n",
            "        [29928],\n",
            "        [ 3815],\n",
            "        [34821],\n",
            "        [36742],\n",
            "        [ 3983],\n",
            "        [11941],\n",
            "        [23367],\n",
            "        [20284],\n",
            "        [ 4614],\n",
            "        [11225]])\n",
            "tensor([[-20.4200, -23.5689, -19.9479,  ..., -23.1440, -18.7452, -17.4966],\n",
            "        [-24.9936, -24.2968, -23.5503,  ..., -27.4952, -22.8696, -22.3878],\n",
            "        [-20.5847, -22.6405, -21.5428,  ..., -24.1972, -21.1068, -20.5307],\n",
            "        ...,\n",
            "        [-17.5521, -19.6467, -16.0862,  ..., -19.7250, -16.8757, -17.1489],\n",
            "        [-19.2801, -18.3465, -17.3856,  ..., -20.7950, -16.4707, -17.8064],\n",
            "        [-16.8597, -19.6645, -16.5676,  ..., -20.7341, -17.3982, -17.9348]]) tensor([[ 5277],\n",
            "        [22411],\n",
            "        [21089],\n",
            "        [22038],\n",
            "        [  887],\n",
            "        [23546],\n",
            "        [36742],\n",
            "        [  887],\n",
            "        [29766],\n",
            "        [17211],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 1731],\n",
            "        [ 1991],\n",
            "        [17211],\n",
            "        [ 6780]])\n",
            "tensor([[-21.7224, -26.6075, -23.3279,  ..., -25.4119, -24.8757, -21.9260],\n",
            "        [-26.7987, -30.5084, -30.0891,  ..., -31.7419, -28.5151, -26.1751],\n",
            "        [-15.8306, -20.9547, -18.1495,  ..., -17.3983, -16.3943, -16.9644],\n",
            "        ...,\n",
            "        [-19.0489, -19.0086, -19.7283,  ..., -20.9243, -20.5740, -19.0892],\n",
            "        [-20.7599, -23.7994, -22.1902,  ..., -23.3875, -24.0510, -19.4003],\n",
            "        [-15.4363, -17.8658, -15.0212,  ..., -16.1025, -17.9512, -18.0699]]) tensor([[ 2085],\n",
            "        [23546],\n",
            "        [ 9802],\n",
            "        [12222],\n",
            "        [16486],\n",
            "        [30344],\n",
            "        [11902],\n",
            "        [32863],\n",
            "        [ 1788],\n",
            "        [29679],\n",
            "        [19153],\n",
            "        [11004],\n",
            "        [22789],\n",
            "        [14401],\n",
            "        [26592],\n",
            "        [24971]])\n",
            "tensor([[-20.9137, -23.0798, -20.6960,  ..., -23.3096, -20.7818, -21.3880],\n",
            "        [-24.9019, -25.4556, -23.5184,  ..., -26.4799, -26.1447, -27.2319],\n",
            "        [-20.4442, -20.3644, -21.0443,  ..., -24.6870, -20.4702, -20.1489],\n",
            "        ...,\n",
            "        [-21.4350, -19.4314, -19.0384,  ..., -18.9285, -18.2570, -17.7777],\n",
            "        [-21.8027, -22.8100, -23.2934,  ..., -20.9924, -21.6359, -22.2035],\n",
            "        [-19.1116, -16.3125, -17.0739,  ..., -18.5740, -17.6864, -17.7268]]) tensor([[13277],\n",
            "        [ 2531],\n",
            "        [30392],\n",
            "        [23156],\n",
            "        [ 4915],\n",
            "        [33486],\n",
            "        [21104],\n",
            "        [11247],\n",
            "        [ 9477],\n",
            "        [10836],\n",
            "        [ 2082],\n",
            "        [ 1887],\n",
            "        [35104],\n",
            "        [11902],\n",
            "        [ 5977],\n",
            "        [12049]])\n",
            "tensor([[-16.7505, -17.5032, -14.5767,  ..., -18.2762, -15.7983, -15.5715],\n",
            "        [-17.4523, -18.5900, -15.8202,  ..., -21.3824, -18.6132, -17.3911],\n",
            "        [-19.9951, -17.0576, -16.9411,  ..., -21.2181, -18.0074, -20.8262],\n",
            "        ...,\n",
            "        [-20.9740, -23.7963, -21.7485,  ..., -22.4416, -21.9554, -21.1660],\n",
            "        [-19.3893, -17.6707, -17.8000,  ..., -17.3829, -19.1814, -19.5451],\n",
            "        [-20.5829, -19.8414, -18.9962,  ..., -22.5318, -18.8832, -18.6656]]) tensor([[ 3669],\n",
            "        [23300],\n",
            "        [26547],\n",
            "        [21089],\n",
            "        [33090],\n",
            "        [12049],\n",
            "        [ 9803],\n",
            "        [23546],\n",
            "        [16350],\n",
            "        [ 1887],\n",
            "        [12713],\n",
            "        [23268],\n",
            "        [11486],\n",
            "        [24422],\n",
            "        [15383],\n",
            "        [23546]])\n",
            "tensor([[-22.1593, -26.7744, -25.1758,  ..., -27.6453, -24.9328, -26.1897],\n",
            "        [-18.3980, -19.8647, -19.6319,  ..., -20.1231, -18.5716, -20.7148],\n",
            "        [-23.2975, -25.5330, -23.0524,  ..., -23.4401, -26.1676, -24.4283],\n",
            "        ...,\n",
            "        [-18.7158, -18.5926, -19.5037,  ..., -21.6056, -20.8220, -18.7950],\n",
            "        [-20.2540, -21.4015, -21.2336,  ..., -19.2910, -20.7013, -22.5297],\n",
            "        [-14.4528, -15.8383, -15.0740,  ..., -15.5031, -15.2736, -15.1575]]) tensor([[13971],\n",
            "        [ 2410],\n",
            "        [ 3585],\n",
            "        [29184],\n",
            "        [15383],\n",
            "        [17413],\n",
            "        [20951],\n",
            "        [21035],\n",
            "        [16350],\n",
            "        [12591],\n",
            "        [ 7955],\n",
            "        [ 6438],\n",
            "        [26400],\n",
            "        [ 1887],\n",
            "        [32671],\n",
            "        [  736]])\n",
            "tensor([[-17.2879, -19.3295, -19.2705,  ..., -22.8865, -19.8423, -20.7449],\n",
            "        [-22.2853, -23.2556, -22.0335,  ..., -22.9904, -16.6772, -19.0929],\n",
            "        [-26.9876, -25.5009, -23.5631,  ..., -25.8219, -23.9818, -23.0117],\n",
            "        ...,\n",
            "        [-18.2114, -21.1750, -18.7173,  ..., -20.4837, -19.5432, -19.8052],\n",
            "        [-19.5534, -22.3735, -22.0873,  ..., -23.7558, -20.8826, -20.0438],\n",
            "        [-16.8616, -16.2175, -14.8309,  ..., -17.3796, -16.4171, -14.6916]]) tensor([[ 1887],\n",
            "        [ 9803],\n",
            "        [17082],\n",
            "        [11004],\n",
            "        [36799],\n",
            "        [20951],\n",
            "        [29760],\n",
            "        [ 3669],\n",
            "        [22836],\n",
            "        [25088],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [26196],\n",
            "        [ 7679],\n",
            "        [11902],\n",
            "        [23458]])\n",
            "tensor([[-23.5954, -24.2033, -21.5887,  ..., -27.1036, -22.4834, -26.9577],\n",
            "        [-16.8693, -20.1188, -20.2933,  ..., -20.0602, -18.2836, -19.6548],\n",
            "        [-21.7204, -22.4173, -18.4988,  ..., -24.3951, -20.6171, -20.8781],\n",
            "        ...,\n",
            "        [-27.9843, -28.8547, -27.7203,  ..., -31.6298, -28.5102, -29.0420],\n",
            "        [-22.1723, -22.6939, -21.5607,  ..., -22.9039, -23.3763, -22.2256],\n",
            "        [-27.0970, -27.0182, -27.0114,  ..., -29.3550, -26.6889, -29.3411]]) tensor([[13101],\n",
            "        [30257],\n",
            "        [ 1887],\n",
            "        [18338],\n",
            "        [26936],\n",
            "        [17211],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [26760],\n",
            "        [ 8148],\n",
            "        [21650],\n",
            "        [26164],\n",
            "        [  503],\n",
            "        [ 2454],\n",
            "        [29580],\n",
            "        [11902]])\n",
            "tensor([[-21.3508, -18.6384, -18.6735,  ..., -19.5576, -20.4837, -20.7528],\n",
            "        [-23.5429, -23.7446, -24.3321,  ..., -24.3455, -22.4877, -26.8327],\n",
            "        [-21.2536, -21.7618, -21.1945,  ..., -27.8646, -21.4730, -20.1848],\n",
            "        ...,\n",
            "        [-21.3882, -21.3474, -21.6232,  ..., -22.1649, -21.5794, -20.7194],\n",
            "        [-17.1551, -20.4575, -18.2199,  ..., -20.2114, -16.7754, -19.9584],\n",
            "        [-22.7596, -26.6874, -22.9271,  ..., -27.0687, -22.0731, -23.1567]]) tensor([[32627],\n",
            "        [33726],\n",
            "        [21604],\n",
            "        [19262],\n",
            "        [30405],\n",
            "        [ 7523],\n",
            "        [33893],\n",
            "        [25868],\n",
            "        [ 8148],\n",
            "        [32632],\n",
            "        [18170],\n",
            "        [  736],\n",
            "        [22571],\n",
            "        [  216],\n",
            "        [ 3747],\n",
            "        [36799]])\n",
            "tensor([[-16.2025, -17.5735, -17.1121,  ..., -17.8862, -15.4005, -17.3697],\n",
            "        [-21.9590, -21.1555, -20.9934,  ..., -23.6716, -20.8932, -21.4087],\n",
            "        [-19.7463, -19.3512, -20.5528,  ..., -19.2271, -18.5344, -17.5274],\n",
            "        ...,\n",
            "        [-21.2876, -20.6299, -22.5441,  ..., -23.4012, -22.6392, -23.7033],\n",
            "        [-17.0080, -20.2274, -20.2929,  ..., -17.8571, -19.8107, -18.9846],\n",
            "        [-16.3311, -17.4350, -16.7211,  ..., -17.4831, -20.3780, -16.1312]]) tensor([[18196],\n",
            "        [13753],\n",
            "        [ 2454],\n",
            "        [ 2255],\n",
            "        [21993],\n",
            "        [  957],\n",
            "        [ 4532],\n",
            "        [31352],\n",
            "        [  957],\n",
            "        [11486],\n",
            "        [23367],\n",
            "        [22492],\n",
            "        [  503],\n",
            "        [ 2368],\n",
            "        [33057],\n",
            "        [ 7955]])\n",
            "tensor([[-21.3445, -21.5875, -21.8303,  ..., -21.2500, -19.1611, -20.9903],\n",
            "        [-21.5593, -18.8949, -19.5410,  ..., -20.7956, -18.1064, -19.8883],\n",
            "        [-21.5223, -24.0163, -20.9244,  ..., -23.8744, -20.8920, -19.3301],\n",
            "        ...,\n",
            "        [-17.5300, -17.0754, -17.1323,  ..., -16.7608, -16.9785, -16.4029],\n",
            "        [-23.9538, -23.3238, -19.8434,  ..., -21.5294, -22.4144, -20.6129],\n",
            "        [-24.0727, -22.4444, -25.0414,  ..., -24.1597, -22.0074, -21.6219]]) tensor([[22571],\n",
            "        [31375],\n",
            "        [23144],\n",
            "        [18737],\n",
            "        [ 4001],\n",
            "        [11374],\n",
            "        [28852],\n",
            "        [14529],\n",
            "        [20750],\n",
            "        [10555],\n",
            "        [ 1887],\n",
            "        [29167],\n",
            "        [ 4160],\n",
            "        [11749],\n",
            "        [33324],\n",
            "        [ 4124]])\n",
            "tensor([[-21.4620, -18.4743, -19.6125,  ..., -20.1520, -21.7207, -20.2356],\n",
            "        [-34.1223, -33.0941, -30.7466,  ..., -30.8842, -32.8216, -29.7707],\n",
            "        [-22.7723, -25.6521, -21.3291,  ..., -24.1068, -23.0086, -22.3759],\n",
            "        ...,\n",
            "        [-16.9917, -17.6389, -15.8315,  ..., -16.4338, -18.3842, -15.4695],\n",
            "        [-21.3210, -24.1221, -23.9191,  ..., -25.9021, -23.1271, -23.2259],\n",
            "        [-17.1173, -17.2539, -17.7727,  ..., -17.6138, -19.0083, -20.0288]]) tensor([[22877],\n",
            "        [26257],\n",
            "        [ 4899],\n",
            "        [24491],\n",
            "        [21882],\n",
            "        [ 9788],\n",
            "        [34616],\n",
            "        [17832],\n",
            "        [28035],\n",
            "        [ 8594],\n",
            "        [33793],\n",
            "        [31090],\n",
            "        [34616],\n",
            "        [31889],\n",
            "        [14809],\n",
            "        [27515]])\n",
            "tensor([[-20.5685, -21.5965, -21.9269,  ..., -24.3507, -23.3549, -20.1564],\n",
            "        [-22.4387, -22.3657, -22.5797,  ..., -22.3578, -23.5690, -23.7081],\n",
            "        [-20.7372, -21.4380, -18.6768,  ..., -20.5409, -19.6757, -19.4131],\n",
            "        ...,\n",
            "        [-20.5804, -22.7096, -21.4998,  ..., -23.4061, -20.0172, -21.5371],\n",
            "        [-19.1557, -19.2087, -16.2579,  ..., -18.1744, -16.3689, -15.7274],\n",
            "        [-17.0383, -17.5723, -17.0480,  ..., -17.0148, -17.1427, -18.0645]]) tensor([[ 1887],\n",
            "        [ 9803],\n",
            "        [10176],\n",
            "        [25858],\n",
            "        [  801],\n",
            "        [29661],\n",
            "        [34616],\n",
            "        [34077],\n",
            "        [ 7679],\n",
            "        [12700],\n",
            "        [16261],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [18170],\n",
            "        [ 2305],\n",
            "        [14733]])\n",
            "tensor([[-30.8491, -28.8491, -24.6789,  ..., -27.8218, -25.3092, -29.3163],\n",
            "        [-27.4007, -29.2279, -26.5729,  ..., -29.7753, -25.7381, -27.3680],\n",
            "        [-20.9167, -20.8423, -20.5096,  ..., -19.7826, -21.1786, -21.9601],\n",
            "        ...,\n",
            "        [-18.6210, -18.2883, -17.3761,  ..., -17.3739, -16.6824, -16.6744],\n",
            "        [-18.2015, -20.2459, -17.2673,  ..., -17.3962, -17.4017, -16.5093],\n",
            "        [-24.5535, -25.9596, -23.3679,  ..., -26.6216, -24.0668, -23.3044]]) tensor([[33949],\n",
            "        [32902],\n",
            "        [ 9803],\n",
            "        [11902],\n",
            "        [ 3669],\n",
            "        [36348],\n",
            "        [ 8463],\n",
            "        [14741],\n",
            "        [29580],\n",
            "        [34413],\n",
            "        [14160],\n",
            "        [17306],\n",
            "        [ 1887],\n",
            "        [ 7127],\n",
            "        [ 8463],\n",
            "        [14741]])\n",
            "tensor([[-23.7808, -22.8124, -22.3414,  ..., -24.4634, -24.6784, -23.6726],\n",
            "        [-19.4272, -20.7561, -21.5258,  ..., -22.6660, -20.0246, -16.2109],\n",
            "        [-16.8922, -21.5332, -18.9607,  ..., -23.2388, -16.9451, -20.2703],\n",
            "        ...,\n",
            "        [-25.1445, -23.6693, -22.4550,  ..., -24.1738, -22.4361, -23.9997],\n",
            "        [-16.5760, -15.1542, -17.4947,  ..., -17.4712, -17.8618, -18.0971],\n",
            "        [-25.0319, -26.2850, -23.6232,  ..., -25.3035, -26.1315, -22.6925]]) tensor([[ 1276],\n",
            "        [ 8459],\n",
            "        [31522],\n",
            "        [29580],\n",
            "        [34413],\n",
            "        [14160],\n",
            "        [ 6520],\n",
            "        [ 2305],\n",
            "        [ 5903],\n",
            "        [24491],\n",
            "        [28899],\n",
            "        [25671],\n",
            "        [ 8903],\n",
            "        [21379],\n",
            "        [14756],\n",
            "        [32587]])\n",
            "tensor([[-23.5973, -28.1496, -24.7524,  ..., -28.1275, -26.0203, -23.9411],\n",
            "        [-16.1601, -17.5601, -16.8900,  ..., -15.9452, -15.0864, -14.7693],\n",
            "        [-19.8327, -22.0910, -20.5434,  ..., -25.5004, -19.6872, -18.3980],\n",
            "        ...,\n",
            "        [-23.7778, -25.2425, -24.2409,  ..., -25.7821, -26.2351, -25.0861],\n",
            "        [-23.2713, -26.8743, -24.1742,  ..., -26.4249, -24.6373, -24.0367],\n",
            "        [-19.8945, -18.8456, -21.4925,  ..., -19.5897, -16.8191, -17.9317]]) tensor([[ 9007],\n",
            "        [ 7671],\n",
            "        [ 2255],\n",
            "        [ 9045],\n",
            "        [10842],\n",
            "        [ 1176],\n",
            "        [25671],\n",
            "        [35053],\n",
            "        [  297],\n",
            "        [16350],\n",
            "        [ 2255],\n",
            "        [21993],\n",
            "        [12217],\n",
            "        [33057],\n",
            "        [32397],\n",
            "        [27084]])\n",
            "tensor([[-19.3307, -21.4976, -18.5237,  ..., -22.6023, -20.2418, -18.6882],\n",
            "        [-23.0579, -24.5713, -23.2632,  ..., -26.1397, -22.1968, -21.0702],\n",
            "        [-22.0078, -22.7163, -23.0140,  ..., -22.9963, -20.7969, -20.1835],\n",
            "        ...,\n",
            "        [-19.7275, -18.9706, -20.5044,  ..., -19.2914, -20.5736, -20.1477],\n",
            "        [-16.4054, -17.7030, -17.3138,  ..., -16.8260, -17.9783, -19.3290],\n",
            "        [-16.6339, -19.1599, -16.5171,  ..., -17.5889, -16.6433, -16.9334]]) tensor([[36348],\n",
            "        [ 3669],\n",
            "        [ 2368],\n",
            "        [26650],\n",
            "        [13186],\n",
            "        [21993],\n",
            "        [11437],\n",
            "        [30215],\n",
            "        [21260],\n",
            "        [26400],\n",
            "        [26650],\n",
            "        [ 2368],\n",
            "        [23546],\n",
            "        [ 1887],\n",
            "        [27084],\n",
            "        [11902]])\n",
            "tensor([[-18.9519, -16.8457, -21.2373,  ..., -19.3682, -18.1724, -17.9466],\n",
            "        [-22.4437, -20.9193, -20.6287,  ..., -20.0416, -21.5497, -22.7799],\n",
            "        [-17.3945, -19.5768, -17.3002,  ..., -18.1635, -16.9458, -17.1911],\n",
            "        ...,\n",
            "        [-17.2900, -20.5527, -19.6155,  ..., -18.4880, -17.9037, -18.3849],\n",
            "        [-20.6480, -21.7013, -21.8978,  ..., -21.0059, -24.0180, -22.2723],\n",
            "        [-13.8173, -16.5890, -14.5184,  ..., -16.8965, -14.0103, -16.7850]]) tensor([[ 1099],\n",
            "        [ 9656],\n",
            "        [13186],\n",
            "        [ 9402],\n",
            "        [ 8869],\n",
            "        [21379],\n",
            "        [21089],\n",
            "        [22038],\n",
            "        [  887],\n",
            "        [ 2255],\n",
            "        [35104],\n",
            "        [23635],\n",
            "        [17499],\n",
            "        [18797],\n",
            "        [ 4089],\n",
            "        [ 8940]])\n",
            "tensor([[-18.8203, -19.0321, -18.3255,  ..., -18.3103, -18.2748, -15.2851],\n",
            "        [-15.7430, -15.0113, -16.0383,  ..., -15.1429, -15.5761, -15.6051],\n",
            "        [-15.5479, -17.1922, -16.3158,  ..., -16.0802, -14.0509, -16.7109],\n",
            "        ...,\n",
            "        [-35.8398, -35.3564, -33.6928,  ..., -35.1321, -31.9606, -35.3384],\n",
            "        [-15.6409, -15.6043, -15.1082,  ..., -17.1260, -16.4516, -16.1438],\n",
            "        [-22.8043, -19.8639, -20.1273,  ..., -21.9813, -21.5562, -22.1818]]) tensor([[11902],\n",
            "        [19054],\n",
            "        [14809],\n",
            "        [34762],\n",
            "        [ 3669],\n",
            "        [22704],\n",
            "        [33763],\n",
            "        [ 3669],\n",
            "        [11902],\n",
            "        [27515],\n",
            "        [23069],\n",
            "        [ 3669],\n",
            "        [  752],\n",
            "        [ 8940],\n",
            "        [ 7969],\n",
            "        [28041]])\n",
            "tensor([[-21.8876, -20.6973, -21.5453,  ..., -22.1119, -17.9358, -19.5963],\n",
            "        [-18.1607, -18.5456, -21.2988,  ..., -20.6681, -18.1746, -19.5314],\n",
            "        [-23.2351, -20.4838, -17.8354,  ..., -19.5911, -21.3875, -21.3859],\n",
            "        ...,\n",
            "        [-17.4584, -21.1271, -19.3382,  ..., -22.5374, -20.0527, -21.4342],\n",
            "        [-21.5267, -26.5164, -21.0484,  ..., -25.2864, -25.8245, -23.5714],\n",
            "        [-24.4361, -24.3334, -27.6657,  ..., -29.8893, -26.4065, -24.5820]]) tensor([[11902],\n",
            "        [17413],\n",
            "        [36348],\n",
            "        [13186],\n",
            "        [24861],\n",
            "        [ 1887],\n",
            "        [ 9402],\n",
            "        [17413],\n",
            "        [14809],\n",
            "        [25854],\n",
            "        [35175],\n",
            "        [33694],\n",
            "        [11437],\n",
            "        [ 1720],\n",
            "        [11902],\n",
            "        [ 1218]])\n",
            "tensor([[-19.5328, -21.2700, -22.8919,  ..., -22.7198, -19.1783, -19.8849],\n",
            "        [-17.4241, -18.7187, -19.3241,  ..., -17.4720, -18.7550, -18.3260],\n",
            "        [-16.7805, -20.0634, -19.3785,  ..., -19.4297, -20.0277, -19.0070],\n",
            "        ...,\n",
            "        [-19.1378, -22.7761, -22.7652,  ..., -23.1060, -20.4440, -18.5213],\n",
            "        [-22.2184, -21.8075, -23.8487,  ..., -25.3131, -20.9752, -21.9127],\n",
            "        [-23.8506, -26.8928, -24.5188,  ..., -28.1024, -25.4214, -25.2100]]) tensor([[20319],\n",
            "        [19367],\n",
            "        [19550],\n",
            "        [24491],\n",
            "        [ 1887],\n",
            "        [22345],\n",
            "        [24491],\n",
            "        [20489],\n",
            "        [14807],\n",
            "        [16269],\n",
            "        [25239],\n",
            "        [ 8560],\n",
            "        [26623],\n",
            "        [19478],\n",
            "        [ 5857],\n",
            "        [27665]])\n",
            "tensor([[-30.5373, -29.9966, -27.8187,  ..., -31.4439, -27.4386, -24.7588],\n",
            "        [-26.5665, -24.7155, -24.0734,  ..., -24.8361, -22.4803, -22.5918],\n",
            "        [-20.3355, -17.5297, -18.1816,  ..., -17.8876, -16.9454, -18.5134],\n",
            "        ...,\n",
            "        [-18.5423, -20.0796, -20.5663,  ..., -23.0244, -16.7702, -21.5431],\n",
            "        [-23.0588, -24.0413, -22.5848,  ..., -26.0038, -23.7400, -22.9305],\n",
            "        [-21.4219, -23.3037, -21.7458,  ..., -20.5080, -19.0262, -19.9749]]) tensor([[30845],\n",
            "        [29074],\n",
            "        [35554],\n",
            "        [28379],\n",
            "        [35669],\n",
            "        [33692],\n",
            "        [ 9039],\n",
            "        [12794],\n",
            "        [ 1847],\n",
            "        [ 9070],\n",
            "        [ 1887],\n",
            "        [31906],\n",
            "        [ 5001],\n",
            "        [25241],\n",
            "        [ 9471],\n",
            "        [20951]])\n",
            "tensor([[-21.2164, -21.6684, -22.6384,  ..., -25.4647, -21.1831, -18.0762],\n",
            "        [-23.5557, -26.5528, -24.5002,  ..., -24.8018, -21.2078, -25.0039],\n",
            "        [-30.0723, -30.5547, -29.5083,  ..., -31.8012, -28.7900, -28.7724],\n",
            "        ...,\n",
            "        [-18.5627, -22.2659, -23.0087,  ..., -22.5881, -21.9752, -22.3335],\n",
            "        [-17.9188, -18.9894, -16.8050,  ..., -19.2855, -18.9586, -15.1876],\n",
            "        [-19.4121, -20.1734, -19.7954,  ..., -21.8190, -20.0200, -21.6666]]) tensor([[ 6626],\n",
            "        [24447],\n",
            "        [  426],\n",
            "        [ 9471],\n",
            "        [ 9880],\n",
            "        [ 9070],\n",
            "        [21238],\n",
            "        [24174],\n",
            "        [ 1887],\n",
            "        [29994],\n",
            "        [ 1887],\n",
            "        [30264],\n",
            "        [ 5001],\n",
            "        [32193],\n",
            "        [33304],\n",
            "        [27084]])\n",
            "tensor([[-16.9032, -18.3746, -20.1783,  ..., -20.2395, -17.2703, -16.7328],\n",
            "        [-23.6335, -22.2767, -20.1920,  ..., -20.9369, -22.3200, -19.4190],\n",
            "        [-17.9262, -19.8023, -18.4030,  ..., -19.0122, -18.4447, -16.9777],\n",
            "        ...,\n",
            "        [-18.2433, -17.5022, -17.4454,  ..., -18.6955, -16.9124, -18.4362],\n",
            "        [-19.4709, -22.0731, -22.4595,  ..., -21.5956, -20.1877, -17.7404],\n",
            "        [-16.6288, -15.6522, -15.3510,  ..., -17.3217, -16.9637, -14.1156]]) tensor([[17413],\n",
            "        [10383],\n",
            "        [ 2606],\n",
            "        [29994],\n",
            "        [31352],\n",
            "        [35104],\n",
            "        [29994],\n",
            "        [29994],\n",
            "        [33793],\n",
            "        [ 2628],\n",
            "        [   45],\n",
            "        [36732],\n",
            "        [29598],\n",
            "        [ 7691],\n",
            "        [11085],\n",
            "        [25241]])\n",
            "tensor([[-20.4435, -20.0536, -16.9177,  ..., -20.0642, -18.7570, -16.6034],\n",
            "        [-24.7410, -25.3706, -21.4401,  ..., -25.5211, -22.5904, -24.2288],\n",
            "        [-21.5741, -20.0480, -18.4123,  ..., -21.4659, -19.2363, -18.0433],\n",
            "        ...,\n",
            "        [-32.8136, -33.4004, -32.0921,  ..., -37.6079, -36.2162, -36.7797],\n",
            "        [-25.1795, -30.0477, -28.3445,  ..., -28.3879, -26.4159, -26.5705],\n",
            "        [-20.2026, -20.7057, -23.3935,  ..., -21.5793, -22.0493, -19.3379]]) tensor([[29928],\n",
            "        [23300],\n",
            "        [ 7019],\n",
            "        [ 6395],\n",
            "        [ 4983],\n",
            "        [ 4002],\n",
            "        [14401],\n",
            "        [ 8276],\n",
            "        [ 1887],\n",
            "        [18762],\n",
            "        [34938],\n",
            "        [25239],\n",
            "        [18138],\n",
            "        [19730],\n",
            "        [25310],\n",
            "        [12187]])\n",
            "tensor([[-14.5084, -15.1656, -14.2261,  ..., -15.3395, -14.8409, -14.8781],\n",
            "        [-17.0803, -18.5328, -19.8114,  ..., -19.7865, -17.1329, -19.1891],\n",
            "        [-24.2350, -26.9624, -24.5577,  ..., -23.9646, -27.6582, -23.9144],\n",
            "        ...,\n",
            "        [-16.2394, -19.1540, -17.3743,  ..., -19.9059, -17.8385, -20.9807],\n",
            "        [-21.1242, -21.9019, -22.0349,  ..., -23.6771, -23.8786, -23.2981],\n",
            "        [-19.2637, -19.0644, -16.9398,  ..., -20.1844, -19.5304, -20.1671]]) tensor([[ 4983],\n",
            "        [33177],\n",
            "        [29598],\n",
            "        [36269],\n",
            "        [14598],\n",
            "        [30759],\n",
            "        [29994],\n",
            "        [16203],\n",
            "        [29598],\n",
            "        [  674],\n",
            "        [ 1887],\n",
            "        [ 4983],\n",
            "        [ 4983],\n",
            "        [26454],\n",
            "        [23300],\n",
            "        [10746]])\n",
            "tensor([[-16.3795, -16.0295, -16.8097,  ..., -18.0388, -17.3557, -15.4323],\n",
            "        [-17.8562, -17.6162, -17.7226,  ..., -17.5504, -18.1332, -18.4457],\n",
            "        [-14.9557, -16.7727, -16.0610,  ..., -15.9735, -17.5669, -15.8768],\n",
            "        ...,\n",
            "        [-16.7792, -17.1729, -17.7786,  ..., -18.5259, -18.7421, -18.7544],\n",
            "        [-22.8083, -20.9751, -18.7394,  ..., -22.2025, -18.6222, -22.2616],\n",
            "        [-25.9317, -23.3989, -23.1171,  ..., -26.4110, -23.3554, -25.5439]]) tensor([[ 1433],\n",
            "        [12156],\n",
            "        [29542],\n",
            "        [16544],\n",
            "        [ 2255],\n",
            "        [15386],\n",
            "        [34472],\n",
            "        [33694],\n",
            "        [31017],\n",
            "        [ 5967],\n",
            "        [35104],\n",
            "        [12794],\n",
            "        [24971],\n",
            "        [23800],\n",
            "        [23800],\n",
            "        [ 8276]])\n",
            "tensor([[-19.5564, -18.5214, -21.0230,  ..., -19.0968, -17.3564, -21.9146],\n",
            "        [-23.5955, -27.2857, -25.7815,  ..., -27.4971, -25.8321, -24.1165],\n",
            "        [-15.3427, -17.7956, -15.9159,  ..., -16.3033, -15.9512, -16.1674],\n",
            "        ...,\n",
            "        [-23.2078, -20.5821, -20.8934,  ..., -24.9839, -21.9820, -21.2712],\n",
            "        [-17.4543, -17.8024, -17.4596,  ..., -21.2878, -19.2820, -14.5331],\n",
            "        [-26.3685, -28.7101, -24.7750,  ..., -27.4499, -25.1061, -24.9858]]) tensor([[ 3638],\n",
            "        [27018],\n",
            "        [24971],\n",
            "        [ 6013],\n",
            "        [ 7572],\n",
            "        [ 1887],\n",
            "        [18338],\n",
            "        [34010],\n",
            "        [ 6013],\n",
            "        [20951],\n",
            "        [ 8276],\n",
            "        [  319],\n",
            "        [35393],\n",
            "        [35562],\n",
            "        [ 6395],\n",
            "        [27635]])\n",
            "tensor([[-21.3282, -21.9232, -20.7140,  ..., -21.9764, -20.6928, -20.8794],\n",
            "        [-27.6809, -25.3708, -26.8042,  ..., -27.2267, -24.1073, -26.2405],\n",
            "        [-22.3614, -19.3376, -18.2972,  ..., -20.8941, -22.7439, -18.0878],\n",
            "        ...,\n",
            "        [-15.6462, -17.1745, -16.7054,  ..., -18.1324, -15.6371, -14.4653],\n",
            "        [-21.4757, -21.2499, -20.4797,  ..., -22.4475, -22.4102, -22.4904],\n",
            "        [-16.6070, -17.7054, -16.6835,  ..., -16.6477, -16.8761, -16.4052]]) tensor([[35393],\n",
            "        [35562],\n",
            "        [ 8973],\n",
            "        [ 6013],\n",
            "        [25182],\n",
            "        [ 9803],\n",
            "        [ 4160],\n",
            "        [ 9803],\n",
            "        [  752],\n",
            "        [13211],\n",
            "        [30007],\n",
            "        [22873],\n",
            "        [ 9794],\n",
            "        [17702],\n",
            "        [23635],\n",
            "        [26354]])\n",
            "tensor([[-19.7218, -21.2014, -20.7393,  ..., -21.8264, -21.9093, -20.7454],\n",
            "        [-18.8517, -21.1958, -19.7128,  ..., -21.5676, -21.6693, -17.8081],\n",
            "        [-16.1537, -17.2439, -17.7446,  ..., -18.3033, -16.0021, -15.5877],\n",
            "        ...,\n",
            "        [-28.1539, -26.2792, -24.1732,  ..., -29.0093, -26.7375, -23.7867],\n",
            "        [-20.7902, -24.2498, -23.1126,  ..., -24.8682, -22.4685, -21.4165],\n",
            "        [-19.7184, -23.1402, -22.6896,  ..., -22.3637, -22.0648, -21.1369]]) tensor([[ 3267],\n",
            "        [28147],\n",
            "        [24971],\n",
            "        [11902],\n",
            "        [21089],\n",
            "        [ 3724],\n",
            "        [24661],\n",
            "        [15529],\n",
            "        [23548],\n",
            "        [28265],\n",
            "        [32597],\n",
            "        [29580],\n",
            "        [14262],\n",
            "        [19257],\n",
            "        [35194],\n",
            "        [33726]])\n",
            "tensor([[-19.4017, -23.2458, -21.5120,  ..., -19.8026, -20.0378, -19.2128],\n",
            "        [-23.7129, -23.0882, -22.2023,  ..., -21.5967, -23.3129, -21.0896],\n",
            "        [-32.3435, -30.6735, -28.3655,  ..., -28.4284, -30.4596, -27.6614],\n",
            "        ...,\n",
            "        [-19.5472, -20.2450, -19.1662,  ..., -19.9625, -17.4614, -18.7159],\n",
            "        [-17.0130, -18.6160, -18.7337,  ..., -18.2864, -16.3473, -18.2993],\n",
            "        [-20.8141, -21.9649, -20.1332,  ..., -22.9467, -19.9884, -18.8311]]) tensor([[20561],\n",
            "        [13982],\n",
            "        [24769],\n",
            "        [ 2305],\n",
            "        [ 3669],\n",
            "        [19890],\n",
            "        [23061],\n",
            "        [ 2255],\n",
            "        [ 4005],\n",
            "        [20284],\n",
            "        [21796],\n",
            "        [31352],\n",
            "        [11486],\n",
            "        [11760],\n",
            "        [31168],\n",
            "        [ 2478]])\n",
            "tensor([[-26.4809, -25.9364, -24.7587,  ..., -26.2535, -25.0277, -20.6257],\n",
            "        [-18.6114, -20.9560, -21.7331,  ..., -21.2284, -21.5669, -21.8419],\n",
            "        [-21.8739, -20.6759, -19.6743,  ..., -20.5076, -21.7536, -20.1341],\n",
            "        ...,\n",
            "        [-21.5693, -22.0986, -21.8614,  ..., -20.9618, -20.0979, -18.9645],\n",
            "        [-18.0114, -19.1185, -17.2030,  ..., -18.0033, -16.6083, -14.9230],\n",
            "        [-20.2226, -22.9227, -19.2307,  ..., -21.8363, -21.0197, -21.5969]]) tensor([[10479],\n",
            "        [31480],\n",
            "        [13139],\n",
            "        [ 9803],\n",
            "        [ 2255],\n",
            "        [ 8466],\n",
            "        [21993],\n",
            "        [21230],\n",
            "        [20027],\n",
            "        [ 7987],\n",
            "        [12049],\n",
            "        [33828],\n",
            "        [35967],\n",
            "        [ 7326],\n",
            "        [ 5772],\n",
            "        [23300]])\n",
            "tensor([[-21.7811, -18.8603, -17.1117,  ..., -19.4201, -17.5380, -19.8540],\n",
            "        [-22.8516, -21.2662, -20.6175,  ..., -21.7329, -21.5773, -20.3074],\n",
            "        [-23.8536, -24.3850, -23.2946,  ..., -27.2568, -22.2556, -20.7320],\n",
            "        ...,\n",
            "        [-18.7697, -19.3987, -19.0809,  ..., -19.6816, -19.0975, -18.5036],\n",
            "        [-18.2832, -17.0099, -18.4498,  ..., -19.3295, -17.9543, -19.2918],\n",
            "        [-20.7857, -21.1885, -18.6391,  ..., -22.1587, -16.6517, -19.5564]]) tensor([[29381],\n",
            "        [ 2327],\n",
            "        [30845],\n",
            "        [ 3854],\n",
            "        [ 4363],\n",
            "        [ 5772],\n",
            "        [ 2628],\n",
            "        [10348],\n",
            "        [23300],\n",
            "        [11902],\n",
            "        [30264],\n",
            "        [36424],\n",
            "        [16544],\n",
            "        [ 1103],\n",
            "        [ 4564],\n",
            "        [16726]])\n",
            "tensor([[-21.2913, -21.2291, -18.5322,  ..., -22.2992, -20.5062, -21.7496],\n",
            "        [-20.5780, -19.7991, -16.5001,  ..., -17.1597, -18.6883, -18.9369],\n",
            "        [-31.3747, -35.7810, -38.1716,  ..., -38.2334, -32.7820, -34.1474],\n",
            "        ...,\n",
            "        [-24.7157, -20.7226, -21.7667,  ..., -26.7971, -20.5328, -22.4442],\n",
            "        [-23.6529, -23.3579, -22.5276,  ..., -26.5165, -22.5461, -20.7940],\n",
            "        [-26.2014, -28.4749, -26.4289,  ..., -29.2327, -26.2942, -26.7014]]) tensor([[17809],\n",
            "        [29892],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [17746],\n",
            "        [29746],\n",
            "        [11902],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [19257],\n",
            "        [35194],\n",
            "        [ 4723],\n",
            "        [ 4536],\n",
            "        [14262],\n",
            "        [19257],\n",
            "        [35194]])\n",
            "tensor([[-24.7288, -26.7426, -23.2957,  ..., -29.5109, -24.0810, -25.0525],\n",
            "        [-19.2185, -19.5414, -17.3820,  ..., -18.8706, -18.4435, -18.5228],\n",
            "        [-29.2757, -30.7843, -31.5234,  ..., -33.6907, -29.4009, -30.9082],\n",
            "        ...,\n",
            "        [-19.4864, -17.4053, -19.6606,  ..., -20.6649, -19.5768, -20.0237],\n",
            "        [-20.3444, -19.7002, -19.3085,  ..., -20.1247, -22.0526, -20.4142],\n",
            "        [-25.5046, -26.3686, -25.9776,  ..., -22.6856, -23.9515, -24.6795]]) tensor([[35967],\n",
            "        [ 8594],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [30264],\n",
            "        [11611],\n",
            "        [30264],\n",
            "        [25761],\n",
            "        [ 8940],\n",
            "        [28041],\n",
            "        [ 9289],\n",
            "        [11902],\n",
            "        [35967],\n",
            "        [21575],\n",
            "        [23546],\n",
            "        [23061]])\n",
            "tensor([[-19.5231, -21.6792, -20.3143,  ..., -22.2498, -19.1033, -25.4416],\n",
            "        [-23.0426, -21.9635, -21.8097,  ..., -23.7609, -21.1367, -19.4801],\n",
            "        [-22.7803, -22.3946, -22.9392,  ..., -23.7579, -22.1072, -20.6595],\n",
            "        ...,\n",
            "        [-23.6682, -25.3929, -22.5078,  ..., -26.5176, -22.4507, -23.7171],\n",
            "        [-22.1691, -23.4486, -22.2356,  ..., -25.7559, -23.4251, -25.5560],\n",
            "        [-15.5010, -18.6541, -16.0469,  ..., -18.2878, -15.4477, -17.6557]]) tensor([[  957],\n",
            "        [31017],\n",
            "        [32626],\n",
            "        [29930],\n",
            "        [ 3433],\n",
            "        [ 9661],\n",
            "        [11756],\n",
            "        [12700],\n",
            "        [25977],\n",
            "        [20324],\n",
            "        [  850],\n",
            "        [29892],\n",
            "        [36273],\n",
            "        [ 9289],\n",
            "        [36866],\n",
            "        [29892]])\n",
            "tensor([[-22.4207, -21.9722, -21.5168,  ..., -19.8511, -19.3959, -20.5454],\n",
            "        [-23.3707, -25.1659, -24.4472,  ..., -29.2031, -26.2970, -23.9180],\n",
            "        [-17.4430, -21.4463, -21.7539,  ..., -24.9099, -20.6015, -19.8257],\n",
            "        ...,\n",
            "        [-22.0980, -23.4834, -19.8892,  ..., -24.9627, -22.3692, -22.4870],\n",
            "        [-24.9764, -24.2345, -22.5320,  ..., -24.8542, -22.0625, -24.2777],\n",
            "        [-21.0966, -21.5234, -21.4813,  ..., -20.1112, -21.2932, -17.9643]]) tensor([[21808],\n",
            "        [17811],\n",
            "        [30845],\n",
            "        [ 3854],\n",
            "        [24936],\n",
            "        [23546],\n",
            "        [24911],\n",
            "        [ 2181],\n",
            "        [  736],\n",
            "        [ 9803],\n",
            "        [11902],\n",
            "        [34010],\n",
            "        [32039],\n",
            "        [31545],\n",
            "        [21067],\n",
            "        [  930]])\n",
            "tensor([[-19.4836, -17.1458, -19.5506,  ..., -19.3610, -17.4291, -18.6684],\n",
            "        [-20.3830, -20.0458, -19.0128,  ..., -20.3065, -18.1188, -16.4151],\n",
            "        [-20.7685, -19.7027, -20.0845,  ..., -18.7153, -16.8899, -16.3082],\n",
            "        ...,\n",
            "        [-19.5539, -17.2398, -15.9096,  ..., -17.9194, -17.5221, -18.3493],\n",
            "        [-35.8053, -36.2407, -37.9294,  ..., -39.2994, -35.2159, -35.6264],\n",
            "        [-29.7820, -30.9048, -28.9435,  ..., -32.2086, -28.0510, -29.9997]]) tensor([[33316],\n",
            "        [25677],\n",
            "        [32337],\n",
            "        [28615],\n",
            "        [33971],\n",
            "        [14401],\n",
            "        [26145],\n",
            "        [26282],\n",
            "        [34010],\n",
            "        [16486],\n",
            "        [22704],\n",
            "        [10410],\n",
            "        [ 9154],\n",
            "        [ 3570],\n",
            "        [36281],\n",
            "        [21214]])\n",
            "tensor([[-15.9530, -19.7991, -17.1016,  ..., -18.8853, -16.4732, -19.2488],\n",
            "        [-16.6262, -17.4743, -14.7975,  ..., -18.2056, -17.4257, -15.6306],\n",
            "        [-17.5217, -16.9394, -18.8304,  ..., -16.6426, -15.8223, -17.6442],\n",
            "        ...,\n",
            "        [-35.9508, -37.3166, -35.0393,  ..., -42.7099, -36.0085, -39.0960],\n",
            "        [-29.9796, -34.6107, -31.7907,  ..., -36.4361, -29.6395, -31.1887],\n",
            "        [-22.1480, -25.5701, -22.2671,  ..., -23.9282, -20.0929, -21.9037]]) tensor([[12199],\n",
            "        [ 4385],\n",
            "        [34022],\n",
            "        [22877],\n",
            "        [11902],\n",
            "        [17480],\n",
            "        [36388],\n",
            "        [21089],\n",
            "        [29598],\n",
            "        [16350],\n",
            "        [11902],\n",
            "        [12049],\n",
            "        [22593],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 7341]])\n",
            "tensor([[-22.2344, -21.5413, -20.7720,  ..., -22.1742, -23.3828, -22.6497],\n",
            "        [-20.1240, -21.4609, -21.2712,  ..., -22.9875, -20.4112, -20.1952],\n",
            "        [-20.7667, -20.8107, -22.2637,  ..., -22.9254, -20.3408, -20.3866],\n",
            "        ...,\n",
            "        [-20.3576, -20.2018, -17.4536,  ..., -21.3221, -20.9847, -20.5439],\n",
            "        [-18.5747, -18.9619, -18.9010,  ..., -20.8022, -18.9375, -21.2717],\n",
            "        [-21.0729, -24.5510, -23.0489,  ..., -24.7598, -23.2851, -23.1305]]) tensor([[11538],\n",
            "        [26354],\n",
            "        [  661],\n",
            "        [11611],\n",
            "        [12049],\n",
            "        [ 7341],\n",
            "        [27737],\n",
            "        [18138],\n",
            "        [19543],\n",
            "        [35967],\n",
            "        [13615],\n",
            "        [16350],\n",
            "        [25239],\n",
            "        [11263],\n",
            "        [ 1433],\n",
            "        [12199]])\n",
            "tensor([[-16.5457, -20.1304, -16.7428,  ..., -21.2594, -18.2095, -17.6461],\n",
            "        [-19.1578, -19.4222, -19.5318,  ..., -20.3498, -17.3583, -17.4789],\n",
            "        [-21.1411, -22.3983, -21.5952,  ..., -22.8992, -18.8134, -22.1428],\n",
            "        ...,\n",
            "        [-22.8621, -26.7818, -22.5210,  ..., -23.4791, -26.1787, -25.9299],\n",
            "        [-14.5278, -18.0314, -17.4173,  ..., -19.8526, -18.2153, -15.6789],\n",
            "        [-27.7531, -27.6880, -27.7279,  ..., -26.8857, -27.8624, -26.4958]]) tensor([[32782],\n",
            "        [16870],\n",
            "        [35554],\n",
            "        [ 1045],\n",
            "        [29892],\n",
            "        [28852],\n",
            "        [30423],\n",
            "        [ 3046],\n",
            "        [12075],\n",
            "        [35686],\n",
            "        [ 2255],\n",
            "        [31776],\n",
            "        [25854],\n",
            "        [17506],\n",
            "        [ 8695],\n",
            "        [ 3669]])\n",
            "tensor([[-22.6147, -24.0047, -21.5773,  ..., -24.4012, -21.9583, -22.5707],\n",
            "        [-17.6847, -16.7732, -19.6986,  ..., -19.5696, -19.6201, -18.2448],\n",
            "        [-24.0202, -25.7602, -23.8805,  ..., -26.0183, -23.4818, -24.1164],\n",
            "        ...,\n",
            "        [-20.2998, -23.1412, -19.5669,  ..., -24.3299, -19.7128, -19.4424],\n",
            "        [-23.0362, -23.4168, -19.7960,  ..., -19.9456, -18.8047, -20.3730],\n",
            "        [-18.1566, -22.2764, -21.7998,  ..., -23.3492, -19.7665, -20.0240]]) tensor([[24971],\n",
            "        [ 1065],\n",
            "        [ 3669],\n",
            "        [ 9070],\n",
            "        [26354],\n",
            "        [27494],\n",
            "        [ 1788],\n",
            "        [18099],\n",
            "        [35611],\n",
            "        [11790],\n",
            "        [30264],\n",
            "        [31203],\n",
            "        [23876],\n",
            "        [15880],\n",
            "        [ 8653],\n",
            "        [27248]])\n",
            "tensor([[-27.8450, -28.1706, -28.5337,  ..., -24.1765, -24.7184, -26.8569],\n",
            "        [-16.4091, -18.1137, -19.0163,  ..., -18.4544, -15.9758, -16.5541],\n",
            "        [-19.3119, -21.0730, -19.4819,  ..., -21.5732, -19.1841, -17.6226],\n",
            "        ...,\n",
            "        [-20.9274, -22.1690, -17.2359,  ..., -20.9884, -19.8423, -20.2027],\n",
            "        [-26.4856, -27.9498, -27.4104,  ..., -28.0091, -26.3534, -25.3938],\n",
            "        [-22.1952, -23.8085, -20.2675,  ..., -20.3626, -21.5625, -18.7917]]) tensor([[29598],\n",
            "        [35104],\n",
            "        [ 5418],\n",
            "        [ 7297],\n",
            "        [30768],\n",
            "        [29598],\n",
            "        [ 7828],\n",
            "        [ 6777],\n",
            "        [  426],\n",
            "        [12591],\n",
            "        [30486],\n",
            "        [32391],\n",
            "        [ 8653],\n",
            "        [15880],\n",
            "        [22345],\n",
            "        [15880]])\n",
            "tensor([[-19.6874, -22.2708, -21.0899,  ..., -23.2033, -22.8496, -21.5408],\n",
            "        [-30.1845, -31.3756, -34.6307,  ..., -33.5320, -28.9353, -32.7674],\n",
            "        [-20.6791, -26.8291, -22.6132,  ..., -26.7486, -23.5761, -21.7848],\n",
            "        ...,\n",
            "        [-19.1450, -18.7401, -19.0871,  ..., -20.7107, -16.4619, -17.9079],\n",
            "        [-16.6588, -14.3007, -14.1102,  ..., -15.9366, -14.6924, -13.2720],\n",
            "        [-18.7082, -20.4518, -18.0187,  ..., -21.3006, -17.0521, -17.2312]]) tensor([[36160],\n",
            "        [ 2606],\n",
            "        [34907],\n",
            "        [18737],\n",
            "        [ 9508],\n",
            "        [ 1276],\n",
            "        [ 8176],\n",
            "        [ 2993],\n",
            "        [22543],\n",
            "        [ 2683],\n",
            "        [ 4311],\n",
            "        [ 4160],\n",
            "        [20563],\n",
            "        [ 1738],\n",
            "        [  887],\n",
            "        [ 2068]])\n",
            "tensor([[-15.9852, -19.2605, -17.0371,  ..., -20.1738, -17.4464, -16.8046],\n",
            "        [-17.3034, -16.6987, -17.8067,  ..., -18.4505, -17.6944, -18.0933],\n",
            "        [-15.0036, -15.6853, -15.4163,  ..., -15.0486, -15.5223, -15.1446],\n",
            "        ...,\n",
            "        [-22.9347, -22.2427, -22.5625,  ..., -21.5248, -20.5458, -19.5319],\n",
            "        [-15.9644, -19.3912, -16.8211,  ..., -19.3945, -17.0192, -17.5780],\n",
            "        [-21.0215, -21.3239, -19.6939,  ..., -21.1918, -21.0124, -20.4979]]) tensor([[15272],\n",
            "        [ 4160],\n",
            "        [14529],\n",
            "        [23949],\n",
            "        [29167],\n",
            "        [ 7297],\n",
            "        [26600],\n",
            "        [18170],\n",
            "        [17842],\n",
            "        [35308],\n",
            "        [33057],\n",
            "        [ 9471],\n",
            "        [20570],\n",
            "        [20563],\n",
            "        [ 2255],\n",
            "        [12049]])\n",
            "tensor([[-16.0408, -20.1302, -19.9019,  ..., -21.5813, -18.5968, -19.0161],\n",
            "        [-24.0198, -23.9728, -20.6549,  ..., -27.7897, -23.8636, -21.6791],\n",
            "        [-25.2429, -26.0383, -24.1659,  ..., -28.3845, -24.3769, -23.5091],\n",
            "        ...,\n",
            "        [-28.4455, -30.3392, -29.3074,  ..., -26.5000, -26.0336, -26.2151],\n",
            "        [-18.6919, -18.4783, -19.7099,  ..., -17.5303, -19.0079, -16.4518],\n",
            "        [-19.5351, -20.2369, -17.9680,  ..., -21.7854, -17.9435, -19.3298]]) tensor([[14641],\n",
            "        [17832],\n",
            "        [ 2879],\n",
            "        [16442],\n",
            "        [14217],\n",
            "        [ 8641],\n",
            "        [29580],\n",
            "        [ 2255],\n",
            "        [ 5019],\n",
            "        [26905],\n",
            "        [19917],\n",
            "        [36786],\n",
            "        [ 5813],\n",
            "        [24187],\n",
            "        [32398],\n",
            "        [36726]])\n",
            "tensor([[-23.1240, -22.0689, -21.3632,  ..., -23.4505, -22.3353, -19.8145],\n",
            "        [-16.6916, -17.5093, -17.5894,  ..., -17.0886, -16.5429, -16.3661],\n",
            "        [-19.1097, -19.7872, -15.2844,  ..., -19.0159, -17.8507, -16.7025],\n",
            "        ...,\n",
            "        [-26.3262, -26.1813, -24.3469,  ..., -24.5006, -22.4142, -21.8190],\n",
            "        [-26.6888, -22.8807, -24.4928,  ..., -28.0137, -26.5095, -27.2731],\n",
            "        [-22.4183, -22.8453, -19.5145,  ..., -23.6128, -22.4439, -23.4864]]) tensor([[ 1887],\n",
            "        [23473],\n",
            "        [33897],\n",
            "        [11902],\n",
            "        [29580],\n",
            "        [23300],\n",
            "        [ 5708],\n",
            "        [ 1887],\n",
            "        [23255],\n",
            "        [36281],\n",
            "        [33316],\n",
            "        [22940],\n",
            "        [34465],\n",
            "        [23655],\n",
            "        [13783],\n",
            "        [20667]])\n",
            "tensor([[-24.1112, -26.6962, -24.2816,  ..., -27.6464, -24.0610, -21.9793],\n",
            "        [-25.2222, -25.7185, -22.5046,  ..., -24.4173, -22.2533, -21.1868],\n",
            "        [-27.4132, -26.9899, -26.6011,  ..., -27.7737, -28.8029, -27.2652],\n",
            "        ...,\n",
            "        [-19.3489, -22.8974, -20.7163,  ..., -24.5853, -21.5566, -21.5795],\n",
            "        [-19.2728, -19.4883, -21.4221,  ..., -22.8682, -18.7753, -20.3025],\n",
            "        [-16.6066, -19.1597, -17.3242,  ..., -19.6441, -18.5487, -17.4338]]) tensor([[24174],\n",
            "        [22944],\n",
            "        [22517],\n",
            "        [ 1276],\n",
            "        [34956],\n",
            "        [11902],\n",
            "        [19674],\n",
            "        [20557],\n",
            "        [33316],\n",
            "        [22940],\n",
            "        [23300],\n",
            "        [28485],\n",
            "        [24971],\n",
            "        [33305],\n",
            "        [26571],\n",
            "        [24536]])\n",
            "tensor([[-20.6242, -20.7552, -23.1402,  ..., -20.8779, -19.7525, -19.9338],\n",
            "        [-20.8888, -23.6089, -21.3351,  ..., -23.0198, -19.4977, -22.2620],\n",
            "        [-26.5640, -30.3149, -26.2093,  ..., -28.9510, -27.4149, -27.1440],\n",
            "        ...,\n",
            "        [-21.3363, -21.1124, -20.2950,  ..., -19.6629, -20.2889, -19.1703],\n",
            "        [-24.3450, -24.7240, -22.9902,  ..., -25.2688, -23.0672, -24.7930],\n",
            "        [-19.8562, -20.6678, -19.4652,  ..., -20.9316, -18.3800, -20.2043]]) tensor([[26014],\n",
            "        [32940],\n",
            "        [11902],\n",
            "        [ 3028],\n",
            "        [28146],\n",
            "        [24526],\n",
            "        [29580],\n",
            "        [ 6013],\n",
            "        [ 2255],\n",
            "        [18487],\n",
            "        [34956],\n",
            "        [20182],\n",
            "        [ 9803],\n",
            "        [18092],\n",
            "        [13822],\n",
            "        [32146]])\n",
            "tensor([[-17.0630, -19.7874, -17.7904,  ..., -21.3872, -20.0030, -18.7767],\n",
            "        [-22.5968, -20.1396, -21.7834,  ..., -22.1485, -22.9246, -19.9306],\n",
            "        [-27.4920, -26.8015, -25.3266,  ..., -27.7941, -26.0496, -25.7402],\n",
            "        ...,\n",
            "        [-22.8906, -25.1219, -25.4375,  ..., -25.5924, -22.3457, -20.9342],\n",
            "        [-18.4513, -18.5904, -18.4698,  ..., -19.9763, -21.3699, -19.9629],\n",
            "        [-20.5406, -21.7467, -21.2556,  ..., -19.9010, -19.6172, -21.3193]]) tensor([[ 9661],\n",
            "        [31128],\n",
            "        [11421],\n",
            "        [17413],\n",
            "        [13759],\n",
            "        [23086],\n",
            "        [ 8940],\n",
            "        [ 7969],\n",
            "        [17413],\n",
            "        [26571],\n",
            "        [32902],\n",
            "        [  565],\n",
            "        [19309],\n",
            "        [22944],\n",
            "        [11312],\n",
            "        [ 7127]])\n",
            "tensor([[-18.4589, -17.5187, -20.8924,  ..., -18.6226, -20.0793, -18.6135],\n",
            "        [-19.8535, -22.7053, -17.8654,  ..., -20.8114, -19.7232, -18.7278],\n",
            "        [-22.1034, -21.2321, -21.2371,  ..., -21.6569, -20.3856, -22.6070],\n",
            "        ...,\n",
            "        [-18.8998, -19.0900, -17.1255,  ..., -20.4842, -16.9120, -16.4949],\n",
            "        [-25.3514, -23.5114, -20.4306,  ..., -23.7325, -22.1328, -24.6228],\n",
            "        [-19.8525, -18.9494, -21.2991,  ..., -22.5399, -19.2715, -17.5786]]) tensor([[  801],\n",
            "        [ 9203],\n",
            "        [11902],\n",
            "        [23300],\n",
            "        [ 6348],\n",
            "        [19432],\n",
            "        [10251],\n",
            "        [21786],\n",
            "        [22509],\n",
            "        [24744],\n",
            "        [32632],\n",
            "        [31128],\n",
            "        [11902],\n",
            "        [22940],\n",
            "        [29580],\n",
            "        [16525]])\n",
            "tensor([[-22.9324, -25.1829, -22.5519,  ..., -24.1345, -21.1257, -24.0522],\n",
            "        [-23.6095, -22.5098, -23.2082,  ..., -26.0790, -22.8313, -21.7648],\n",
            "        [-19.7949, -19.5068, -20.5862,  ..., -21.3037, -19.6753, -18.4181],\n",
            "        ...,\n",
            "        [-21.4305, -22.8688, -22.8016,  ..., -21.3677, -20.7705, -20.5930],\n",
            "        [-23.1215, -23.8992, -24.2445,  ..., -24.7033, -22.0644, -24.0013],\n",
            "        [-18.5478, -19.2387, -18.7997,  ..., -22.5078, -19.4413, -18.6120]]) tensor([[35161],\n",
            "        [  736],\n",
            "        [11263],\n",
            "        [10814],\n",
            "        [22940],\n",
            "        [ 7290],\n",
            "        [14809],\n",
            "        [ 9917],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 1138],\n",
            "        [20324],\n",
            "        [ 2410],\n",
            "        [18357],\n",
            "        [22246],\n",
            "        [ 7893]])\n",
            "tensor([[-20.8791, -21.5554, -19.2852,  ..., -24.7012, -19.6947, -18.5660],\n",
            "        [-19.7513, -21.4356, -17.3924,  ..., -23.8259, -19.5445, -19.5053],\n",
            "        [-22.9107, -20.8368, -18.0960,  ..., -25.1662, -21.4876, -18.2162],\n",
            "        ...,\n",
            "        [-33.4984, -35.2297, -32.9549,  ..., -35.2707, -31.3818, -34.0873],\n",
            "        [-23.6316, -20.8843, -20.9255,  ..., -23.1108, -22.1016, -19.1129],\n",
            "        [-16.8606, -19.4976, -17.4753,  ..., -18.2868, -17.2376, -18.5273]]) tensor([[33305],\n",
            "        [15186],\n",
            "        [16525],\n",
            "        [  602],\n",
            "        [23735],\n",
            "        [34010],\n",
            "        [11443],\n",
            "        [34465],\n",
            "        [ 4559],\n",
            "        [14217],\n",
            "        [33726],\n",
            "        [10500],\n",
            "        [23300],\n",
            "        [11902],\n",
            "        [21214],\n",
            "        [18258]])\n",
            "tensor([[-20.3449, -23.7030, -20.7572,  ..., -20.6548, -19.5886, -18.9019],\n",
            "        [-29.5805, -26.2948, -26.9715,  ..., -25.0901, -28.5103, -25.5868],\n",
            "        [-21.1262, -24.1581, -20.8456,  ..., -20.0174, -20.0458, -21.1119],\n",
            "        ...,\n",
            "        [-21.0045, -19.5953, -21.5855,  ..., -19.1539, -19.2637, -18.7759],\n",
            "        [-16.5837, -18.8665, -18.9935,  ..., -20.2625, -16.5815, -18.2680],\n",
            "        [-17.1066, -20.4932, -18.9799,  ..., -22.7888, -17.5509, -19.5046]]) tensor([[15038],\n",
            "        [ 1887],\n",
            "        [15148],\n",
            "        [23061],\n",
            "        [29892],\n",
            "        [27515],\n",
            "        [17811],\n",
            "        [32626],\n",
            "        [18490],\n",
            "        [ 6800],\n",
            "        [19004],\n",
            "        [20756],\n",
            "        [ 2585],\n",
            "        [ 1276],\n",
            "        [12175],\n",
            "        [27515]])\n",
            "tensor([[-22.7696, -22.7215, -23.3908,  ..., -26.7421, -23.7577, -21.8061],\n",
            "        [-16.9778, -19.0044, -18.3585,  ..., -18.6452, -17.7507, -17.3072],\n",
            "        [-25.7723, -27.6839, -28.6857,  ..., -29.3306, -25.3370, -26.5013],\n",
            "        ...,\n",
            "        [-24.2298, -21.6107, -20.5156,  ..., -21.1055, -21.2680, -20.5153],\n",
            "        [-20.0124, -18.5764, -18.4343,  ..., -20.4316, -21.2780, -18.1951],\n",
            "        [-18.4782, -21.7348, -17.4643,  ..., -20.6132, -18.2444, -17.7006]]) tensor([[ 1276],\n",
            "        [32626],\n",
            "        [23565],\n",
            "        [32587],\n",
            "        [18242],\n",
            "        [15656],\n",
            "        [29580],\n",
            "        [14333],\n",
            "        [31017],\n",
            "        [23474],\n",
            "        [34818],\n",
            "        [24628],\n",
            "        [ 8463],\n",
            "        [21038],\n",
            "        [36658],\n",
            "        [ 3222]])\n",
            "tensor([[-23.9251, -26.4604, -23.3999,  ..., -24.7806, -23.2580, -25.5580],\n",
            "        [-23.0141, -21.2463, -25.0389,  ..., -22.0174, -21.9882, -21.5739],\n",
            "        [-24.6321, -23.1983, -21.6248,  ..., -25.0260, -22.2561, -23.5498],\n",
            "        ...,\n",
            "        [-18.0601, -21.8230, -20.9833,  ..., -22.7391, -22.1140, -20.5863],\n",
            "        [-27.2732, -26.3799, -27.2271,  ..., -27.6536, -29.7110, -27.6801],\n",
            "        [-20.0072, -20.3768, -19.4939,  ..., -22.4090, -18.1519, -19.9816]]) tensor([[23061],\n",
            "        [14453],\n",
            "        [36599],\n",
            "        [21993],\n",
            "        [10365],\n",
            "        [ 2368],\n",
            "        [21801],\n",
            "        [17063],\n",
            "        [36658],\n",
            "        [29580],\n",
            "        [12175],\n",
            "        [ 2255],\n",
            "        [ 2368],\n",
            "        [23546],\n",
            "        [17334],\n",
            "        [12633]])\n",
            "tensor([[-25.4748, -26.5427, -27.1780,  ..., -27.1525, -24.6817, -26.0444],\n",
            "        [-30.8935, -31.4276, -27.4755,  ..., -28.2645, -26.8714, -25.9131],\n",
            "        [-23.7893, -24.2808, -22.2462,  ..., -23.5575, -22.7000, -24.4836],\n",
            "        ...,\n",
            "        [-17.4253, -17.4078, -18.3645,  ..., -19.4717, -17.3446, -16.5510],\n",
            "        [-22.6985, -21.0865, -20.3655,  ..., -22.4809, -21.6454, -19.2285],\n",
            "        [-38.2918, -35.1193, -35.5299,  ..., -38.1122, -36.0024, -36.3661]]) tensor([[23546],\n",
            "        [26257],\n",
            "        [31495],\n",
            "        [10365],\n",
            "        [ 2368],\n",
            "        [12633],\n",
            "        [ 2255],\n",
            "        [16350],\n",
            "        [21238],\n",
            "        [13254],\n",
            "        [30111],\n",
            "        [35967],\n",
            "        [ 2368],\n",
            "        [14333],\n",
            "        [ 9803],\n",
            "        [32845]])\n",
            "tensor([[-14.7386, -15.0736, -15.0146,  ..., -16.6425, -15.8967, -16.7214],\n",
            "        [-17.7323, -22.8240, -18.0374,  ..., -21.7799, -20.8932, -19.1332],\n",
            "        [-20.9177, -21.0294, -20.8130,  ..., -21.6591, -20.7735, -19.2337],\n",
            "        ...,\n",
            "        [-22.6313, -22.4904, -23.1743,  ..., -23.7440, -22.0802, -23.2385],\n",
            "        [-16.1056, -17.3802, -17.0232,  ..., -19.6471, -16.4019, -18.2811],\n",
            "        [-20.5504, -20.5832, -21.1320,  ..., -21.6218, -20.2498, -17.7065]]) tensor([[  736],\n",
            "        [18258],\n",
            "        [19761],\n",
            "        [31216],\n",
            "        [26257],\n",
            "        [ 9917],\n",
            "        [14809],\n",
            "        [14756],\n",
            "        [27494],\n",
            "        [18737],\n",
            "        [ 4001],\n",
            "        [35053],\n",
            "        [33722],\n",
            "        [ 5450],\n",
            "        [16350],\n",
            "        [17211]])\n",
            "tensor([[-25.6929, -24.8296, -24.0851,  ..., -27.7803, -26.7215, -24.8348],\n",
            "        [-18.8995, -19.3130, -19.4803,  ..., -18.6892, -20.5761, -18.2009],\n",
            "        [-23.1268, -21.2430, -21.2345,  ..., -21.9693, -23.3953, -23.1746],\n",
            "        ...,\n",
            "        [-17.5433, -18.5138, -18.9342,  ..., -17.9688, -18.7570, -18.2167],\n",
            "        [-22.2759, -21.9879, -21.1209,  ..., -20.0283, -20.1942, -21.9635],\n",
            "        [-19.5552, -19.5764, -18.0389,  ..., -20.7303, -17.8050, -16.7212]]) tensor([[12156],\n",
            "        [18670],\n",
            "        [23061],\n",
            "        [17583],\n",
            "        [36084],\n",
            "        [ 6177],\n",
            "        [32195],\n",
            "        [15656],\n",
            "        [14581],\n",
            "        [ 3384],\n",
            "        [ 5899],\n",
            "        [ 7019],\n",
            "        [12049],\n",
            "        [ 6792],\n",
            "        [28074],\n",
            "        [17464]])\n",
            "tensor([[-23.5783, -24.5795, -22.1713,  ..., -24.7341, -22.4569, -22.6995],\n",
            "        [-22.7189, -22.6925, -22.3919,  ..., -22.7991, -20.2191, -19.3080],\n",
            "        [-19.0421, -22.5470, -18.8510,  ..., -20.8280, -18.1219, -17.5245],\n",
            "        ...,\n",
            "        [-16.5642, -19.3203, -17.4873,  ..., -21.1524, -18.6860, -18.9868],\n",
            "        [-17.4047, -16.2119, -16.6686,  ..., -15.4285, -16.0508, -15.8008],\n",
            "        [-22.8359, -25.0045, -25.8811,  ..., -23.7894, -23.1120, -25.1506]]) tensor([[ 5086],\n",
            "        [28852],\n",
            "        [25231],\n",
            "        [28074],\n",
            "        [17811],\n",
            "        [30845],\n",
            "        [ 6448],\n",
            "        [11004],\n",
            "        [25032],\n",
            "        [33694],\n",
            "        [11443],\n",
            "        [ 1276],\n",
            "        [30845],\n",
            "        [28074],\n",
            "        [ 1887],\n",
            "        [ 2255]])\n",
            "tensor([[-19.9246, -20.8020, -20.0383,  ..., -21.3510, -19.5595, -17.5421],\n",
            "        [-17.4174, -21.7220, -18.0983,  ..., -20.8909, -18.1414, -18.2943],\n",
            "        [-28.8661, -28.4879, -30.6547,  ..., -30.5082, -28.2081, -29.4450],\n",
            "        ...,\n",
            "        [-22.9179, -21.8407, -22.9077,  ..., -23.9549, -22.4480, -19.4103],\n",
            "        [-20.0056, -20.7730, -18.4489,  ..., -19.7628, -17.7314, -16.9670],\n",
            "        [-21.4587, -24.7738, -21.3109,  ..., -22.8839, -21.3410, -22.2025]]) tensor([[34843],\n",
            "        [ 5772],\n",
            "        [ 5899],\n",
            "        [ 7019],\n",
            "        [12049],\n",
            "        [25780],\n",
            "        [17211],\n",
            "        [ 1887],\n",
            "        [14363],\n",
            "        [28074],\n",
            "        [16544],\n",
            "        [14756],\n",
            "        [17583],\n",
            "        [25000],\n",
            "        [25276],\n",
            "        [ 5899]])\n",
            "tensor([[-21.1688, -22.7764, -21.1513,  ..., -21.8768, -21.2783, -20.8937],\n",
            "        [-26.0940, -23.2373, -20.2839,  ..., -22.1375, -22.0869, -20.4903],\n",
            "        [-17.1453, -19.5535, -20.0168,  ..., -20.7397, -20.0200, -20.3709],\n",
            "        ...,\n",
            "        [-17.7235, -20.3402, -20.1919,  ..., -21.2882, -19.7239, -21.4226],\n",
            "        [-20.5940, -19.6782, -21.4763,  ..., -22.9242, -20.6656, -19.6137],\n",
            "        [-32.0151, -30.6039, -29.5736,  ..., -28.4335, -27.4857, -27.3002]]) tensor([[ 7019],\n",
            "        [27367],\n",
            "        [14304],\n",
            "        [ 1788],\n",
            "        [32940],\n",
            "        [33893],\n",
            "        [30264],\n",
            "        [12049],\n",
            "        [31017],\n",
            "        [16761],\n",
            "        [ 2255],\n",
            "        [12049],\n",
            "        [29892],\n",
            "        [ 6072],\n",
            "        [18783],\n",
            "        [14807]])\n",
            "tensor([[-18.5119, -18.8648, -17.3103,  ..., -19.9222, -18.0886, -19.2656],\n",
            "        [-18.2012, -20.4249, -19.2030,  ..., -20.2808, -19.5266, -18.2895],\n",
            "        [-21.3286, -20.0614, -20.6936,  ..., -22.3063, -20.0864, -19.9310],\n",
            "        ...,\n",
            "        [-15.5006, -19.3446, -18.6159,  ..., -20.7492, -17.1421, -19.4745],\n",
            "        [-24.9339, -24.2896, -24.9158,  ..., -24.0534, -22.8758, -24.1916],\n",
            "        [-17.4087, -17.4204, -20.0699,  ..., -17.0797, -18.0625, -17.5971]]) tensor([[36732],\n",
            "        [ 5418],\n",
            "        [ 4005],\n",
            "        [14807],\n",
            "        [36732],\n",
            "        [ 9370],\n",
            "        [34843],\n",
            "        [ 6072],\n",
            "        [ 3516],\n",
            "        [28043],\n",
            "        [ 2628],\n",
            "        [24971],\n",
            "        [24971],\n",
            "        [14756],\n",
            "        [ 1887],\n",
            "        [14733]])\n",
            "tensor([[-20.7609, -19.6448, -19.5406,  ..., -22.4907, -19.6533, -18.2791],\n",
            "        [-22.0966, -20.2469, -18.2304,  ..., -22.3602, -18.9298, -17.8463],\n",
            "        [-22.9271, -26.2631, -22.5242,  ..., -25.1749, -24.1213, -24.6897],\n",
            "        ...,\n",
            "        [-34.7533, -37.0693, -35.7952,  ..., -36.0884, -33.6837, -36.5270],\n",
            "        [-20.9527, -23.3757, -21.9187,  ..., -22.1332, -19.4980, -21.8853],\n",
            "        [-23.8808, -23.3978, -19.9241,  ..., -23.7654, -21.8408, -25.8590]]) tensor([[29437],\n",
            "        [12700],\n",
            "        [22492],\n",
            "        [17480],\n",
            "        [ 7721],\n",
            "        [22492],\n",
            "        [24491],\n",
            "        [17480],\n",
            "        [29613],\n",
            "        [31621],\n",
            "        [31495],\n",
            "        [18170],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [11263],\n",
            "        [33274]])\n",
            "tensor([[-23.3518, -22.5673, -20.9891,  ..., -26.4600, -20.5708, -20.9383],\n",
            "        [-19.9212, -19.6517, -17.8941,  ..., -20.3932, -19.2052, -21.5161],\n",
            "        [-22.9782, -28.6883, -23.2913,  ..., -29.3059, -24.5216, -25.5446],\n",
            "        ...,\n",
            "        [-21.3597, -23.5700, -23.6200,  ..., -22.6936, -19.0909, -23.3742],\n",
            "        [-20.9721, -20.2702, -19.8978,  ..., -21.0485, -22.8366, -21.4265],\n",
            "        [-17.2111, -17.1459, -17.2464,  ..., -18.5740, -17.4625, -16.7898]]) tensor([[18797],\n",
            "        [ 7702],\n",
            "        [10365],\n",
            "        [ 2368],\n",
            "        [13254],\n",
            "        [ 2255],\n",
            "        [ 2368],\n",
            "        [11902],\n",
            "        [11676],\n",
            "        [25854],\n",
            "        [35180],\n",
            "        [25141],\n",
            "        [23546],\n",
            "        [23949],\n",
            "        [ 7721],\n",
            "        [31352]])\n",
            "tensor([[-16.7289, -17.2211, -18.9048,  ..., -17.9478, -16.3588, -15.9565],\n",
            "        [-25.3969, -25.2310, -24.5166,  ..., -24.6966, -23.0600, -25.9236],\n",
            "        [-27.4742, -25.7652, -26.1120,  ..., -26.8176, -25.4662, -24.1767],\n",
            "        ...,\n",
            "        [-26.7858, -23.0046, -22.3304,  ..., -27.3503, -24.4277, -24.7378],\n",
            "        [-21.9116, -20.1652, -18.4770,  ..., -20.6098, -19.6348, -19.8125],\n",
            "        [-30.2288, -32.1298, -30.5593,  ..., -33.5698, -28.5136, -29.6596]]) tensor([[23300],\n",
            "        [33828],\n",
            "        [11902],\n",
            "        [ 5782],\n",
            "        [ 9917],\n",
            "        [14809],\n",
            "        [24323],\n",
            "        [36732],\n",
            "        [21230],\n",
            "        [26845],\n",
            "        [14809],\n",
            "        [14108],\n",
            "        [32587],\n",
            "        [ 6822],\n",
            "        [24861],\n",
            "        [36281]])\n",
            "tensor([[-30.0639, -31.5532, -29.9552,  ..., -33.3989, -29.4684, -29.4407],\n",
            "        [-19.2304, -22.3438, -17.5508,  ..., -20.8045, -18.7003, -19.2102],\n",
            "        [-23.2140, -22.4860, -21.2925,  ..., -23.9430, -25.1908, -22.6232],\n",
            "        ...,\n",
            "        [-14.3108, -17.0056, -17.6209,  ..., -17.7836, -14.8741, -16.3608],\n",
            "        [-19.8408, -23.1321, -20.1690,  ..., -22.1050, -18.6127, -22.6855],\n",
            "        [-25.0690, -26.7949, -26.3300,  ..., -28.0992, -25.6431, -25.8318]]) tensor([[21214],\n",
            "        [31225],\n",
            "        [ 3638],\n",
            "        [31225],\n",
            "        [27652],\n",
            "        [ 3638],\n",
            "        [22789],\n",
            "        [31225],\n",
            "        [33828],\n",
            "        [ 9380],\n",
            "        [34010],\n",
            "        [19539],\n",
            "        [16486],\n",
            "        [25141],\n",
            "        [ 9803],\n",
            "        [25032]])\n",
            "tensor([[-30.3565, -29.6316, -30.1755,  ..., -29.5904, -30.4908, -30.3350],\n",
            "        [-19.7577, -21.9054, -21.3914,  ..., -22.3587, -20.9469, -22.1196],\n",
            "        [-24.6101, -23.1269, -23.2318,  ..., -24.6823, -21.5862, -18.8128],\n",
            "        ...,\n",
            "        [-16.7055, -18.5296, -16.9111,  ..., -18.9333, -17.1812, -16.2158],\n",
            "        [-27.1919, -32.1565, -27.8540,  ..., -29.4529, -26.8850, -25.3318],\n",
            "        [-23.1547, -24.6656, -24.8472,  ..., -25.2450, -24.9919, -25.3171]]) tensor([[12199],\n",
            "        [ 6438],\n",
            "        [14108],\n",
            "        [ 1488],\n",
            "        [26845],\n",
            "        [ 8175],\n",
            "        [ 6180],\n",
            "        [  504],\n",
            "        [ 2811],\n",
            "        [24923],\n",
            "        [13945],\n",
            "        [ 6448],\n",
            "        [ 6448],\n",
            "        [25854],\n",
            "        [10417],\n",
            "        [27248]])\n",
            "tensor([[-18.3649, -20.9983, -16.8472,  ..., -19.0617, -19.8988, -19.5942],\n",
            "        [-18.1826, -19.5021, -19.3536,  ..., -16.5972, -16.3051, -17.6647],\n",
            "        [-17.9882, -19.6742, -19.8049,  ..., -20.7960, -20.3467, -19.4278],\n",
            "        ...,\n",
            "        [-20.7946, -22.1854, -19.2212,  ..., -22.0641, -21.7341, -19.4269],\n",
            "        [-21.3406, -23.5415, -21.4778,  ..., -22.1229, -20.5909, -19.9636],\n",
            "        [-22.7388, -20.9857, -20.6721,  ..., -21.4143, -20.6792, -18.1692]]) tensor([[27690],\n",
            "        [11902],\n",
            "        [32229],\n",
            "        [30760],\n",
            "        [24323],\n",
            "        [ 6822],\n",
            "        [14108],\n",
            "        [29875],\n",
            "        [26845],\n",
            "        [12199],\n",
            "        [ 6438],\n",
            "        [32762],\n",
            "        [33519],\n",
            "        [34125],\n",
            "        [ 2186],\n",
            "        [21631]])\n",
            "tensor([[-20.8815, -23.6892, -20.9416,  ..., -23.2533, -21.6446, -20.0298],\n",
            "        [-25.8782, -27.2023, -24.3474,  ..., -27.9269, -25.9249, -23.7998],\n",
            "        [-20.1676, -23.1558, -22.5540,  ..., -22.0363, -22.6981, -22.8776],\n",
            "        ...,\n",
            "        [-18.7639, -20.3001, -18.9751,  ..., -20.0184, -17.6336, -20.6666],\n",
            "        [-19.0834, -19.4643, -18.7624,  ..., -20.6356, -18.6557, -18.3446],\n",
            "        [-19.1559, -19.4679, -20.8049,  ..., -19.2644, -17.5647, -19.9873]]) tensor([[ 4583],\n",
            "        [30946],\n",
            "        [13304],\n",
            "        [16897],\n",
            "        [34907],\n",
            "        [31256],\n",
            "        [32706],\n",
            "        [24491],\n",
            "        [ 4931],\n",
            "        [32940],\n",
            "        [ 5603],\n",
            "        [32940],\n",
            "        [ 6822],\n",
            "        [27339],\n",
            "        [11855],\n",
            "        [18170]])\n",
            "tensor([[-26.2545, -26.4394, -24.9352,  ..., -26.6192, -26.0959, -27.3135],\n",
            "        [-24.5178, -27.2106, -27.0234,  ..., -31.4037, -24.7710, -28.3933],\n",
            "        [-31.0575, -34.0143, -35.8964,  ..., -31.8671, -30.4463, -32.5986],\n",
            "        ...,\n",
            "        [-24.6396, -25.4359, -22.6551,  ..., -24.0704, -24.0273, -24.4496],\n",
            "        [-22.3915, -22.8601, -24.3224,  ..., -24.0246, -23.0936, -22.5057],\n",
            "        [-17.5475, -20.5284, -17.4397,  ..., -19.6689, -18.9441, -19.0426]]) tensor([[17211],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 9902],\n",
            "        [23271],\n",
            "        [26253],\n",
            "        [17211],\n",
            "        [36281],\n",
            "        [21214],\n",
            "        [ 1433],\n",
            "        [12309],\n",
            "        [ 2691],\n",
            "        [34848],\n",
            "        [20951],\n",
            "        [11676],\n",
            "        [22816]])\n",
            "tensor([[-28.2694, -27.8424, -26.8190,  ..., -29.6653, -26.6116, -22.7777],\n",
            "        [-19.9317, -22.4261, -17.4075,  ..., -19.0884, -19.7989, -18.4173],\n",
            "        [-19.8409, -21.1795, -19.9983,  ..., -22.3340, -21.8439, -20.5232],\n",
            "        ...,\n",
            "        [-21.6873, -23.3689, -21.2318,  ..., -19.9353, -19.9477, -19.9951],\n",
            "        [-26.5060, -27.5546, -25.1214,  ..., -26.3053, -25.1061, -26.9311],\n",
            "        [-20.9250, -19.3613, -20.5558,  ..., -19.2878, -19.3772, -19.2874]]) tensor([[31017],\n",
            "        [32626],\n",
            "        [29892],\n",
            "        [25141],\n",
            "        [ 9803],\n",
            "        [25032],\n",
            "        [31375],\n",
            "        [13753],\n",
            "        [25032],\n",
            "        [28160],\n",
            "        [ 6800],\n",
            "        [11676],\n",
            "        [36708],\n",
            "        [24762],\n",
            "        [ 8276],\n",
            "        [16062]])\n",
            "tensor([[-23.6704, -28.2457, -22.1972,  ..., -25.1726, -22.5997, -26.3316],\n",
            "        [-22.0184, -24.8064, -21.9425,  ..., -24.7684, -23.3912, -23.1853],\n",
            "        [-23.9496, -21.8765, -21.6130,  ..., -21.3197, -22.4020, -21.7139],\n",
            "        ...,\n",
            "        [-17.5749, -19.0168, -16.9732,  ..., -21.9364, -18.8516, -18.7208],\n",
            "        [-15.3684, -16.9511, -14.3042,  ..., -17.2255, -16.2240, -17.3964],\n",
            "        [-18.2501, -20.8407, -20.8166,  ..., -17.8827, -18.2423, -18.5013]]) tensor([[ 4540],\n",
            "        [ 8276],\n",
            "        [24187],\n",
            "        [22944],\n",
            "        [22944],\n",
            "        [ 6822],\n",
            "        [33694],\n",
            "        [35040],\n",
            "        [24594],\n",
            "        [10461],\n",
            "        [25032],\n",
            "        [26196],\n",
            "        [ 6924],\n",
            "        [10461],\n",
            "        [15103],\n",
            "        [22944]])\n",
            "tensor([[-20.8554, -22.2987, -22.0916,  ..., -20.9695, -23.3588, -21.8953],\n",
            "        [-17.5389, -22.0099, -19.9966,  ..., -23.2082, -19.8057, -18.9742],\n",
            "        [-29.8763, -31.0927, -28.2026,  ..., -32.0150, -25.6255, -29.5434],\n",
            "        ...,\n",
            "        [-18.6714, -16.2718, -16.4405,  ..., -17.5176, -18.2402, -16.2829],\n",
            "        [-20.2568, -21.0139, -19.4126,  ..., -22.8234, -20.1375, -22.3995],\n",
            "        [-20.9675, -20.8556, -21.7289,  ..., -23.8901, -21.4809, -20.0772]]) tensor([[17811],\n",
            "        [ 6822],\n",
            "        [ 7381],\n",
            "        [ 6871],\n",
            "        [ 3638],\n",
            "        [31225],\n",
            "        [25254],\n",
            "        [22944],\n",
            "        [ 6822],\n",
            "        [  801],\n",
            "        [ 7702],\n",
            "        [ 7702],\n",
            "        [30007],\n",
            "        [ 3669],\n",
            "        [  736],\n",
            "        [29580]])\n",
            "tensor([[-20.7923, -21.0635, -23.0067,  ..., -24.8998, -19.7783, -22.0058],\n",
            "        [-18.7182, -17.2233, -17.3215,  ..., -18.6321, -19.2139, -17.5414],\n",
            "        [-18.4942, -19.1841, -18.3081,  ..., -19.8570, -19.7861, -19.8533],\n",
            "        ...,\n",
            "        [-20.5633, -22.0814, -22.1568,  ..., -21.1738, -19.5431, -21.9359],\n",
            "        [-22.1577, -21.2592, -21.3967,  ..., -25.6477, -22.1079, -21.4803],\n",
            "        [-22.1601, -22.1222, -23.1725,  ..., -25.7332, -21.6202, -22.3024]]) tensor([[ 2255],\n",
            "        [21063],\n",
            "        [10826],\n",
            "        [33726],\n",
            "        [ 3993],\n",
            "        [35083],\n",
            "        [32174],\n",
            "        [11760],\n",
            "        [29580],\n",
            "        [29074],\n",
            "        [24236],\n",
            "        [18170],\n",
            "        [15103],\n",
            "        [25241],\n",
            "        [15918],\n",
            "        [35918]])\n",
            "tensor([[-17.6992, -21.3145, -18.9825,  ..., -23.3592, -17.6992, -20.2493],\n",
            "        [-17.3996, -21.3079, -18.3398,  ..., -20.2371, -17.4177, -17.8450],\n",
            "        [-18.5585, -21.7403, -20.3367,  ..., -22.5521, -17.5274, -17.3905],\n",
            "        ...,\n",
            "        [-19.1347, -21.3246, -21.8116,  ..., -23.0228, -20.9815, -18.0920],\n",
            "        [-27.3493, -27.8146, -26.4562,  ..., -30.5240, -26.3114, -28.9030],\n",
            "        [-21.1153, -19.9906, -22.1579,  ..., -23.3160, -20.1678, -21.9077]]) tensor([[24236],\n",
            "        [ 3668],\n",
            "        [ 2255],\n",
            "        [ 2255],\n",
            "        [19951],\n",
            "        [ 2368],\n",
            "        [ 2410],\n",
            "        [26397],\n",
            "        [17211],\n",
            "        [ 1887],\n",
            "        [35060],\n",
            "        [ 9124],\n",
            "        [11902],\n",
            "        [ 9007],\n",
            "        [19785],\n",
            "        [21298]])\n",
            "tensor([[-21.1337, -20.4789, -26.2287,  ..., -19.5689, -23.0196, -22.3806],\n",
            "        [-23.6704, -25.5293, -23.8045,  ..., -27.1327, -22.4282, -23.3997],\n",
            "        [-25.2739, -21.5917, -20.3046,  ..., -23.5669, -21.9821, -21.0904],\n",
            "        ...,\n",
            "        [-23.0832, -22.7385, -21.3533,  ..., -22.1002, -23.4796, -22.3482],\n",
            "        [-19.2515, -20.7349, -20.2699,  ..., -21.4185, -19.3096, -16.8522],\n",
            "        [-22.6531, -22.2520, -20.4494,  ..., -22.4947, -20.1984, -19.7373]]) tensor([[ 6836],\n",
            "        [15176],\n",
            "        [ 1887],\n",
            "        [ 9124],\n",
            "        [ 5703],\n",
            "        [36117],\n",
            "        [23546],\n",
            "        [12370],\n",
            "        [29161],\n",
            "        [18666],\n",
            "        [25076],\n",
            "        [ 8940],\n",
            "        [ 2255],\n",
            "        [ 5639],\n",
            "        [ 7061],\n",
            "        [ 3669]])\n",
            "tensor([[-23.6859, -20.1711, -21.0459,  ..., -22.6386, -22.9324, -20.4338],\n",
            "        [-20.7604, -21.8139, -18.9280,  ..., -22.0960, -20.3758, -20.4800],\n",
            "        [-23.9333, -24.0762, -24.3572,  ..., -25.6852, -23.2616, -25.5754],\n",
            "        ...,\n",
            "        [-25.0461, -26.8996, -23.5635,  ..., -26.3090, -25.5636, -23.0146],\n",
            "        [-24.0270, -25.8438, -24.6451,  ..., -25.5699, -23.6716, -23.2544],\n",
            "        [-22.9700, -20.4304, -21.0292,  ..., -22.7402, -22.5648, -19.5022]]) tensor([[21904],\n",
            "        [ 8641],\n",
            "        [23970],\n",
            "        [13193],\n",
            "        [26397],\n",
            "        [ 3539],\n",
            "        [ 1887],\n",
            "        [30264],\n",
            "        [ 8940],\n",
            "        [ 2255],\n",
            "        [15383],\n",
            "        [34101],\n",
            "        [32354],\n",
            "        [13635],\n",
            "        [27215],\n",
            "        [15341]])\n",
            "tensor([[-23.1684, -24.3548, -23.1004,  ..., -23.5709, -23.3604, -25.3111],\n",
            "        [-18.8881, -20.4696, -17.2131,  ..., -18.1682, -18.2902, -18.2646],\n",
            "        [-29.2494, -30.7822, -30.1874,  ..., -33.3689, -29.1888, -27.6883],\n",
            "        ...,\n",
            "        [-16.1058, -18.9238, -18.2729,  ..., -21.8515, -19.5417, -21.7717],\n",
            "        [-22.1947, -25.1531, -22.6518,  ..., -27.0773, -19.2557, -21.2636],\n",
            "        [-19.4276, -15.8711, -16.9985,  ..., -17.9466, -18.3000, -15.1110]]) tensor([[21993],\n",
            "        [  957],\n",
            "        [15383],\n",
            "        [22257],\n",
            "        [34721],\n",
            "        [30534],\n",
            "        [21993],\n",
            "        [22168],\n",
            "        [32885],\n",
            "        [21089],\n",
            "        [  887],\n",
            "        [ 2255],\n",
            "        [33127],\n",
            "        [31889],\n",
            "        [11902],\n",
            "        [ 5735]])\n",
            "tensor([[-25.5045, -22.4539, -19.9822,  ..., -23.5131, -20.6973, -23.3479],\n",
            "        [-26.9517, -26.7384, -27.0200,  ..., -27.8488, -25.5875, -25.5738],\n",
            "        [-19.9432, -18.7828, -15.6476,  ..., -20.4689, -18.7660, -17.8637],\n",
            "        ...,\n",
            "        [-23.1035, -21.6235, -22.1962,  ..., -24.8068, -21.8425, -21.2125],\n",
            "        [-20.9859, -20.6371, -19.6045,  ..., -19.4380, -20.9318, -19.2756],\n",
            "        [-22.6519, -22.0758, -21.6509,  ..., -25.4768, -22.0443, -22.8469]]) tensor([[ 9089],\n",
            "        [ 6893],\n",
            "        [  168],\n",
            "        [ 8176],\n",
            "        [27515],\n",
            "        [ 4703],\n",
            "        [34821],\n",
            "        [ 4703],\n",
            "        [ 6516],\n",
            "        [ 6086],\n",
            "        [21993],\n",
            "        [18771],\n",
            "        [15035],\n",
            "        [27515],\n",
            "        [31946],\n",
            "        [  736]])\n",
            "tensor([[-19.8427, -21.1753, -17.4527,  ..., -21.1099, -18.1662, -18.0942],\n",
            "        [-21.0164, -19.3361, -18.2544,  ..., -19.9438, -19.1844, -19.2538],\n",
            "        [-27.0744, -25.0653, -28.5167,  ..., -25.7307, -25.1670, -26.2201],\n",
            "        ...,\n",
            "        [-24.4635, -25.9612, -27.1782,  ..., -27.2417, -21.6297, -24.9682],\n",
            "        [-23.1780, -24.4341, -26.2522,  ..., -24.8777, -21.1095, -22.7384],\n",
            "        [-24.4537, -25.8987, -25.1843,  ..., -28.1072, -22.8915, -23.1026]]) tensor([[11676],\n",
            "        [ 9917],\n",
            "        [22789],\n",
            "        [34010],\n",
            "        [36732],\n",
            "        [30814],\n",
            "        [12222],\n",
            "        [29081],\n",
            "        [25854],\n",
            "        [21230],\n",
            "        [14707],\n",
            "        [22704],\n",
            "        [ 6871],\n",
            "        [ 9714],\n",
            "        [34010],\n",
            "        [11443]])\n",
            "tensor([[-18.7228, -22.8993, -21.1406,  ..., -22.1308, -17.4824, -18.8057],\n",
            "        [-19.4953, -18.1567, -16.9459,  ..., -17.7507, -17.7533, -16.5266],\n",
            "        [-18.7521, -16.4096, -14.7461,  ..., -17.4429, -17.8929, -17.9626],\n",
            "        ...,\n",
            "        [-27.2329, -28.3366, -27.8519,  ..., -28.6178, -25.2433, -25.7579],\n",
            "        [-21.4679, -18.5644, -21.6164,  ..., -22.6333, -21.7859, -19.9733],\n",
            "        [-26.6753, -29.4681, -26.2957,  ..., -30.6782, -26.1306, -28.3326]]) tensor([[28485],\n",
            "        [ 9440],\n",
            "        [17811],\n",
            "        [23061],\n",
            "        [18170],\n",
            "        [ 7545],\n",
            "        [27928],\n",
            "        [ 1801],\n",
            "        [14108],\n",
            "        [ 2041],\n",
            "        [ 6566],\n",
            "        [33079],\n",
            "        [16442],\n",
            "        [11902],\n",
            "        [13971],\n",
            "        [ 9794]])\n",
            "tensor([[-24.2738, -23.3043, -21.7864,  ..., -22.2714, -23.6258, -22.9251],\n",
            "        [-23.9337, -24.4741, -25.2286,  ..., -26.9169, -24.0091, -25.4174],\n",
            "        [-27.2183, -28.4493, -28.0786,  ..., -26.6558, -27.5332, -28.3133],\n",
            "        ...,\n",
            "        [-22.2959, -22.2224, -21.2429,  ..., -24.1456, -19.9092, -21.4591],\n",
            "        [-23.7871, -23.7462, -20.8423,  ..., -22.6313, -19.6256, -21.1577],\n",
            "        [-18.6171, -18.9141, -17.9135,  ..., -21.3766, -17.9859, -19.3276]]) tensor([[ 6275],\n",
            "        [35967],\n",
            "        [35686],\n",
            "        [26397],\n",
            "        [ 9902],\n",
            "        [ 1738],\n",
            "        [17413],\n",
            "        [14535],\n",
            "        [36799],\n",
            "        [35031],\n",
            "        [ 2368],\n",
            "        [35104],\n",
            "        [31352],\n",
            "        [35031],\n",
            "        [ 4411],\n",
            "        [ 6275]])\n",
            "tensor([[-21.4883, -22.0407, -21.7715,  ..., -24.0467, -19.5080, -20.3758],\n",
            "        [-21.1670, -22.5133, -24.0410,  ..., -22.6805, -19.6973, -24.8145],\n",
            "        [-23.7975, -25.9730, -24.0651,  ..., -27.5116, -24.4379, -23.7393],\n",
            "        ...,\n",
            "        [-33.7860, -33.3262, -32.9293,  ..., -32.0084, -31.7842, -33.3123],\n",
            "        [-20.0701, -23.6960, -21.5920,  ..., -23.5481, -20.4321, -22.5292],\n",
            "        [-19.7415, -20.6493, -20.9525,  ..., -20.9575, -17.6606, -19.8907]]) tensor([[ 5972],\n",
            "        [ 5237],\n",
            "        [34616],\n",
            "        [12486],\n",
            "        [30729],\n",
            "        [ 9471],\n",
            "        [21737],\n",
            "        [23061],\n",
            "        [10860],\n",
            "        [13052],\n",
            "        [21561],\n",
            "        [35104],\n",
            "        [21628],\n",
            "        [ 1728],\n",
            "        [35104],\n",
            "        [ 2549]])\n",
            "tensor([[-18.8349, -22.7480, -20.5588,  ..., -21.4762, -21.1630, -22.2870],\n",
            "        [-17.5025, -21.5931, -20.9557,  ..., -20.6307, -19.3315, -16.1296],\n",
            "        [-22.1315, -20.1764, -18.3032,  ..., -18.3122, -21.4013, -19.8884],\n",
            "        ...,\n",
            "        [-30.3415, -33.7165, -32.1718,  ..., -34.1474, -32.6565, -31.0281],\n",
            "        [-19.5300, -19.8302, -20.0893,  ..., -21.8899, -21.2131, -21.2286],\n",
            "        [-17.8060, -16.6897, -18.4033,  ..., -18.2033, -18.4400, -17.9019]]) tensor([[16350],\n",
            "        [ 9440],\n",
            "        [  403],\n",
            "        [ 3088],\n",
            "        [17413],\n",
            "        [34010],\n",
            "        [ 6778],\n",
            "        [ 2255],\n",
            "        [21319],\n",
            "        [17583],\n",
            "        [21379],\n",
            "        [17211],\n",
            "        [ 6924],\n",
            "        [11112],\n",
            "        [12049],\n",
            "        [ 3996]])\n",
            "tensor([[-19.2209, -19.5053, -18.6433,  ..., -22.4121, -20.0750, -19.1304],\n",
            "        [-17.8104, -20.9056, -19.8240,  ..., -19.9718, -18.9375, -17.4017],\n",
            "        [-20.5001, -21.0696, -21.9478,  ..., -19.9958, -18.8435, -19.5772],\n",
            "        ...,\n",
            "        [-21.8434, -19.4126, -19.3343,  ..., -20.8382, -19.0891, -19.1538],\n",
            "        [-25.8034, -23.0088, -22.8778,  ..., -23.5068, -23.4364, -23.3695],\n",
            "        [-22.2134, -25.7258, -23.2682,  ..., -24.5054, -21.9380, -21.3973]]) tensor([[36097],\n",
            "        [13542],\n",
            "        [ 3827],\n",
            "        [19653],\n",
            "        [12049],\n",
            "        [27928],\n",
            "        [30593],\n",
            "        [11112],\n",
            "        [12049],\n",
            "        [26368],\n",
            "        [14813],\n",
            "        [18569],\n",
            "        [21298],\n",
            "        [ 9440],\n",
            "        [ 9665],\n",
            "        [26368]])\n",
            "tensor([[-16.8748, -20.0594, -20.5213,  ..., -18.9047, -19.4584, -18.1568],\n",
            "        [-29.9635, -27.4372, -28.7395,  ..., -29.8475, -27.0524, -28.0831],\n",
            "        [-16.6338, -17.8048, -16.3116,  ..., -17.2017, -15.1107, -13.3630],\n",
            "        ...,\n",
            "        [-19.1639, -22.5654, -18.4243,  ..., -21.5953, -19.2378, -17.7979],\n",
            "        [-23.8968, -22.9385, -21.8444,  ..., -23.4679, -20.2603, -21.5109],\n",
            "        [-22.4496, -23.3822, -20.5845,  ..., -24.6201, -21.6804, -22.3178]]) tensor([[ 2252],\n",
            "        [18170],\n",
            "        [11112],\n",
            "        [12049],\n",
            "        [24179],\n",
            "        [21089],\n",
            "        [  887],\n",
            "        [11786],\n",
            "        [29437],\n",
            "        [ 7499],\n",
            "        [34053],\n",
            "        [36529],\n",
            "        [14948],\n",
            "        [26143],\n",
            "        [24901],\n",
            "        [ 9227]])\n",
            "tensor([[-28.4621, -29.2242, -27.1512,  ..., -29.1727, -25.9535, -27.6973],\n",
            "        [-22.7924, -25.8521, -23.9026,  ..., -27.6002, -23.8822, -24.7038],\n",
            "        [-16.3877, -15.5192, -14.8908,  ..., -17.4335, -15.3253, -14.7298],\n",
            "        ...,\n",
            "        [-27.8716, -26.8717, -30.5587,  ..., -30.5364, -26.7333, -27.8202],\n",
            "        [-21.6350, -20.0997, -22.4149,  ..., -22.2336, -19.9333, -21.1399],\n",
            "        [-26.0713, -28.3032, -31.0816,  ..., -25.4762, -27.6875, -28.3209]]) tensor([[26096],\n",
            "        [30534],\n",
            "        [17809],\n",
            "        [ 9252],\n",
            "        [15272],\n",
            "        [17583],\n",
            "        [36084],\n",
            "        [ 9329],\n",
            "        [36084],\n",
            "        [27232],\n",
            "        [23144],\n",
            "        [24323],\n",
            "        [36732],\n",
            "        [15101],\n",
            "        [26095],\n",
            "        [ 5603]])\n",
            "tensor([[-20.3902, -22.4290, -22.6784,  ..., -23.1595, -20.4477, -22.2076],\n",
            "        [-21.7902, -23.2715, -19.0240,  ..., -23.6361, -21.3311, -19.4973],\n",
            "        [-16.1835, -19.6968, -16.9360,  ..., -18.0034, -16.9258, -17.8878],\n",
            "        ...,\n",
            "        [-21.9348, -24.3378, -24.5460,  ..., -25.7072, -19.1899, -22.0533],\n",
            "        [-16.7360, -16.3556, -15.9536,  ..., -16.3233, -16.2034, -13.8762],\n",
            "        [-26.3120, -28.6996, -23.7713,  ..., -30.2531, -25.7392, -26.8146]]) tensor([[35104],\n",
            "        [36117],\n",
            "        [ 8709],\n",
            "        [ 7499],\n",
            "        [ 5209],\n",
            "        [ 3353],\n",
            "        [ 2255],\n",
            "        [10125],\n",
            "        [24523],\n",
            "        [10444],\n",
            "        [17211],\n",
            "        [28427],\n",
            "        [12318],\n",
            "        [24567],\n",
            "        [ 1437],\n",
            "        [ 1103]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-1356efff86eb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# disable gradient computation, since it is only needed when backward() is called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-e3a69391771e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifSSFzN2o9Sr",
        "outputId": "fe04cc99-067e-4290-8275-3a5cf2403d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(36872, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(df, col, BATCH_SIZE=16):\n",
        "    \"\"\"wrapper function to create all the necessary data\"\"\"\n",
        "    data, word_to_ix, vocab = create_vocab_and_data(df, col, context_size=CONTEXT_SIZE)\n",
        "    # data = data[:100]\n",
        "    split_index= int(len(data)*0.9)\n",
        "    train_X, train_Y = data_to_tensor(data[:split_index], word_to_ix)\n",
        "    test_X, test_Y = data_to_tensor(data[split_index:], word_to_ix)\n",
        "    print(train_X.shape)\n",
        "    print(train_Y.shape)\n",
        "    train_set = MyDataset(train_X, train_Y)\n",
        "    test_set = MyDataset(test_X, test_Y)\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "    return train_loader, test_loader, vocab, word_to_ix"
      ],
      "metadata": {
        "id": "fKMe-VIUo9Tp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainig_loop(model, train_loader, optimizer, loss_func, num_epochs, device):\n",
        "    \"\"\"wrapper function for training for better reusability\"\"\"\n",
        "    model.train()\n",
        "    num_batches = len(train_loader)\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "      for batch_num, (inputs, y_true) in enumerate(train_loader, 1):\n",
        "          optimizer.zero_grad()\n",
        "          #print(inputs.shape, y_true.shape, len(vocab), y_true.squeeze().shape)\n",
        "          y_pred = model(inputs)\n",
        "          loss = loss_func(y_pred, y_true.squeeze())\n",
        "          loss_batch = loss.item()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          print(f'Epoch [{epoch}/{num_epochs}], batch: [{batch_num}/{num_batches}, loss: {loss_batch:.4f}]')\n",
        "\n"
      ],
      "metadata": {
        "id": "74Z51LMB3Mg9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Train CBOW2 with a context width of 2 (in both directions) for the Hotel Reviews dataset."
      ],
      "metadata": {
        "id": "5UEHh3zP9nUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "train_loader, test_loader, vocab, word_to_ix = create_dataloader(df, 'Review')"
      ],
      "metadata": {
        "id": "vHVDR0bq-Cqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cadb42-1df8-4dd3-c0ff-8359cda9c6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([946944, 4])\n",
            "torch.Size([946944, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Train CBOW5 with a context width of 5 (in both directions) for the Hotel Reviews dataset.  \n",
        "\n",
        "ðŸ—’â“ Are predictions made by the model sensitive towards the context size?"
      ],
      "metadata": {
        "id": "qn7teyu7987m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dont run it again\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 128\n",
        "model = CBOW(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "num_epochs = 15\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "model.to(device)\n",
        "trainig_loop(model, train_loader, optimizer, loss_func, num_epochs, device)\n",
        "Modelname = \"CBOW2new.pt\"\n",
        "PATH  =f\"/content/drive/MyDrive/Colab Notebooks/{Modelname}\"\n",
        "\n",
        "# Save\n",
        "torch.save(model, PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JSQ4Uvlr07Xc",
        "outputId": "6c334280-d153-4a6d-f192-5a916addf86e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CBOW' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2328995936b6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONTEXT_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CBOW' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No need to run the code above"
      ],
      "metadata": {
        "id": "U_frXNwfWw4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * 2* embedding_dim, hidden_dim)     # multipy with two because you have a left anfd a right ontext\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=-1)\n",
        "        return log_probs\n",
        "\n",
        "model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/CBOW2.pt\")\n",
        "model.eval()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PXUFRjeHZV1",
        "outputId": "3f80e1be-6996-43d6-b0b3-b7ebe80aadbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b0889e51b265>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/CBOW2.pt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embeddings): Embedding(36872, 50)\n",
              "  (linear1): Linear(in_features=200, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=36872, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVCdGXMGJD--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW2.ptvocab.txt','r') as data:\n",
        "      print(data.read())\n",
        "      data.seek(0)\n",
        "      vocab = eval(data.read())\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW2.ptword_to_ix.txt','r') as data:\n",
        "\n",
        "      data.seek(0)\n",
        "      word_to_ix = eval(data.read())\n",
        "      print(word_to_ix)\n"
      ],
      "metadata": {
        "id": "wOTcWpBtOvXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(word_to_ix['ls'])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccm9zvF6UQWV",
        "outputId": "f0a2ac93-791c-4915-d56c-87dbc12b9277"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "36872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def get_closest_word(word, topn=5):\n",
        "    index_to_word = {value: key for key, value in word_to_ix.items()}   # reversed dictonair to find tokens by their index in the vocab\n",
        "    word_distance = []\n",
        "    model.eval()\n",
        "    emb = model.embeddings\n",
        "    pdist = nn.PairwiseDistance()\n",
        "    i = word_to_ix[word]\n",
        "\n",
        "    lookup_tensor_i = torch.tensor([i], dtype=torch.long).to(device)\n",
        "    #print(lookup_tensor_i)\n",
        "    v_i = emb(lookup_tensor_i)\n",
        "    #print(i, lookup_tensor_i, v_i)\n",
        "    for j in range(1, len(vocab)):\n",
        "      if j != i:\n",
        "          lookup_tensor_j = torch.tensor([j], dtype=torch.long).to(device)\n",
        "          v_j = emb(lookup_tensor_j)\n",
        "          #print(j, lookup_tensor_j, v_j)\n",
        "          word_distance.append((index_to_word[j], float(pdist(v_i, v_j))))\n",
        "    word_distance.sort(key=lambda x: x[1])\n",
        "    return word_distance[:topn]"
      ],
      "metadata": {
        "id": "pnMAbYUI9wtI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_closest_word('hotel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T34g5DNn4Lz-",
        "outputId": "e65a7b11-917c-4cba-f726-82d22e99d1bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('resort', 3.125905752182007),\n",
              " ('rooms', 3.7057602405548096),\n",
              " ('great', 3.832054376602173),\n",
              " ('excellent', 3.9706552028656006),\n",
              " ('room', 4.023690700531006)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebt6Hmb7LFXH",
        "outputId": "ba722c2f-4d11-4e33-f3e2-d51528d0a675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_closest_word('room'))\n",
        "print(get_closest_word('atlantic'))\n",
        "print(get_closest_word('beautiful'))\n",
        "print(get_closest_word('great'))\n",
        "print(get_closest_word('did'))\n",
        "print(get_closest_word('stay'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "EUjn_u6WBrYL",
        "outputId": "22a52a3d-7c9d-4901-91fc-8c48cd3e0abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rooms', 2.089414358139038), ('hotel', 4.023687362670898), ('bathroom', 4.101468086242676), ('suite', 4.250602722167969), ('floor', 4.554897785186768)]\n",
            "[('nb', 5.9741644859313965), ('place', 6.12546443939209), ('notices', 6.195394992828369), ('carme', 6.19688606262207), ('days', 6.261256217956543)]\n",
            "[('nice', 3.7269740104675293), ('amazing', 3.8383333683013916), ('clean', 3.928826332092285), ('great', 4.006716251373291), ('lovely', 4.092239856719971)]\n",
            "[('good', 2.28143310546875), ('excellent', 2.3379077911376953), ('wonderful', 2.5288219451904297), ('nice', 2.9609107971191406), ('lovely', 3.273216485977173)]\n",
            "[('does', 3.630962371826172), ('went', 4.198888301849365), ('not', 4.366249084472656), ('got', 4.486250877380371), ('think', 4.57169246673584)]\n",
            "[('visit', 4.014122486114502), ('think', 4.113770484924316), ('hotel', 4.233322620391846), ('staying', 4.359683990478516), ('experience', 4.486968994140625)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'well-furnished'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-966c044fba16>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'did'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'well-furnished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ineffective'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'copley'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f20b7a507c63>\u001b[0m in \u001b[0;36mget_closest_word\u001b[0;34m(word, topn)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPairwiseDistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlookup_tensor_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'well-furnished'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_closest_word('ineffective'))\n",
        "print(get_closest_word('copley'))\n",
        "print(get_closest_word('cracked'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsA6z6ReITPp",
        "outputId": "968aac21-61a8-4222-8b85-0b8d5375a9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('coordinators', 7.205868721008301), ('cynicism', 7.256261348724365), ('sophisticated', 7.271458625793457), ('deters', 7.288957595825195), ('911', 7.342331409454346)]\n",
            "[('clean', 6.673245906829834), ('diva', 6.845100402832031), ('franglish', 6.883596897125244), ('handwritten', 6.986283779144287), ('recommend', 7.025819778442383)]\n",
            "[('trian', 6.5322346687316895), ('spacous', 6.660488128662109), ('faux', 6.74277400970459), ('crossisants', 6.7488861083984375), ('knocking', 6.758415222167969)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['hotel', 'room', 'copley', 'did', 'stay', 'trian' 'beautiful', 'ineffective', 'cracked']"
      ],
      "metadata": {
        "id": "SEuli591FFkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to figure out what are mor frequent and less frequent words, we could use the index numbers in the vocab. I think the higher the index number the rare more rare, as they only get added if not already in the vocab? but i am not sure as we create a set from it, and they are not ordered\n"
      ],
      "metadata": {
        "id": "2-BGSpVrCH6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 5\n",
        "# train_loader, test_loader, vocab, word_to_ix = create_dataloader(df, 'Review')\n",
        "# Modelname = \"CBOW5.pt\"\n",
        "# PATH  =f\"/content/drive/MyDrive/Colab Notebooks/{Modelname}\"\n",
        "\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW5.ptvocab.txt','r') as data:\n",
        "      print(data.read())\n",
        "      data.seek(0)\n",
        "      vocab = eval(data.read())\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW5.ptword_to_ix.txt','r') as data:\n",
        "\n",
        "      data.seek(0)\n",
        "      word_to_ix = eval(data.read())\n",
        "      print(word_to_ix)\n"
      ],
      "metadata": {
        "id": "IeOX2qC3_3NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce49200c-8525-4fff-b162-5869223130a8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([892950, 10])\n",
            "torch.Size([892950, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 128\n",
        "model = CBOW(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "num_epochs = 15\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "model.to(device)\n",
        "trainig_loop(model, train_loader, optimizer, loss_func, num_epochs, device)\n",
        "\n",
        "Modelname = \"CBOW5new.pt\"\n",
        "PATH  =f\"/content/drive/MyDrive/Colab Notebooks/{Modelname}\"\n",
        "\n",
        "torch.save(model, PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "y0j14nsVAB8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8afab5f5-2568-40d3-a290-23bf85cdf98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [5/15], batch: [51417/55810, loss: 5.1319]\n",
            "Epoch [5/15], batch: [51418/55810, loss: 7.0256]\n",
            "Epoch [5/15], batch: [51419/55810, loss: 6.4952]\n",
            "Epoch [5/15], batch: [51420/55810, loss: 6.2850]\n",
            "Epoch [5/15], batch: [51421/55810, loss: 4.8743]\n",
            "Epoch [5/15], batch: [51422/55810, loss: 5.6820]\n",
            "Epoch [5/15], batch: [51423/55810, loss: 4.5090]\n",
            "Epoch [5/15], batch: [51424/55810, loss: 5.2988]\n",
            "Epoch [5/15], batch: [51425/55810, loss: 6.0163]\n",
            "Epoch [5/15], batch: [51426/55810, loss: 6.9619]\n",
            "Epoch [5/15], batch: [51427/55810, loss: 6.5024]\n",
            "Epoch [5/15], batch: [51428/55810, loss: 5.4902]\n",
            "Epoch [5/15], batch: [51429/55810, loss: 6.0992]\n",
            "Epoch [5/15], batch: [51430/55810, loss: 7.4890]\n",
            "Epoch [5/15], batch: [51431/55810, loss: 7.3463]\n",
            "Epoch [5/15], batch: [51432/55810, loss: 6.4128]\n",
            "Epoch [5/15], batch: [51433/55810, loss: 7.0579]\n",
            "Epoch [5/15], batch: [51434/55810, loss: 7.1029]\n",
            "Epoch [5/15], batch: [51435/55810, loss: 6.1884]\n",
            "Epoch [5/15], batch: [51436/55810, loss: 6.3095]\n",
            "Epoch [5/15], batch: [51437/55810, loss: 8.2047]\n",
            "Epoch [5/15], batch: [51438/55810, loss: 6.7943]\n",
            "Epoch [5/15], batch: [51439/55810, loss: 7.9042]\n",
            "Epoch [5/15], batch: [51440/55810, loss: 7.5113]\n",
            "Epoch [5/15], batch: [51441/55810, loss: 6.2180]\n",
            "Epoch [5/15], batch: [51442/55810, loss: 8.5229]\n",
            "Epoch [5/15], batch: [51443/55810, loss: 8.3467]\n",
            "Epoch [5/15], batch: [51444/55810, loss: 7.2811]\n",
            "Epoch [5/15], batch: [51445/55810, loss: 6.7879]\n",
            "Epoch [5/15], batch: [51446/55810, loss: 6.2824]\n",
            "Epoch [5/15], batch: [51447/55810, loss: 7.1570]\n",
            "Epoch [5/15], batch: [51448/55810, loss: 6.7868]\n",
            "Epoch [5/15], batch: [51449/55810, loss: 8.0376]\n",
            "Epoch [5/15], batch: [51450/55810, loss: 5.5464]\n",
            "Epoch [5/15], batch: [51451/55810, loss: 6.9193]\n",
            "Epoch [5/15], batch: [51452/55810, loss: 5.6422]\n",
            "Epoch [5/15], batch: [51453/55810, loss: 7.5536]\n",
            "Epoch [5/15], batch: [51454/55810, loss: 5.8253]\n",
            "Epoch [5/15], batch: [51455/55810, loss: 7.5064]\n",
            "Epoch [5/15], batch: [51456/55810, loss: 6.0921]\n",
            "Epoch [5/15], batch: [51457/55810, loss: 7.1200]\n",
            "Epoch [5/15], batch: [51458/55810, loss: 7.4726]\n",
            "Epoch [5/15], batch: [51459/55810, loss: 6.8845]\n",
            "Epoch [5/15], batch: [51460/55810, loss: 3.6694]\n",
            "Epoch [5/15], batch: [51461/55810, loss: 6.4112]\n",
            "Epoch [5/15], batch: [51462/55810, loss: 6.6519]\n",
            "Epoch [5/15], batch: [51463/55810, loss: 6.1211]\n",
            "Epoch [5/15], batch: [51464/55810, loss: 7.0466]\n",
            "Epoch [5/15], batch: [51465/55810, loss: 7.5977]\n",
            "Epoch [5/15], batch: [51466/55810, loss: 7.1394]\n",
            "Epoch [5/15], batch: [51467/55810, loss: 6.4568]\n",
            "Epoch [5/15], batch: [51468/55810, loss: 6.5949]\n",
            "Epoch [5/15], batch: [51469/55810, loss: 6.1851]\n",
            "Epoch [5/15], batch: [51470/55810, loss: 7.0872]\n",
            "Epoch [5/15], batch: [51471/55810, loss: 6.6043]\n",
            "Epoch [5/15], batch: [51472/55810, loss: 7.0005]\n",
            "Epoch [5/15], batch: [51473/55810, loss: 7.7659]\n",
            "Epoch [5/15], batch: [51474/55810, loss: 6.9013]\n",
            "Epoch [5/15], batch: [51475/55810, loss: 8.2230]\n",
            "Epoch [5/15], batch: [51476/55810, loss: 7.4709]\n",
            "Epoch [5/15], batch: [51477/55810, loss: 7.2090]\n",
            "Epoch [5/15], batch: [51478/55810, loss: 7.4465]\n",
            "Epoch [5/15], batch: [51479/55810, loss: 6.8894]\n",
            "Epoch [5/15], batch: [51480/55810, loss: 7.2016]\n",
            "Epoch [5/15], batch: [51481/55810, loss: 7.0391]\n",
            "Epoch [5/15], batch: [51482/55810, loss: 6.3049]\n",
            "Epoch [5/15], batch: [51483/55810, loss: 6.8957]\n",
            "Epoch [5/15], batch: [51484/55810, loss: 6.5719]\n",
            "Epoch [5/15], batch: [51485/55810, loss: 7.6333]\n",
            "Epoch [5/15], batch: [51486/55810, loss: 8.9357]\n",
            "Epoch [5/15], batch: [51487/55810, loss: 6.6912]\n",
            "Epoch [5/15], batch: [51488/55810, loss: 6.6179]\n",
            "Epoch [5/15], batch: [51489/55810, loss: 8.3551]\n",
            "Epoch [5/15], batch: [51490/55810, loss: 7.8059]\n",
            "Epoch [5/15], batch: [51491/55810, loss: 7.1015]\n",
            "Epoch [5/15], batch: [51492/55810, loss: 5.5520]\n",
            "Epoch [5/15], batch: [51493/55810, loss: 8.3463]\n",
            "Epoch [5/15], batch: [51494/55810, loss: 6.8407]\n",
            "Epoch [5/15], batch: [51495/55810, loss: 3.8100]\n",
            "Epoch [5/15], batch: [51496/55810, loss: 6.7543]\n",
            "Epoch [5/15], batch: [51497/55810, loss: 7.5211]\n",
            "Epoch [5/15], batch: [51498/55810, loss: 7.3805]\n",
            "Epoch [5/15], batch: [51499/55810, loss: 7.1790]\n",
            "Epoch [5/15], batch: [51500/55810, loss: 6.5165]\n",
            "Epoch [5/15], batch: [51501/55810, loss: 7.2891]\n",
            "Epoch [5/15], batch: [51502/55810, loss: 9.3911]\n",
            "Epoch [5/15], batch: [51503/55810, loss: 6.6716]\n",
            "Epoch [5/15], batch: [51504/55810, loss: 7.2019]\n",
            "Epoch [5/15], batch: [51505/55810, loss: 6.3474]\n",
            "Epoch [5/15], batch: [51506/55810, loss: 6.5635]\n",
            "Epoch [5/15], batch: [51507/55810, loss: 4.5660]\n",
            "Epoch [5/15], batch: [51508/55810, loss: 6.3365]\n",
            "Epoch [5/15], batch: [51509/55810, loss: 6.8313]\n",
            "Epoch [5/15], batch: [51510/55810, loss: 5.5905]\n",
            "Epoch [5/15], batch: [51511/55810, loss: 3.8467]\n",
            "Epoch [5/15], batch: [51512/55810, loss: 6.2660]\n",
            "Epoch [5/15], batch: [51513/55810, loss: 7.1921]\n",
            "Epoch [5/15], batch: [51514/55810, loss: 5.4863]\n",
            "Epoch [5/15], batch: [51515/55810, loss: 5.0657]\n",
            "Epoch [5/15], batch: [51516/55810, loss: 5.3738]\n",
            "Epoch [5/15], batch: [51517/55810, loss: 6.3066]\n",
            "Epoch [5/15], batch: [51518/55810, loss: 4.3260]\n",
            "Epoch [5/15], batch: [51519/55810, loss: 5.0780]\n",
            "Epoch [5/15], batch: [51520/55810, loss: 7.0522]\n",
            "Epoch [5/15], batch: [51521/55810, loss: 6.0823]\n",
            "Epoch [5/15], batch: [51522/55810, loss: 6.2283]\n",
            "Epoch [5/15], batch: [51523/55810, loss: 5.0877]\n",
            "Epoch [5/15], batch: [51524/55810, loss: 4.9186]\n",
            "Epoch [5/15], batch: [51525/55810, loss: 6.0011]\n",
            "Epoch [5/15], batch: [51526/55810, loss: 7.3387]\n",
            "Epoch [5/15], batch: [51527/55810, loss: 7.4339]\n",
            "Epoch [5/15], batch: [51528/55810, loss: 7.3145]\n",
            "Epoch [5/15], batch: [51529/55810, loss: 6.8028]\n",
            "Epoch [5/15], batch: [51530/55810, loss: 8.3672]\n",
            "Epoch [5/15], batch: [51531/55810, loss: 8.0732]\n",
            "Epoch [5/15], batch: [51532/55810, loss: 6.7625]\n",
            "Epoch [5/15], batch: [51533/55810, loss: 5.4542]\n",
            "Epoch [5/15], batch: [51534/55810, loss: 3.6069]\n",
            "Epoch [5/15], batch: [51535/55810, loss: 7.1615]\n",
            "Epoch [5/15], batch: [51536/55810, loss: 5.9196]\n",
            "Epoch [5/15], batch: [51537/55810, loss: 6.5810]\n",
            "Epoch [5/15], batch: [51538/55810, loss: 6.2562]\n",
            "Epoch [5/15], batch: [51539/55810, loss: 6.8030]\n",
            "Epoch [5/15], batch: [51540/55810, loss: 6.6116]\n",
            "Epoch [5/15], batch: [51541/55810, loss: 4.7059]\n",
            "Epoch [5/15], batch: [51542/55810, loss: 5.8361]\n",
            "Epoch [5/15], batch: [51543/55810, loss: 6.5781]\n",
            "Epoch [5/15], batch: [51544/55810, loss: 6.9433]\n",
            "Epoch [5/15], batch: [51545/55810, loss: 3.4809]\n",
            "Epoch [5/15], batch: [51546/55810, loss: 4.6651]\n",
            "Epoch [5/15], batch: [51547/55810, loss: 5.6152]\n",
            "Epoch [5/15], batch: [51548/55810, loss: 7.6966]\n",
            "Epoch [5/15], batch: [51549/55810, loss: 7.5649]\n",
            "Epoch [5/15], batch: [51550/55810, loss: 7.3623]\n",
            "Epoch [5/15], batch: [51551/55810, loss: 5.2884]\n",
            "Epoch [5/15], batch: [51552/55810, loss: 5.5623]\n",
            "Epoch [5/15], batch: [51553/55810, loss: 4.9462]\n",
            "Epoch [5/15], batch: [51554/55810, loss: 6.6114]\n",
            "Epoch [5/15], batch: [51555/55810, loss: 7.5920]\n",
            "Epoch [5/15], batch: [51556/55810, loss: 7.5792]\n",
            "Epoch [5/15], batch: [51557/55810, loss: 6.2238]\n",
            "Epoch [5/15], batch: [51558/55810, loss: 5.9450]\n",
            "Epoch [5/15], batch: [51559/55810, loss: 7.0619]\n",
            "Epoch [5/15], batch: [51560/55810, loss: 6.0780]\n",
            "Epoch [5/15], batch: [51561/55810, loss: 7.2647]\n",
            "Epoch [5/15], batch: [51562/55810, loss: 5.9811]\n",
            "Epoch [5/15], batch: [51563/55810, loss: 6.6547]\n",
            "Epoch [5/15], batch: [51564/55810, loss: 6.2980]\n",
            "Epoch [5/15], batch: [51565/55810, loss: 6.7599]\n",
            "Epoch [5/15], batch: [51566/55810, loss: 5.3529]\n",
            "Epoch [5/15], batch: [51567/55810, loss: 6.8021]\n",
            "Epoch [5/15], batch: [51568/55810, loss: 5.8722]\n",
            "Epoch [5/15], batch: [51569/55810, loss: 7.8740]\n",
            "Epoch [5/15], batch: [51570/55810, loss: 6.6980]\n",
            "Epoch [5/15], batch: [51571/55810, loss: 7.3694]\n",
            "Epoch [5/15], batch: [51572/55810, loss: 8.7988]\n",
            "Epoch [5/15], batch: [51573/55810, loss: 7.5602]\n",
            "Epoch [5/15], batch: [51574/55810, loss: 5.5860]\n",
            "Epoch [5/15], batch: [51575/55810, loss: 5.1586]\n",
            "Epoch [5/15], batch: [51576/55810, loss: 6.4303]\n",
            "Epoch [5/15], batch: [51577/55810, loss: 7.2750]\n",
            "Epoch [5/15], batch: [51578/55810, loss: 6.8987]\n",
            "Epoch [5/15], batch: [51579/55810, loss: 5.9360]\n",
            "Epoch [5/15], batch: [51580/55810, loss: 6.1824]\n",
            "Epoch [5/15], batch: [51581/55810, loss: 5.8386]\n",
            "Epoch [5/15], batch: [51582/55810, loss: 5.3069]\n",
            "Epoch [5/15], batch: [51583/55810, loss: 6.7312]\n",
            "Epoch [5/15], batch: [51584/55810, loss: 6.8701]\n",
            "Epoch [5/15], batch: [51585/55810, loss: 7.5223]\n",
            "Epoch [5/15], batch: [51586/55810, loss: 7.1231]\n",
            "Epoch [5/15], batch: [51587/55810, loss: 7.4992]\n",
            "Epoch [5/15], batch: [51588/55810, loss: 7.3877]\n",
            "Epoch [5/15], batch: [51589/55810, loss: 5.7911]\n",
            "Epoch [5/15], batch: [51590/55810, loss: 8.1930]\n",
            "Epoch [5/15], batch: [51591/55810, loss: 6.4965]\n",
            "Epoch [5/15], batch: [51592/55810, loss: 6.1497]\n",
            "Epoch [5/15], batch: [51593/55810, loss: 7.1142]\n",
            "Epoch [5/15], batch: [51594/55810, loss: 6.5324]\n",
            "Epoch [5/15], batch: [51595/55810, loss: 7.3425]\n",
            "Epoch [5/15], batch: [51596/55810, loss: 6.7562]\n",
            "Epoch [5/15], batch: [51597/55810, loss: 7.6663]\n",
            "Epoch [5/15], batch: [51598/55810, loss: 7.7807]\n",
            "Epoch [5/15], batch: [51599/55810, loss: 7.8328]\n",
            "Epoch [5/15], batch: [51600/55810, loss: 8.1226]\n",
            "Epoch [5/15], batch: [51601/55810, loss: 6.8092]\n",
            "Epoch [5/15], batch: [51602/55810, loss: 8.3304]\n",
            "Epoch [5/15], batch: [51603/55810, loss: 5.7905]\n",
            "Epoch [5/15], batch: [51604/55810, loss: 7.6773]\n",
            "Epoch [5/15], batch: [51605/55810, loss: 7.1915]\n",
            "Epoch [5/15], batch: [51606/55810, loss: 7.6335]\n",
            "Epoch [5/15], batch: [51607/55810, loss: 6.1842]\n",
            "Epoch [5/15], batch: [51608/55810, loss: 6.7859]\n",
            "Epoch [5/15], batch: [51609/55810, loss: 5.4673]\n",
            "Epoch [5/15], batch: [51610/55810, loss: 6.2537]\n",
            "Epoch [5/15], batch: [51611/55810, loss: 5.3646]\n",
            "Epoch [5/15], batch: [51612/55810, loss: 8.1648]\n",
            "Epoch [5/15], batch: [51613/55810, loss: 6.4877]\n",
            "Epoch [5/15], batch: [51614/55810, loss: 7.6027]\n",
            "Epoch [5/15], batch: [51615/55810, loss: 6.3281]\n",
            "Epoch [5/15], batch: [51616/55810, loss: 8.5340]\n",
            "Epoch [5/15], batch: [51617/55810, loss: 6.9338]\n",
            "Epoch [5/15], batch: [51618/55810, loss: 6.5596]\n",
            "Epoch [5/15], batch: [51619/55810, loss: 6.7466]\n",
            "Epoch [5/15], batch: [51620/55810, loss: 7.1642]\n",
            "Epoch [5/15], batch: [51621/55810, loss: 6.7495]\n",
            "Epoch [5/15], batch: [51622/55810, loss: 7.6016]\n",
            "Epoch [5/15], batch: [51623/55810, loss: 6.3186]\n",
            "Epoch [5/15], batch: [51624/55810, loss: 5.8200]\n",
            "Epoch [5/15], batch: [51625/55810, loss: 7.1012]\n",
            "Epoch [5/15], batch: [51626/55810, loss: 6.1101]\n",
            "Epoch [5/15], batch: [51627/55810, loss: 6.9477]\n",
            "Epoch [5/15], batch: [51628/55810, loss: 7.9003]\n",
            "Epoch [5/15], batch: [51629/55810, loss: 6.8110]\n",
            "Epoch [5/15], batch: [51630/55810, loss: 4.8610]\n",
            "Epoch [5/15], batch: [51631/55810, loss: 8.3116]\n",
            "Epoch [5/15], batch: [51632/55810, loss: 7.1224]\n",
            "Epoch [5/15], batch: [51633/55810, loss: 7.7972]\n",
            "Epoch [5/15], batch: [51634/55810, loss: 6.9492]\n",
            "Epoch [5/15], batch: [51635/55810, loss: 9.3821]\n",
            "Epoch [5/15], batch: [51636/55810, loss: 6.6628]\n",
            "Epoch [5/15], batch: [51637/55810, loss: 7.1502]\n",
            "Epoch [5/15], batch: [51638/55810, loss: 6.6684]\n",
            "Epoch [5/15], batch: [51639/55810, loss: 8.4507]\n",
            "Epoch [5/15], batch: [51640/55810, loss: 9.4515]\n",
            "Epoch [5/15], batch: [51641/55810, loss: 8.3280]\n",
            "Epoch [5/15], batch: [51642/55810, loss: 7.0508]\n",
            "Epoch [5/15], batch: [51643/55810, loss: 8.9072]\n",
            "Epoch [5/15], batch: [51644/55810, loss: 9.0150]\n",
            "Epoch [5/15], batch: [51645/55810, loss: 7.7608]\n",
            "Epoch [5/15], batch: [51646/55810, loss: 5.4510]\n",
            "Epoch [5/15], batch: [51647/55810, loss: 6.0026]\n",
            "Epoch [5/15], batch: [51648/55810, loss: 5.2073]\n",
            "Epoch [5/15], batch: [51649/55810, loss: 4.9487]\n",
            "Epoch [5/15], batch: [51650/55810, loss: 5.8071]\n",
            "Epoch [5/15], batch: [51651/55810, loss: 6.3035]\n",
            "Epoch [5/15], batch: [51652/55810, loss: 6.5960]\n",
            "Epoch [5/15], batch: [51653/55810, loss: 7.3630]\n",
            "Epoch [5/15], batch: [51654/55810, loss: 8.1392]\n",
            "Epoch [5/15], batch: [51655/55810, loss: 7.0951]\n",
            "Epoch [5/15], batch: [51656/55810, loss: 7.0230]\n",
            "Epoch [5/15], batch: [51657/55810, loss: 7.7597]\n",
            "Epoch [5/15], batch: [51658/55810, loss: 6.9004]\n",
            "Epoch [5/15], batch: [51659/55810, loss: 5.4193]\n",
            "Epoch [5/15], batch: [51660/55810, loss: 7.7377]\n",
            "Epoch [5/15], batch: [51661/55810, loss: 7.1976]\n",
            "Epoch [5/15], batch: [51662/55810, loss: 7.1164]\n",
            "Epoch [5/15], batch: [51663/55810, loss: 7.0936]\n",
            "Epoch [5/15], batch: [51664/55810, loss: 6.7257]\n",
            "Epoch [5/15], batch: [51665/55810, loss: 7.9031]\n",
            "Epoch [5/15], batch: [51666/55810, loss: 6.9860]\n",
            "Epoch [5/15], batch: [51667/55810, loss: 6.0432]\n",
            "Epoch [5/15], batch: [51668/55810, loss: 7.5994]\n",
            "Epoch [5/15], batch: [51669/55810, loss: 5.7651]\n",
            "Epoch [5/15], batch: [51670/55810, loss: 4.6830]\n",
            "Epoch [5/15], batch: [51671/55810, loss: 6.7200]\n",
            "Epoch [5/15], batch: [51672/55810, loss: 4.8905]\n",
            "Epoch [5/15], batch: [51673/55810, loss: 7.1739]\n",
            "Epoch [5/15], batch: [51674/55810, loss: 7.4030]\n",
            "Epoch [5/15], batch: [51675/55810, loss: 7.5061]\n",
            "Epoch [5/15], batch: [51676/55810, loss: 6.7208]\n",
            "Epoch [5/15], batch: [51677/55810, loss: 6.0656]\n",
            "Epoch [5/15], batch: [51678/55810, loss: 6.7004]\n",
            "Epoch [5/15], batch: [51679/55810, loss: 6.1587]\n",
            "Epoch [5/15], batch: [51680/55810, loss: 6.7864]\n",
            "Epoch [5/15], batch: [51681/55810, loss: 7.2976]\n",
            "Epoch [5/15], batch: [51682/55810, loss: 7.1362]\n",
            "Epoch [5/15], batch: [51683/55810, loss: 7.6056]\n",
            "Epoch [5/15], batch: [51684/55810, loss: 5.9476]\n",
            "Epoch [5/15], batch: [51685/55810, loss: 7.2481]\n",
            "Epoch [5/15], batch: [51686/55810, loss: 7.0607]\n",
            "Epoch [5/15], batch: [51687/55810, loss: 5.6664]\n",
            "Epoch [5/15], batch: [51688/55810, loss: 4.0739]\n",
            "Epoch [5/15], batch: [51689/55810, loss: 7.9386]\n",
            "Epoch [5/15], batch: [51690/55810, loss: 5.3793]\n",
            "Epoch [5/15], batch: [51691/55810, loss: 4.3586]\n",
            "Epoch [5/15], batch: [51692/55810, loss: 6.2432]\n",
            "Epoch [5/15], batch: [51693/55810, loss: 7.8366]\n",
            "Epoch [5/15], batch: [51694/55810, loss: 9.4747]\n",
            "Epoch [5/15], batch: [51695/55810, loss: 8.9803]\n",
            "Epoch [5/15], batch: [51696/55810, loss: 6.8207]\n",
            "Epoch [5/15], batch: [51697/55810, loss: 6.8420]\n",
            "Epoch [5/15], batch: [51698/55810, loss: 6.3256]\n",
            "Epoch [5/15], batch: [51699/55810, loss: 5.6980]\n",
            "Epoch [5/15], batch: [51700/55810, loss: 5.6434]\n",
            "Epoch [5/15], batch: [51701/55810, loss: 6.9925]\n",
            "Epoch [5/15], batch: [51702/55810, loss: 7.1563]\n",
            "Epoch [5/15], batch: [51703/55810, loss: 5.8147]\n",
            "Epoch [5/15], batch: [51704/55810, loss: 6.1032]\n",
            "Epoch [5/15], batch: [51705/55810, loss: 5.2239]\n",
            "Epoch [5/15], batch: [51706/55810, loss: 7.0712]\n",
            "Epoch [5/15], batch: [51707/55810, loss: 8.6609]\n",
            "Epoch [5/15], batch: [51708/55810, loss: 8.4397]\n",
            "Epoch [5/15], batch: [51709/55810, loss: 7.0380]\n",
            "Epoch [5/15], batch: [51710/55810, loss: 6.7499]\n",
            "Epoch [5/15], batch: [51711/55810, loss: 6.7667]\n",
            "Epoch [5/15], batch: [51712/55810, loss: 7.9858]\n",
            "Epoch [5/15], batch: [51713/55810, loss: 6.0017]\n",
            "Epoch [5/15], batch: [51714/55810, loss: 6.6613]\n",
            "Epoch [5/15], batch: [51715/55810, loss: 6.4229]\n",
            "Epoch [5/15], batch: [51716/55810, loss: 4.9501]\n",
            "Epoch [5/15], batch: [51717/55810, loss: 5.6947]\n",
            "Epoch [5/15], batch: [51718/55810, loss: 6.7062]\n",
            "Epoch [5/15], batch: [51719/55810, loss: 6.5820]\n",
            "Epoch [5/15], batch: [51720/55810, loss: 7.5892]\n",
            "Epoch [5/15], batch: [51721/55810, loss: 7.4641]\n",
            "Epoch [5/15], batch: [51722/55810, loss: 8.7010]\n",
            "Epoch [5/15], batch: [51723/55810, loss: 6.9968]\n",
            "Epoch [5/15], batch: [51724/55810, loss: 7.1552]\n",
            "Epoch [5/15], batch: [51725/55810, loss: 7.1813]\n",
            "Epoch [5/15], batch: [51726/55810, loss: 4.9376]\n",
            "Epoch [5/15], batch: [51727/55810, loss: 6.2261]\n",
            "Epoch [5/15], batch: [51728/55810, loss: 5.9549]\n",
            "Epoch [5/15], batch: [51729/55810, loss: 5.2151]\n",
            "Epoch [5/15], batch: [51730/55810, loss: 5.6832]\n",
            "Epoch [5/15], batch: [51731/55810, loss: 6.4169]\n",
            "Epoch [5/15], batch: [51732/55810, loss: 6.5704]\n",
            "Epoch [5/15], batch: [51733/55810, loss: 6.3951]\n",
            "Epoch [5/15], batch: [51734/55810, loss: 7.0937]\n",
            "Epoch [5/15], batch: [51735/55810, loss: 3.8467]\n",
            "Epoch [5/15], batch: [51736/55810, loss: 5.2224]\n",
            "Epoch [5/15], batch: [51737/55810, loss: 6.1175]\n",
            "Epoch [5/15], batch: [51738/55810, loss: 6.4746]\n",
            "Epoch [5/15], batch: [51739/55810, loss: 5.3015]\n",
            "Epoch [5/15], batch: [51740/55810, loss: 6.5729]\n",
            "Epoch [5/15], batch: [51741/55810, loss: 6.0793]\n",
            "Epoch [5/15], batch: [51742/55810, loss: 6.9167]\n",
            "Epoch [5/15], batch: [51743/55810, loss: 5.3306]\n",
            "Epoch [5/15], batch: [51744/55810, loss: 6.4677]\n",
            "Epoch [5/15], batch: [51745/55810, loss: 5.2747]\n",
            "Epoch [5/15], batch: [51746/55810, loss: 5.6037]\n",
            "Epoch [5/15], batch: [51747/55810, loss: 6.9044]\n",
            "Epoch [5/15], batch: [51748/55810, loss: 5.4257]\n",
            "Epoch [5/15], batch: [51749/55810, loss: 6.0731]\n",
            "Epoch [5/15], batch: [51750/55810, loss: 5.6522]\n",
            "Epoch [5/15], batch: [51751/55810, loss: 3.8678]\n",
            "Epoch [5/15], batch: [51752/55810, loss: 6.7903]\n",
            "Epoch [5/15], batch: [51753/55810, loss: 7.6734]\n",
            "Epoch [5/15], batch: [51754/55810, loss: 5.9284]\n",
            "Epoch [5/15], batch: [51755/55810, loss: 6.4526]\n",
            "Epoch [5/15], batch: [51756/55810, loss: 5.1908]\n",
            "Epoch [5/15], batch: [51757/55810, loss: 5.8909]\n",
            "Epoch [5/15], batch: [51758/55810, loss: 6.8691]\n",
            "Epoch [5/15], batch: [51759/55810, loss: 5.7259]\n",
            "Epoch [5/15], batch: [51760/55810, loss: 6.7056]\n",
            "Epoch [5/15], batch: [51761/55810, loss: 7.7676]\n",
            "Epoch [5/15], batch: [51762/55810, loss: 7.0935]\n",
            "Epoch [5/15], batch: [51763/55810, loss: 4.2162]\n",
            "Epoch [5/15], batch: [51764/55810, loss: 5.7093]\n",
            "Epoch [5/15], batch: [51765/55810, loss: 4.8561]\n",
            "Epoch [5/15], batch: [51766/55810, loss: 6.6242]\n",
            "Epoch [5/15], batch: [51767/55810, loss: 4.8083]\n",
            "Epoch [5/15], batch: [51768/55810, loss: 5.8954]\n",
            "Epoch [5/15], batch: [51769/55810, loss: 7.6930]\n",
            "Epoch [5/15], batch: [51770/55810, loss: 7.9957]\n",
            "Epoch [5/15], batch: [51771/55810, loss: 6.9669]\n",
            "Epoch [5/15], batch: [51772/55810, loss: 7.8156]\n",
            "Epoch [5/15], batch: [51773/55810, loss: 8.1924]\n",
            "Epoch [5/15], batch: [51774/55810, loss: 7.6660]\n",
            "Epoch [5/15], batch: [51775/55810, loss: 7.0057]\n",
            "Epoch [5/15], batch: [51776/55810, loss: 7.3770]\n",
            "Epoch [5/15], batch: [51777/55810, loss: 7.8756]\n",
            "Epoch [5/15], batch: [51778/55810, loss: 8.0319]\n",
            "Epoch [5/15], batch: [51779/55810, loss: 8.6611]\n",
            "Epoch [5/15], batch: [51780/55810, loss: 8.4447]\n",
            "Epoch [5/15], batch: [51781/55810, loss: 7.5045]\n",
            "Epoch [5/15], batch: [51782/55810, loss: 8.2251]\n",
            "Epoch [5/15], batch: [51783/55810, loss: 7.8452]\n",
            "Epoch [5/15], batch: [51784/55810, loss: 6.8811]\n",
            "Epoch [5/15], batch: [51785/55810, loss: 7.8350]\n",
            "Epoch [5/15], batch: [51786/55810, loss: 7.1647]\n",
            "Epoch [5/15], batch: [51787/55810, loss: 8.1295]\n",
            "Epoch [5/15], batch: [51788/55810, loss: 6.1498]\n",
            "Epoch [5/15], batch: [51789/55810, loss: 6.5420]\n",
            "Epoch [5/15], batch: [51790/55810, loss: 4.9296]\n",
            "Epoch [5/15], batch: [51791/55810, loss: 7.9868]\n",
            "Epoch [5/15], batch: [51792/55810, loss: 7.5223]\n",
            "Epoch [5/15], batch: [51793/55810, loss: 6.1070]\n",
            "Epoch [5/15], batch: [51794/55810, loss: 7.8869]\n",
            "Epoch [5/15], batch: [51795/55810, loss: 6.0997]\n",
            "Epoch [5/15], batch: [51796/55810, loss: 5.4324]\n",
            "Epoch [5/15], batch: [51797/55810, loss: 8.0925]\n",
            "Epoch [5/15], batch: [51798/55810, loss: 5.9142]\n",
            "Epoch [5/15], batch: [51799/55810, loss: 5.3538]\n",
            "Epoch [5/15], batch: [51800/55810, loss: 7.4582]\n",
            "Epoch [5/15], batch: [51801/55810, loss: 6.9919]\n",
            "Epoch [5/15], batch: [51802/55810, loss: 6.3469]\n",
            "Epoch [5/15], batch: [51803/55810, loss: 7.9479]\n",
            "Epoch [5/15], batch: [51804/55810, loss: 7.3113]\n",
            "Epoch [5/15], batch: [51805/55810, loss: 8.3743]\n",
            "Epoch [5/15], batch: [51806/55810, loss: 6.0000]\n",
            "Epoch [5/15], batch: [51807/55810, loss: 7.1486]\n",
            "Epoch [5/15], batch: [51808/55810, loss: 8.4528]\n",
            "Epoch [5/15], batch: [51809/55810, loss: 7.1415]\n",
            "Epoch [5/15], batch: [51810/55810, loss: 5.1322]\n",
            "Epoch [5/15], batch: [51811/55810, loss: 7.8077]\n",
            "Epoch [5/15], batch: [51812/55810, loss: 7.7305]\n",
            "Epoch [5/15], batch: [51813/55810, loss: 6.3332]\n",
            "Epoch [5/15], batch: [51814/55810, loss: 6.2699]\n",
            "Epoch [5/15], batch: [51815/55810, loss: 4.9165]\n",
            "Epoch [5/15], batch: [51816/55810, loss: 6.1139]\n",
            "Epoch [5/15], batch: [51817/55810, loss: 8.0069]\n",
            "Epoch [5/15], batch: [51818/55810, loss: 7.2168]\n",
            "Epoch [5/15], batch: [51819/55810, loss: 5.7204]\n",
            "Epoch [5/15], batch: [51820/55810, loss: 6.4187]\n",
            "Epoch [5/15], batch: [51821/55810, loss: 4.8728]\n",
            "Epoch [5/15], batch: [51822/55810, loss: 5.2373]\n",
            "Epoch [5/15], batch: [51823/55810, loss: 5.8372]\n",
            "Epoch [5/15], batch: [51824/55810, loss: 7.0771]\n",
            "Epoch [5/15], batch: [51825/55810, loss: 6.7721]\n",
            "Epoch [5/15], batch: [51826/55810, loss: 5.2956]\n",
            "Epoch [5/15], batch: [51827/55810, loss: 7.1034]\n",
            "Epoch [5/15], batch: [51828/55810, loss: 7.7176]\n",
            "Epoch [5/15], batch: [51829/55810, loss: 6.6866]\n",
            "Epoch [5/15], batch: [51830/55810, loss: 5.8397]\n",
            "Epoch [5/15], batch: [51831/55810, loss: 6.1673]\n",
            "Epoch [5/15], batch: [51832/55810, loss: 5.6514]\n",
            "Epoch [5/15], batch: [51833/55810, loss: 6.7699]\n",
            "Epoch [5/15], batch: [51834/55810, loss: 6.1846]\n",
            "Epoch [5/15], batch: [51835/55810, loss: 7.1958]\n",
            "Epoch [5/15], batch: [51836/55810, loss: 6.0735]\n",
            "Epoch [5/15], batch: [51837/55810, loss: 6.1370]\n",
            "Epoch [5/15], batch: [51838/55810, loss: 4.9617]\n",
            "Epoch [5/15], batch: [51839/55810, loss: 5.7202]\n",
            "Epoch [5/15], batch: [51840/55810, loss: 6.1496]\n",
            "Epoch [5/15], batch: [51841/55810, loss: 6.9070]\n",
            "Epoch [5/15], batch: [51842/55810, loss: 8.1970]\n",
            "Epoch [5/15], batch: [51843/55810, loss: 6.8514]\n",
            "Epoch [5/15], batch: [51844/55810, loss: 6.6758]\n",
            "Epoch [5/15], batch: [51845/55810, loss: 6.1771]\n",
            "Epoch [5/15], batch: [51846/55810, loss: 5.5941]\n",
            "Epoch [5/15], batch: [51847/55810, loss: 5.8413]\n",
            "Epoch [5/15], batch: [51848/55810, loss: 6.2929]\n",
            "Epoch [5/15], batch: [51849/55810, loss: 5.3390]\n",
            "Epoch [5/15], batch: [51850/55810, loss: 5.5509]\n",
            "Epoch [5/15], batch: [51851/55810, loss: 5.4705]\n",
            "Epoch [5/15], batch: [51852/55810, loss: 6.1933]\n",
            "Epoch [5/15], batch: [51853/55810, loss: 6.6737]\n",
            "Epoch [5/15], batch: [51854/55810, loss: 6.8331]\n",
            "Epoch [5/15], batch: [51855/55810, loss: 7.7012]\n",
            "Epoch [5/15], batch: [51856/55810, loss: 5.9006]\n",
            "Epoch [5/15], batch: [51857/55810, loss: 5.9595]\n",
            "Epoch [5/15], batch: [51858/55810, loss: 6.8278]\n",
            "Epoch [5/15], batch: [51859/55810, loss: 8.1884]\n",
            "Epoch [5/15], batch: [51860/55810, loss: 7.5376]\n",
            "Epoch [5/15], batch: [51861/55810, loss: 8.3366]\n",
            "Epoch [5/15], batch: [51862/55810, loss: 7.6411]\n",
            "Epoch [5/15], batch: [51863/55810, loss: 6.3169]\n",
            "Epoch [5/15], batch: [51864/55810, loss: 5.6923]\n",
            "Epoch [5/15], batch: [51865/55810, loss: 6.6264]\n",
            "Epoch [5/15], batch: [51866/55810, loss: 6.0455]\n",
            "Epoch [5/15], batch: [51867/55810, loss: 6.4757]\n",
            "Epoch [5/15], batch: [51868/55810, loss: 5.8055]\n",
            "Epoch [5/15], batch: [51869/55810, loss: 5.0382]\n",
            "Epoch [5/15], batch: [51870/55810, loss: 4.8729]\n",
            "Epoch [5/15], batch: [51871/55810, loss: 4.8615]\n",
            "Epoch [5/15], batch: [51872/55810, loss: 4.9672]\n",
            "Epoch [5/15], batch: [51873/55810, loss: 8.0553]\n",
            "Epoch [5/15], batch: [51874/55810, loss: 7.9317]\n",
            "Epoch [5/15], batch: [51875/55810, loss: 6.5297]\n",
            "Epoch [5/15], batch: [51876/55810, loss: 7.3694]\n",
            "Epoch [5/15], batch: [51877/55810, loss: 7.7854]\n",
            "Epoch [5/15], batch: [51878/55810, loss: 5.7403]\n",
            "Epoch [5/15], batch: [51879/55810, loss: 5.8933]\n",
            "Epoch [5/15], batch: [51880/55810, loss: 5.2729]\n",
            "Epoch [5/15], batch: [51881/55810, loss: 6.9322]\n",
            "Epoch [5/15], batch: [51882/55810, loss: 5.3569]\n",
            "Epoch [5/15], batch: [51883/55810, loss: 5.2025]\n",
            "Epoch [5/15], batch: [51884/55810, loss: 7.9882]\n",
            "Epoch [5/15], batch: [51885/55810, loss: 6.5287]\n",
            "Epoch [5/15], batch: [51886/55810, loss: 6.9793]\n",
            "Epoch [5/15], batch: [51887/55810, loss: 4.9677]\n",
            "Epoch [5/15], batch: [51888/55810, loss: 7.6854]\n",
            "Epoch [5/15], batch: [51889/55810, loss: 7.2616]\n",
            "Epoch [5/15], batch: [51890/55810, loss: 7.7247]\n",
            "Epoch [5/15], batch: [51891/55810, loss: 7.4592]\n",
            "Epoch [5/15], batch: [51892/55810, loss: 5.7366]\n",
            "Epoch [5/15], batch: [51893/55810, loss: 7.8280]\n",
            "Epoch [5/15], batch: [51894/55810, loss: 7.2049]\n",
            "Epoch [5/15], batch: [51895/55810, loss: 7.8958]\n",
            "Epoch [5/15], batch: [51896/55810, loss: 8.0218]\n",
            "Epoch [5/15], batch: [51897/55810, loss: 7.1521]\n",
            "Epoch [5/15], batch: [51898/55810, loss: 7.1833]\n",
            "Epoch [5/15], batch: [51899/55810, loss: 7.0906]\n",
            "Epoch [5/15], batch: [51900/55810, loss: 7.1850]\n",
            "Epoch [5/15], batch: [51901/55810, loss: 8.5790]\n",
            "Epoch [5/15], batch: [51902/55810, loss: 8.9412]\n",
            "Epoch [5/15], batch: [51903/55810, loss: 8.3565]\n",
            "Epoch [5/15], batch: [51904/55810, loss: 7.5525]\n",
            "Epoch [5/15], batch: [51905/55810, loss: 6.5140]\n",
            "Epoch [5/15], batch: [51906/55810, loss: 8.3593]\n",
            "Epoch [5/15], batch: [51907/55810, loss: 7.8940]\n",
            "Epoch [5/15], batch: [51908/55810, loss: 7.3720]\n",
            "Epoch [5/15], batch: [51909/55810, loss: 8.0250]\n",
            "Epoch [5/15], batch: [51910/55810, loss: 7.7586]\n",
            "Epoch [5/15], batch: [51911/55810, loss: 7.1974]\n",
            "Epoch [5/15], batch: [51912/55810, loss: 9.1558]\n",
            "Epoch [5/15], batch: [51913/55810, loss: 7.9578]\n",
            "Epoch [5/15], batch: [51914/55810, loss: 8.0806]\n",
            "Epoch [5/15], batch: [51915/55810, loss: 7.6291]\n",
            "Epoch [5/15], batch: [51916/55810, loss: 7.7062]\n",
            "Epoch [5/15], batch: [51917/55810, loss: 7.7847]\n",
            "Epoch [5/15], batch: [51918/55810, loss: 8.9241]\n",
            "Epoch [5/15], batch: [51919/55810, loss: 8.2180]\n",
            "Epoch [5/15], batch: [51920/55810, loss: 8.3364]\n",
            "Epoch [5/15], batch: [51921/55810, loss: 7.3316]\n",
            "Epoch [5/15], batch: [51922/55810, loss: 8.6658]\n",
            "Epoch [5/15], batch: [51923/55810, loss: 7.6570]\n",
            "Epoch [5/15], batch: [51924/55810, loss: 8.4889]\n",
            "Epoch [5/15], batch: [51925/55810, loss: 7.4653]\n",
            "Epoch [5/15], batch: [51926/55810, loss: 8.3058]\n",
            "Epoch [5/15], batch: [51927/55810, loss: 7.6962]\n",
            "Epoch [5/15], batch: [51928/55810, loss: 8.0806]\n",
            "Epoch [5/15], batch: [51929/55810, loss: 7.7834]\n",
            "Epoch [5/15], batch: [51930/55810, loss: 8.9970]\n",
            "Epoch [5/15], batch: [51931/55810, loss: 9.6131]\n",
            "Epoch [5/15], batch: [51932/55810, loss: 8.1922]\n",
            "Epoch [5/15], batch: [51933/55810, loss: 8.8414]\n",
            "Epoch [5/15], batch: [51934/55810, loss: 8.2295]\n",
            "Epoch [5/15], batch: [51935/55810, loss: 8.3735]\n",
            "Epoch [5/15], batch: [51936/55810, loss: 8.4974]\n",
            "Epoch [5/15], batch: [51937/55810, loss: 8.7614]\n",
            "Epoch [5/15], batch: [51938/55810, loss: 9.1881]\n",
            "Epoch [5/15], batch: [51939/55810, loss: 6.9431]\n",
            "Epoch [5/15], batch: [51940/55810, loss: 8.8195]\n",
            "Epoch [5/15], batch: [51941/55810, loss: 7.7435]\n",
            "Epoch [5/15], batch: [51942/55810, loss: 9.0519]\n",
            "Epoch [5/15], batch: [51943/55810, loss: 8.8984]\n",
            "Epoch [5/15], batch: [51944/55810, loss: 8.7234]\n",
            "Epoch [5/15], batch: [51945/55810, loss: 6.6026]\n",
            "Epoch [5/15], batch: [51946/55810, loss: 7.0929]\n",
            "Epoch [5/15], batch: [51947/55810, loss: 6.0701]\n",
            "Epoch [5/15], batch: [51948/55810, loss: 6.5200]\n",
            "Epoch [5/15], batch: [51949/55810, loss: 7.2966]\n",
            "Epoch [5/15], batch: [51950/55810, loss: 8.5698]\n",
            "Epoch [5/15], batch: [51951/55810, loss: 9.7944]\n",
            "Epoch [5/15], batch: [51952/55810, loss: 8.6538]\n",
            "Epoch [5/15], batch: [51953/55810, loss: 6.9440]\n",
            "Epoch [5/15], batch: [51954/55810, loss: 7.4559]\n",
            "Epoch [5/15], batch: [51955/55810, loss: 6.4875]\n",
            "Epoch [5/15], batch: [51956/55810, loss: 7.0814]\n",
            "Epoch [5/15], batch: [51957/55810, loss: 6.8513]\n",
            "Epoch [5/15], batch: [51958/55810, loss: 3.6277]\n",
            "Epoch [5/15], batch: [51959/55810, loss: 5.1444]\n",
            "Epoch [5/15], batch: [51960/55810, loss: 5.8318]\n",
            "Epoch [5/15], batch: [51961/55810, loss: 6.3888]\n",
            "Epoch [5/15], batch: [51962/55810, loss: 5.4036]\n",
            "Epoch [5/15], batch: [51963/55810, loss: 7.8480]\n",
            "Epoch [5/15], batch: [51964/55810, loss: 7.3775]\n",
            "Epoch [5/15], batch: [51965/55810, loss: 6.3920]\n",
            "Epoch [5/15], batch: [51966/55810, loss: 6.0870]\n",
            "Epoch [5/15], batch: [51967/55810, loss: 4.9120]\n",
            "Epoch [5/15], batch: [51968/55810, loss: 7.2085]\n",
            "Epoch [5/15], batch: [51969/55810, loss: 5.2516]\n",
            "Epoch [5/15], batch: [51970/55810, loss: 6.6953]\n",
            "Epoch [5/15], batch: [51971/55810, loss: 7.0377]\n",
            "Epoch [5/15], batch: [51972/55810, loss: 8.5389]\n",
            "Epoch [5/15], batch: [51973/55810, loss: 7.0310]\n",
            "Epoch [5/15], batch: [51974/55810, loss: 7.4871]\n",
            "Epoch [5/15], batch: [51975/55810, loss: 5.3760]\n",
            "Epoch [5/15], batch: [51976/55810, loss: 7.4140]\n",
            "Epoch [5/15], batch: [51977/55810, loss: 7.4230]\n",
            "Epoch [5/15], batch: [51978/55810, loss: 6.1236]\n",
            "Epoch [5/15], batch: [51979/55810, loss: 5.6664]\n",
            "Epoch [5/15], batch: [51980/55810, loss: 3.5682]\n",
            "Epoch [5/15], batch: [51981/55810, loss: 6.6032]\n",
            "Epoch [5/15], batch: [51982/55810, loss: 4.6790]\n",
            "Epoch [5/15], batch: [51983/55810, loss: 4.7318]\n",
            "Epoch [5/15], batch: [51984/55810, loss: 6.1252]\n",
            "Epoch [5/15], batch: [51985/55810, loss: 7.4312]\n",
            "Epoch [5/15], batch: [51986/55810, loss: 6.5221]\n",
            "Epoch [5/15], batch: [51987/55810, loss: 5.7964]\n",
            "Epoch [5/15], batch: [51988/55810, loss: 5.6278]\n",
            "Epoch [5/15], batch: [51989/55810, loss: 6.3737]\n",
            "Epoch [5/15], batch: [51990/55810, loss: 7.9420]\n",
            "Epoch [5/15], batch: [51991/55810, loss: 7.5403]\n",
            "Epoch [5/15], batch: [51992/55810, loss: 7.9008]\n",
            "Epoch [5/15], batch: [51993/55810, loss: 7.6442]\n",
            "Epoch [5/15], batch: [51994/55810, loss: 7.8396]\n",
            "Epoch [5/15], batch: [51995/55810, loss: 5.4864]\n",
            "Epoch [5/15], batch: [51996/55810, loss: 5.7267]\n",
            "Epoch [5/15], batch: [51997/55810, loss: 6.8462]\n",
            "Epoch [5/15], batch: [51998/55810, loss: 7.0546]\n",
            "Epoch [5/15], batch: [51999/55810, loss: 7.4010]\n",
            "Epoch [5/15], batch: [52000/55810, loss: 6.7967]\n",
            "Epoch [5/15], batch: [52001/55810, loss: 7.3871]\n",
            "Epoch [5/15], batch: [52002/55810, loss: 6.2955]\n",
            "Epoch [5/15], batch: [52003/55810, loss: 7.4679]\n",
            "Epoch [5/15], batch: [52004/55810, loss: 5.1601]\n",
            "Epoch [5/15], batch: [52005/55810, loss: 5.7220]\n",
            "Epoch [5/15], batch: [52006/55810, loss: 8.1505]\n",
            "Epoch [5/15], batch: [52007/55810, loss: 8.5532]\n",
            "Epoch [5/15], batch: [52008/55810, loss: 6.8962]\n",
            "Epoch [5/15], batch: [52009/55810, loss: 7.7223]\n",
            "Epoch [5/15], batch: [52010/55810, loss: 6.6796]\n",
            "Epoch [5/15], batch: [52011/55810, loss: 6.6968]\n",
            "Epoch [5/15], batch: [52012/55810, loss: 5.5604]\n",
            "Epoch [5/15], batch: [52013/55810, loss: 6.5100]\n",
            "Epoch [5/15], batch: [52014/55810, loss: 6.8453]\n",
            "Epoch [5/15], batch: [52015/55810, loss: 7.1255]\n",
            "Epoch [5/15], batch: [52016/55810, loss: 6.8055]\n",
            "Epoch [5/15], batch: [52017/55810, loss: 6.6969]\n",
            "Epoch [5/15], batch: [52018/55810, loss: 7.1368]\n",
            "Epoch [5/15], batch: [52019/55810, loss: 6.2430]\n",
            "Epoch [5/15], batch: [52020/55810, loss: 6.5720]\n",
            "Epoch [5/15], batch: [52021/55810, loss: 5.4772]\n",
            "Epoch [5/15], batch: [52022/55810, loss: 5.8720]\n",
            "Epoch [5/15], batch: [52023/55810, loss: 6.4984]\n",
            "Epoch [5/15], batch: [52024/55810, loss: 6.5784]\n",
            "Epoch [5/15], batch: [52025/55810, loss: 7.0218]\n",
            "Epoch [5/15], batch: [52026/55810, loss: 4.6539]\n",
            "Epoch [5/15], batch: [52027/55810, loss: 6.0598]\n",
            "Epoch [5/15], batch: [52028/55810, loss: 6.2166]\n",
            "Epoch [5/15], batch: [52029/55810, loss: 4.4065]\n",
            "Epoch [5/15], batch: [52030/55810, loss: 6.9921]\n",
            "Epoch [5/15], batch: [52031/55810, loss: 7.8080]\n",
            "Epoch [5/15], batch: [52032/55810, loss: 7.8987]\n",
            "Epoch [5/15], batch: [52033/55810, loss: 6.6069]\n",
            "Epoch [5/15], batch: [52034/55810, loss: 6.7824]\n",
            "Epoch [5/15], batch: [52035/55810, loss: 6.4426]\n",
            "Epoch [5/15], batch: [52036/55810, loss: 4.1479]\n",
            "Epoch [5/15], batch: [52037/55810, loss: 5.5892]\n",
            "Epoch [5/15], batch: [52038/55810, loss: 6.0312]\n",
            "Epoch [5/15], batch: [52039/55810, loss: 5.3177]\n",
            "Epoch [5/15], batch: [52040/55810, loss: 7.2474]\n",
            "Epoch [5/15], batch: [52041/55810, loss: 5.4583]\n",
            "Epoch [5/15], batch: [52042/55810, loss: 6.5381]\n",
            "Epoch [5/15], batch: [52043/55810, loss: 7.8335]\n",
            "Epoch [5/15], batch: [52044/55810, loss: 7.3779]\n",
            "Epoch [5/15], batch: [52045/55810, loss: 7.3289]\n",
            "Epoch [5/15], batch: [52046/55810, loss: 7.1544]\n",
            "Epoch [5/15], batch: [52047/55810, loss: 7.8485]\n",
            "Epoch [5/15], batch: [52048/55810, loss: 7.9519]\n",
            "Epoch [5/15], batch: [52049/55810, loss: 6.7481]\n",
            "Epoch [5/15], batch: [52050/55810, loss: 9.0474]\n",
            "Epoch [5/15], batch: [52051/55810, loss: 9.0802]\n",
            "Epoch [5/15], batch: [52052/55810, loss: 8.6554]\n",
            "Epoch [5/15], batch: [52053/55810, loss: 9.5783]\n",
            "Epoch [5/15], batch: [52054/55810, loss: 7.4646]\n",
            "Epoch [5/15], batch: [52055/55810, loss: 8.1470]\n",
            "Epoch [5/15], batch: [52056/55810, loss: 8.3963]\n",
            "Epoch [5/15], batch: [52057/55810, loss: 8.2749]\n",
            "Epoch [5/15], batch: [52058/55810, loss: 5.3339]\n",
            "Epoch [5/15], batch: [52059/55810, loss: 7.1708]\n",
            "Epoch [5/15], batch: [52060/55810, loss: 7.6008]\n",
            "Epoch [5/15], batch: [52061/55810, loss: 5.6848]\n",
            "Epoch [5/15], batch: [52062/55810, loss: 7.5671]\n",
            "Epoch [5/15], batch: [52063/55810, loss: 6.5881]\n",
            "Epoch [5/15], batch: [52064/55810, loss: 6.6734]\n",
            "Epoch [5/15], batch: [52065/55810, loss: 6.2954]\n",
            "Epoch [5/15], batch: [52066/55810, loss: 6.7802]\n",
            "Epoch [5/15], batch: [52067/55810, loss: 5.9625]\n",
            "Epoch [5/15], batch: [52068/55810, loss: 5.3419]\n",
            "Epoch [5/15], batch: [52069/55810, loss: 4.8568]\n",
            "Epoch [5/15], batch: [52070/55810, loss: 5.6943]\n",
            "Epoch [5/15], batch: [52071/55810, loss: 7.3255]\n",
            "Epoch [5/15], batch: [52072/55810, loss: 5.4037]\n",
            "Epoch [5/15], batch: [52073/55810, loss: 6.7286]\n",
            "Epoch [5/15], batch: [52074/55810, loss: 8.4909]\n",
            "Epoch [5/15], batch: [52075/55810, loss: 4.7622]\n",
            "Epoch [5/15], batch: [52076/55810, loss: 5.6210]\n",
            "Epoch [5/15], batch: [52077/55810, loss: 5.6607]\n",
            "Epoch [5/15], batch: [52078/55810, loss: 6.4473]\n",
            "Epoch [5/15], batch: [52079/55810, loss: 6.3269]\n",
            "Epoch [5/15], batch: [52080/55810, loss: 5.5191]\n",
            "Epoch [5/15], batch: [52081/55810, loss: 5.6590]\n",
            "Epoch [5/15], batch: [52082/55810, loss: 5.8841]\n",
            "Epoch [5/15], batch: [52083/55810, loss: 5.0399]\n",
            "Epoch [5/15], batch: [52084/55810, loss: 7.0743]\n",
            "Epoch [5/15], batch: [52085/55810, loss: 5.1297]\n",
            "Epoch [5/15], batch: [52086/55810, loss: 4.1886]\n",
            "Epoch [5/15], batch: [52087/55810, loss: 6.8306]\n",
            "Epoch [5/15], batch: [52088/55810, loss: 6.3357]\n",
            "Epoch [5/15], batch: [52089/55810, loss: 5.8974]\n",
            "Epoch [5/15], batch: [52090/55810, loss: 5.6272]\n",
            "Epoch [5/15], batch: [52091/55810, loss: 4.3352]\n",
            "Epoch [5/15], batch: [52092/55810, loss: 6.9989]\n",
            "Epoch [5/15], batch: [52093/55810, loss: 6.0777]\n",
            "Epoch [5/15], batch: [52094/55810, loss: 8.0065]\n",
            "Epoch [5/15], batch: [52095/55810, loss: 6.1554]\n",
            "Epoch [5/15], batch: [52096/55810, loss: 5.3974]\n",
            "Epoch [5/15], batch: [52097/55810, loss: 5.6867]\n",
            "Epoch [5/15], batch: [52098/55810, loss: 6.0531]\n",
            "Epoch [5/15], batch: [52099/55810, loss: 5.0258]\n",
            "Epoch [5/15], batch: [52100/55810, loss: 3.8760]\n",
            "Epoch [5/15], batch: [52101/55810, loss: 6.8866]\n",
            "Epoch [5/15], batch: [52102/55810, loss: 6.0180]\n",
            "Epoch [5/15], batch: [52103/55810, loss: 5.0352]\n",
            "Epoch [5/15], batch: [52104/55810, loss: 5.8148]\n",
            "Epoch [5/15], batch: [52105/55810, loss: 6.8602]\n",
            "Epoch [5/15], batch: [52106/55810, loss: 4.6065]\n",
            "Epoch [5/15], batch: [52107/55810, loss: 4.8972]\n",
            "Epoch [5/15], batch: [52108/55810, loss: 5.5732]\n",
            "Epoch [5/15], batch: [52109/55810, loss: 6.2224]\n",
            "Epoch [5/15], batch: [52110/55810, loss: 5.1798]\n",
            "Epoch [5/15], batch: [52111/55810, loss: 6.8600]\n",
            "Epoch [5/15], batch: [52112/55810, loss: 6.5086]\n",
            "Epoch [5/15], batch: [52113/55810, loss: 5.7738]\n",
            "Epoch [5/15], batch: [52114/55810, loss: 5.3139]\n",
            "Epoch [5/15], batch: [52115/55810, loss: 6.9540]\n",
            "Epoch [5/15], batch: [52116/55810, loss: 4.7965]\n",
            "Epoch [5/15], batch: [52117/55810, loss: 3.7746]\n",
            "Epoch [5/15], batch: [52118/55810, loss: 5.2087]\n",
            "Epoch [5/15], batch: [52119/55810, loss: 4.8788]\n",
            "Epoch [5/15], batch: [52120/55810, loss: 5.7840]\n",
            "Epoch [5/15], batch: [52121/55810, loss: 7.3106]\n",
            "Epoch [5/15], batch: [52122/55810, loss: 7.3943]\n",
            "Epoch [5/15], batch: [52123/55810, loss: 5.1984]\n",
            "Epoch [5/15], batch: [52124/55810, loss: 5.9466]\n",
            "Epoch [5/15], batch: [52125/55810, loss: 6.9640]\n",
            "Epoch [5/15], batch: [52126/55810, loss: 5.7109]\n",
            "Epoch [5/15], batch: [52127/55810, loss: 6.8624]\n",
            "Epoch [5/15], batch: [52128/55810, loss: 6.7611]\n",
            "Epoch [5/15], batch: [52129/55810, loss: 5.1850]\n",
            "Epoch [5/15], batch: [52130/55810, loss: 6.1233]\n",
            "Epoch [5/15], batch: [52131/55810, loss: 5.5642]\n",
            "Epoch [5/15], batch: [52132/55810, loss: 6.3555]\n",
            "Epoch [5/15], batch: [52133/55810, loss: 7.9424]\n",
            "Epoch [5/15], batch: [52134/55810, loss: 5.4059]\n",
            "Epoch [5/15], batch: [52135/55810, loss: 7.0643]\n",
            "Epoch [5/15], batch: [52136/55810, loss: 8.4457]\n",
            "Epoch [5/15], batch: [52137/55810, loss: 5.3972]\n",
            "Epoch [5/15], batch: [52138/55810, loss: 5.6538]\n",
            "Epoch [5/15], batch: [52139/55810, loss: 6.4695]\n",
            "Epoch [5/15], batch: [52140/55810, loss: 7.1809]\n",
            "Epoch [5/15], batch: [52141/55810, loss: 8.1755]\n",
            "Epoch [5/15], batch: [52142/55810, loss: 8.5873]\n",
            "Epoch [5/15], batch: [52143/55810, loss: 4.7642]\n",
            "Epoch [5/15], batch: [52144/55810, loss: 6.4382]\n",
            "Epoch [5/15], batch: [52145/55810, loss: 5.8419]\n",
            "Epoch [5/15], batch: [52146/55810, loss: 5.3314]\n",
            "Epoch [5/15], batch: [52147/55810, loss: 4.9645]\n",
            "Epoch [5/15], batch: [52148/55810, loss: 6.8532]\n",
            "Epoch [5/15], batch: [52149/55810, loss: 4.3173]\n",
            "Epoch [5/15], batch: [52150/55810, loss: 5.3900]\n",
            "Epoch [5/15], batch: [52151/55810, loss: 6.5304]\n",
            "Epoch [5/15], batch: [52152/55810, loss: 6.5436]\n",
            "Epoch [5/15], batch: [52153/55810, loss: 6.5258]\n",
            "Epoch [5/15], batch: [52154/55810, loss: 6.9433]\n",
            "Epoch [5/15], batch: [52155/55810, loss: 6.6978]\n",
            "Epoch [5/15], batch: [52156/55810, loss: 6.7116]\n",
            "Epoch [5/15], batch: [52157/55810, loss: 8.1657]\n",
            "Epoch [5/15], batch: [52158/55810, loss: 6.9165]\n",
            "Epoch [5/15], batch: [52159/55810, loss: 6.9981]\n",
            "Epoch [5/15], batch: [52160/55810, loss: 6.1660]\n",
            "Epoch [5/15], batch: [52161/55810, loss: 6.7726]\n",
            "Epoch [5/15], batch: [52162/55810, loss: 8.2807]\n",
            "Epoch [5/15], batch: [52163/55810, loss: 7.0669]\n",
            "Epoch [5/15], batch: [52164/55810, loss: 6.5020]\n",
            "Epoch [5/15], batch: [52165/55810, loss: 7.0697]\n",
            "Epoch [5/15], batch: [52166/55810, loss: 5.3021]\n",
            "Epoch [5/15], batch: [52167/55810, loss: 7.2234]\n",
            "Epoch [5/15], batch: [52168/55810, loss: 5.5361]\n",
            "Epoch [5/15], batch: [52169/55810, loss: 5.9930]\n",
            "Epoch [5/15], batch: [52170/55810, loss: 7.4008]\n",
            "Epoch [5/15], batch: [52171/55810, loss: 3.6872]\n",
            "Epoch [5/15], batch: [52172/55810, loss: 5.2530]\n",
            "Epoch [5/15], batch: [52173/55810, loss: 5.4164]\n",
            "Epoch [5/15], batch: [52174/55810, loss: 6.4064]\n",
            "Epoch [5/15], batch: [52175/55810, loss: 5.1704]\n",
            "Epoch [5/15], batch: [52176/55810, loss: 6.2224]\n",
            "Epoch [5/15], batch: [52177/55810, loss: 7.1955]\n",
            "Epoch [5/15], batch: [52178/55810, loss: 6.6546]\n",
            "Epoch [5/15], batch: [52179/55810, loss: 7.6648]\n",
            "Epoch [5/15], batch: [52180/55810, loss: 8.7109]\n",
            "Epoch [5/15], batch: [52181/55810, loss: 6.7949]\n",
            "Epoch [5/15], batch: [52182/55810, loss: 4.2455]\n",
            "Epoch [5/15], batch: [52183/55810, loss: 6.2846]\n",
            "Epoch [5/15], batch: [52184/55810, loss: 5.3344]\n",
            "Epoch [5/15], batch: [52185/55810, loss: 6.5098]\n",
            "Epoch [5/15], batch: [52186/55810, loss: 6.4030]\n",
            "Epoch [5/15], batch: [52187/55810, loss: 6.5397]\n",
            "Epoch [5/15], batch: [52188/55810, loss: 6.2926]\n",
            "Epoch [5/15], batch: [52189/55810, loss: 7.6716]\n",
            "Epoch [5/15], batch: [52190/55810, loss: 6.2224]\n",
            "Epoch [5/15], batch: [52191/55810, loss: 7.0865]\n",
            "Epoch [5/15], batch: [52192/55810, loss: 7.4875]\n",
            "Epoch [5/15], batch: [52193/55810, loss: 6.1385]\n",
            "Epoch [5/15], batch: [52194/55810, loss: 7.6429]\n",
            "Epoch [5/15], batch: [52195/55810, loss: 6.9017]\n",
            "Epoch [5/15], batch: [52196/55810, loss: 5.9875]\n",
            "Epoch [5/15], batch: [52197/55810, loss: 5.0126]\n",
            "Epoch [5/15], batch: [52198/55810, loss: 5.4176]\n",
            "Epoch [5/15], batch: [52199/55810, loss: 6.2798]\n",
            "Epoch [5/15], batch: [52200/55810, loss: 6.6221]\n",
            "Epoch [5/15], batch: [52201/55810, loss: 6.8697]\n",
            "Epoch [5/15], batch: [52202/55810, loss: 7.0559]\n",
            "Epoch [5/15], batch: [52203/55810, loss: 5.8067]\n",
            "Epoch [5/15], batch: [52204/55810, loss: 5.9203]\n",
            "Epoch [5/15], batch: [52205/55810, loss: 6.8126]\n",
            "Epoch [5/15], batch: [52206/55810, loss: 7.6657]\n",
            "Epoch [5/15], batch: [52207/55810, loss: 6.7013]\n",
            "Epoch [5/15], batch: [52208/55810, loss: 5.9744]\n",
            "Epoch [5/15], batch: [52209/55810, loss: 6.3472]\n",
            "Epoch [5/15], batch: [52210/55810, loss: 6.9180]\n",
            "Epoch [5/15], batch: [52211/55810, loss: 5.8264]\n",
            "Epoch [5/15], batch: [52212/55810, loss: 5.6074]\n",
            "Epoch [5/15], batch: [52213/55810, loss: 6.3724]\n",
            "Epoch [5/15], batch: [52214/55810, loss: 7.0750]\n",
            "Epoch [5/15], batch: [52215/55810, loss: 5.7580]\n",
            "Epoch [5/15], batch: [52216/55810, loss: 6.2964]\n",
            "Epoch [5/15], batch: [52217/55810, loss: 7.2094]\n",
            "Epoch [5/15], batch: [52218/55810, loss: 4.0593]\n",
            "Epoch [5/15], batch: [52219/55810, loss: 7.2731]\n",
            "Epoch [5/15], batch: [52220/55810, loss: 7.7166]\n",
            "Epoch [5/15], batch: [52221/55810, loss: 6.3386]\n",
            "Epoch [5/15], batch: [52222/55810, loss: 6.2711]\n",
            "Epoch [5/15], batch: [52223/55810, loss: 7.9029]\n",
            "Epoch [5/15], batch: [52224/55810, loss: 5.4318]\n",
            "Epoch [5/15], batch: [52225/55810, loss: 5.7947]\n",
            "Epoch [5/15], batch: [52226/55810, loss: 5.3829]\n",
            "Epoch [5/15], batch: [52227/55810, loss: 6.3544]\n",
            "Epoch [5/15], batch: [52228/55810, loss: 4.4681]\n",
            "Epoch [5/15], batch: [52229/55810, loss: 5.9458]\n",
            "Epoch [5/15], batch: [52230/55810, loss: 6.2847]\n",
            "Epoch [5/15], batch: [52231/55810, loss: 6.9254]\n",
            "Epoch [5/15], batch: [52232/55810, loss: 5.0683]\n",
            "Epoch [5/15], batch: [52233/55810, loss: 6.1977]\n",
            "Epoch [5/15], batch: [52234/55810, loss: 6.2599]\n",
            "Epoch [5/15], batch: [52235/55810, loss: 5.6948]\n",
            "Epoch [5/15], batch: [52236/55810, loss: 6.5840]\n",
            "Epoch [5/15], batch: [52237/55810, loss: 6.6542]\n",
            "Epoch [5/15], batch: [52238/55810, loss: 6.3213]\n",
            "Epoch [5/15], batch: [52239/55810, loss: 5.6265]\n",
            "Epoch [5/15], batch: [52240/55810, loss: 7.3214]\n",
            "Epoch [5/15], batch: [52241/55810, loss: 7.4350]\n",
            "Epoch [5/15], batch: [52242/55810, loss: 7.0115]\n",
            "Epoch [5/15], batch: [52243/55810, loss: 7.7150]\n",
            "Epoch [5/15], batch: [52244/55810, loss: 8.0821]\n",
            "Epoch [5/15], batch: [52245/55810, loss: 7.6774]\n",
            "Epoch [5/15], batch: [52246/55810, loss: 5.5672]\n",
            "Epoch [5/15], batch: [52247/55810, loss: 8.1484]\n",
            "Epoch [5/15], batch: [52248/55810, loss: 4.9562]\n",
            "Epoch [5/15], batch: [52249/55810, loss: 5.7686]\n",
            "Epoch [5/15], batch: [52250/55810, loss: 7.3694]\n",
            "Epoch [5/15], batch: [52251/55810, loss: 6.3163]\n",
            "Epoch [5/15], batch: [52252/55810, loss: 6.0011]\n",
            "Epoch [5/15], batch: [52253/55810, loss: 6.2720]\n",
            "Epoch [5/15], batch: [52254/55810, loss: 6.8788]\n",
            "Epoch [5/15], batch: [52255/55810, loss: 5.7434]\n",
            "Epoch [5/15], batch: [52256/55810, loss: 7.4452]\n",
            "Epoch [5/15], batch: [52257/55810, loss: 7.1851]\n",
            "Epoch [5/15], batch: [52258/55810, loss: 5.9966]\n",
            "Epoch [5/15], batch: [52259/55810, loss: 4.7994]\n",
            "Epoch [5/15], batch: [52260/55810, loss: 8.1432]\n",
            "Epoch [5/15], batch: [52261/55810, loss: 8.5013]\n",
            "Epoch [5/15], batch: [52262/55810, loss: 6.1256]\n",
            "Epoch [5/15], batch: [52263/55810, loss: 9.1595]\n",
            "Epoch [5/15], batch: [52264/55810, loss: 5.9325]\n",
            "Epoch [5/15], batch: [52265/55810, loss: 6.9517]\n",
            "Epoch [5/15], batch: [52266/55810, loss: 5.8324]\n",
            "Epoch [5/15], batch: [52267/55810, loss: 8.3521]\n",
            "Epoch [5/15], batch: [52268/55810, loss: 6.0240]\n",
            "Epoch [5/15], batch: [52269/55810, loss: 7.1238]\n",
            "Epoch [5/15], batch: [52270/55810, loss: 7.3680]\n",
            "Epoch [5/15], batch: [52271/55810, loss: 6.7502]\n",
            "Epoch [5/15], batch: [52272/55810, loss: 7.4976]\n",
            "Epoch [5/15], batch: [52273/55810, loss: 8.7167]\n",
            "Epoch [5/15], batch: [52274/55810, loss: 8.5386]\n",
            "Epoch [5/15], batch: [52275/55810, loss: 8.5100]\n",
            "Epoch [5/15], batch: [52276/55810, loss: 5.8135]\n",
            "Epoch [5/15], batch: [52277/55810, loss: 6.5979]\n",
            "Epoch [5/15], batch: [52278/55810, loss: 8.5399]\n",
            "Epoch [5/15], batch: [52279/55810, loss: 6.7521]\n",
            "Epoch [5/15], batch: [52280/55810, loss: 7.3349]\n",
            "Epoch [5/15], batch: [52281/55810, loss: 6.4526]\n",
            "Epoch [5/15], batch: [52282/55810, loss: 6.1077]\n",
            "Epoch [5/15], batch: [52283/55810, loss: 8.0227]\n",
            "Epoch [5/15], batch: [52284/55810, loss: 6.6745]\n",
            "Epoch [5/15], batch: [52285/55810, loss: 7.8723]\n",
            "Epoch [5/15], batch: [52286/55810, loss: 7.3435]\n",
            "Epoch [5/15], batch: [52287/55810, loss: 7.0565]\n",
            "Epoch [5/15], batch: [52288/55810, loss: 7.7220]\n",
            "Epoch [5/15], batch: [52289/55810, loss: 7.5072]\n",
            "Epoch [5/15], batch: [52290/55810, loss: 6.8013]\n",
            "Epoch [5/15], batch: [52291/55810, loss: 6.6782]\n",
            "Epoch [5/15], batch: [52292/55810, loss: 7.5216]\n",
            "Epoch [5/15], batch: [52293/55810, loss: 7.8544]\n",
            "Epoch [5/15], batch: [52294/55810, loss: 8.2122]\n",
            "Epoch [5/15], batch: [52295/55810, loss: 4.9664]\n",
            "Epoch [5/15], batch: [52296/55810, loss: 5.0294]\n",
            "Epoch [5/15], batch: [52297/55810, loss: 6.2646]\n",
            "Epoch [5/15], batch: [52298/55810, loss: 4.5280]\n",
            "Epoch [5/15], batch: [52299/55810, loss: 6.4977]\n",
            "Epoch [5/15], batch: [52300/55810, loss: 4.9450]\n",
            "Epoch [5/15], batch: [52301/55810, loss: 5.3395]\n",
            "Epoch [5/15], batch: [52302/55810, loss: 5.5762]\n",
            "Epoch [5/15], batch: [52303/55810, loss: 6.6670]\n",
            "Epoch [5/15], batch: [52304/55810, loss: 6.5444]\n",
            "Epoch [5/15], batch: [52305/55810, loss: 6.5970]\n",
            "Epoch [5/15], batch: [52306/55810, loss: 6.1815]\n",
            "Epoch [5/15], batch: [52307/55810, loss: 6.6186]\n",
            "Epoch [5/15], batch: [52308/55810, loss: 8.2672]\n",
            "Epoch [5/15], batch: [52309/55810, loss: 6.8335]\n",
            "Epoch [5/15], batch: [52310/55810, loss: 6.1399]\n",
            "Epoch [5/15], batch: [52311/55810, loss: 6.4346]\n",
            "Epoch [5/15], batch: [52312/55810, loss: 5.6634]\n",
            "Epoch [5/15], batch: [52313/55810, loss: 6.5552]\n",
            "Epoch [5/15], batch: [52314/55810, loss: 5.8993]\n",
            "Epoch [5/15], batch: [52315/55810, loss: 5.6288]\n",
            "Epoch [5/15], batch: [52316/55810, loss: 6.8174]\n",
            "Epoch [5/15], batch: [52317/55810, loss: 6.6315]\n",
            "Epoch [5/15], batch: [52318/55810, loss: 6.3965]\n",
            "Epoch [5/15], batch: [52319/55810, loss: 5.5641]\n",
            "Epoch [5/15], batch: [52320/55810, loss: 4.2589]\n",
            "Epoch [5/15], batch: [52321/55810, loss: 4.1539]\n",
            "Epoch [5/15], batch: [52322/55810, loss: 5.5049]\n",
            "Epoch [5/15], batch: [52323/55810, loss: 4.3593]\n",
            "Epoch [5/15], batch: [52324/55810, loss: 5.8688]\n",
            "Epoch [5/15], batch: [52325/55810, loss: 5.8161]\n",
            "Epoch [5/15], batch: [52326/55810, loss: 5.1939]\n",
            "Epoch [5/15], batch: [52327/55810, loss: 5.9077]\n",
            "Epoch [5/15], batch: [52328/55810, loss: 6.7407]\n",
            "Epoch [5/15], batch: [52329/55810, loss: 5.6952]\n",
            "Epoch [5/15], batch: [52330/55810, loss: 8.2710]\n",
            "Epoch [5/15], batch: [52331/55810, loss: 7.3404]\n",
            "Epoch [5/15], batch: [52332/55810, loss: 6.7437]\n",
            "Epoch [5/15], batch: [52333/55810, loss: 6.8671]\n",
            "Epoch [5/15], batch: [52334/55810, loss: 7.7464]\n",
            "Epoch [5/15], batch: [52335/55810, loss: 6.6565]\n",
            "Epoch [5/15], batch: [52336/55810, loss: 7.8607]\n",
            "Epoch [5/15], batch: [52337/55810, loss: 6.6502]\n",
            "Epoch [5/15], batch: [52338/55810, loss: 7.7288]\n",
            "Epoch [5/15], batch: [52339/55810, loss: 7.1155]\n",
            "Epoch [5/15], batch: [52340/55810, loss: 8.1044]\n",
            "Epoch [5/15], batch: [52341/55810, loss: 7.6138]\n",
            "Epoch [5/15], batch: [52342/55810, loss: 6.8842]\n",
            "Epoch [5/15], batch: [52343/55810, loss: 7.0837]\n",
            "Epoch [5/15], batch: [52344/55810, loss: 5.7674]\n",
            "Epoch [5/15], batch: [52345/55810, loss: 6.6406]\n",
            "Epoch [5/15], batch: [52346/55810, loss: 5.7474]\n",
            "Epoch [5/15], batch: [52347/55810, loss: 6.6053]\n",
            "Epoch [5/15], batch: [52348/55810, loss: 7.7403]\n",
            "Epoch [5/15], batch: [52349/55810, loss: 5.0102]\n",
            "Epoch [5/15], batch: [52350/55810, loss: 6.5719]\n",
            "Epoch [5/15], batch: [52351/55810, loss: 6.3760]\n",
            "Epoch [5/15], batch: [52352/55810, loss: 5.7138]\n",
            "Epoch [5/15], batch: [52353/55810, loss: 4.7836]\n",
            "Epoch [5/15], batch: [52354/55810, loss: 4.0690]\n",
            "Epoch [5/15], batch: [52355/55810, loss: 4.4378]\n",
            "Epoch [5/15], batch: [52356/55810, loss: 5.2474]\n",
            "Epoch [5/15], batch: [52357/55810, loss: 5.1780]\n",
            "Epoch [5/15], batch: [52358/55810, loss: 6.3226]\n",
            "Epoch [5/15], batch: [52359/55810, loss: 7.0244]\n",
            "Epoch [5/15], batch: [52360/55810, loss: 4.2000]\n",
            "Epoch [5/15], batch: [52361/55810, loss: 5.8927]\n",
            "Epoch [5/15], batch: [52362/55810, loss: 6.1078]\n",
            "Epoch [5/15], batch: [52363/55810, loss: 6.3732]\n",
            "Epoch [5/15], batch: [52364/55810, loss: 6.7605]\n",
            "Epoch [5/15], batch: [52365/55810, loss: 5.8510]\n",
            "Epoch [5/15], batch: [52366/55810, loss: 7.3208]\n",
            "Epoch [5/15], batch: [52367/55810, loss: 6.2245]\n",
            "Epoch [5/15], batch: [52368/55810, loss: 8.0609]\n",
            "Epoch [5/15], batch: [52369/55810, loss: 5.3871]\n",
            "Epoch [5/15], batch: [52370/55810, loss: 7.7097]\n",
            "Epoch [5/15], batch: [52371/55810, loss: 7.0961]\n",
            "Epoch [5/15], batch: [52372/55810, loss: 7.9238]\n",
            "Epoch [5/15], batch: [52373/55810, loss: 6.9988]\n",
            "Epoch [5/15], batch: [52374/55810, loss: 7.2449]\n",
            "Epoch [5/15], batch: [52375/55810, loss: 7.8936]\n",
            "Epoch [5/15], batch: [52376/55810, loss: 7.2024]\n",
            "Epoch [5/15], batch: [52377/55810, loss: 5.0122]\n",
            "Epoch [5/15], batch: [52378/55810, loss: 7.8268]\n",
            "Epoch [5/15], batch: [52379/55810, loss: 6.7463]\n",
            "Epoch [5/15], batch: [52380/55810, loss: 3.8928]\n",
            "Epoch [5/15], batch: [52381/55810, loss: 7.7855]\n",
            "Epoch [5/15], batch: [52382/55810, loss: 7.6650]\n",
            "Epoch [5/15], batch: [52383/55810, loss: 4.2371]\n",
            "Epoch [5/15], batch: [52384/55810, loss: 6.4084]\n",
            "Epoch [5/15], batch: [52385/55810, loss: 5.3966]\n",
            "Epoch [5/15], batch: [52386/55810, loss: 9.2695]\n",
            "Epoch [5/15], batch: [52387/55810, loss: 5.7383]\n",
            "Epoch [5/15], batch: [52388/55810, loss: 6.8624]\n",
            "Epoch [5/15], batch: [52389/55810, loss: 7.1032]\n",
            "Epoch [5/15], batch: [52390/55810, loss: 6.2691]\n",
            "Epoch [5/15], batch: [52391/55810, loss: 5.2132]\n",
            "Epoch [5/15], batch: [52392/55810, loss: 7.9761]\n",
            "Epoch [5/15], batch: [52393/55810, loss: 8.3542]\n",
            "Epoch [5/15], batch: [52394/55810, loss: 7.4210]\n",
            "Epoch [5/15], batch: [52395/55810, loss: 7.2768]\n",
            "Epoch [5/15], batch: [52396/55810, loss: 6.6353]\n",
            "Epoch [5/15], batch: [52397/55810, loss: 7.0057]\n",
            "Epoch [5/15], batch: [52398/55810, loss: 5.1648]\n",
            "Epoch [5/15], batch: [52399/55810, loss: 6.0781]\n",
            "Epoch [5/15], batch: [52400/55810, loss: 7.5044]\n",
            "Epoch [5/15], batch: [52401/55810, loss: 7.1877]\n",
            "Epoch [5/15], batch: [52402/55810, loss: 6.8835]\n",
            "Epoch [5/15], batch: [52403/55810, loss: 6.7087]\n",
            "Epoch [5/15], batch: [52404/55810, loss: 5.6730]\n",
            "Epoch [5/15], batch: [52405/55810, loss: 3.6343]\n",
            "Epoch [5/15], batch: [52406/55810, loss: 4.9681]\n",
            "Epoch [5/15], batch: [52407/55810, loss: 5.2761]\n",
            "Epoch [5/15], batch: [52408/55810, loss: 5.0846]\n",
            "Epoch [5/15], batch: [52409/55810, loss: 6.1771]\n",
            "Epoch [5/15], batch: [52410/55810, loss: 6.0537]\n",
            "Epoch [5/15], batch: [52411/55810, loss: 4.4778]\n",
            "Epoch [5/15], batch: [52412/55810, loss: 5.6285]\n",
            "Epoch [5/15], batch: [52413/55810, loss: 7.0093]\n",
            "Epoch [5/15], batch: [52414/55810, loss: 4.9617]\n",
            "Epoch [5/15], batch: [52415/55810, loss: 6.7878]\n",
            "Epoch [5/15], batch: [52416/55810, loss: 6.1471]\n",
            "Epoch [5/15], batch: [52417/55810, loss: 6.5443]\n",
            "Epoch [5/15], batch: [52418/55810, loss: 5.0442]\n",
            "Epoch [5/15], batch: [52419/55810, loss: 5.9327]\n",
            "Epoch [5/15], batch: [52420/55810, loss: 7.0969]\n",
            "Epoch [5/15], batch: [52421/55810, loss: 7.0476]\n",
            "Epoch [5/15], batch: [52422/55810, loss: 5.7465]\n",
            "Epoch [5/15], batch: [52423/55810, loss: 5.2868]\n",
            "Epoch [5/15], batch: [52424/55810, loss: 5.8072]\n",
            "Epoch [5/15], batch: [52425/55810, loss: 5.4626]\n",
            "Epoch [5/15], batch: [52426/55810, loss: 6.1391]\n",
            "Epoch [5/15], batch: [52427/55810, loss: 6.9540]\n",
            "Epoch [5/15], batch: [52428/55810, loss: 7.0315]\n",
            "Epoch [5/15], batch: [52429/55810, loss: 5.9967]\n",
            "Epoch [5/15], batch: [52430/55810, loss: 7.7702]\n",
            "Epoch [5/15], batch: [52431/55810, loss: 6.0207]\n",
            "Epoch [5/15], batch: [52432/55810, loss: 5.8538]\n",
            "Epoch [5/15], batch: [52433/55810, loss: 7.7712]\n",
            "Epoch [5/15], batch: [52434/55810, loss: 6.4705]\n",
            "Epoch [5/15], batch: [52435/55810, loss: 6.3065]\n",
            "Epoch [5/15], batch: [52436/55810, loss: 4.6050]\n",
            "Epoch [5/15], batch: [52437/55810, loss: 6.1145]\n",
            "Epoch [5/15], batch: [52438/55810, loss: 6.5707]\n",
            "Epoch [5/15], batch: [52439/55810, loss: 4.3891]\n",
            "Epoch [5/15], batch: [52440/55810, loss: 6.3674]\n",
            "Epoch [5/15], batch: [52441/55810, loss: 7.2863]\n",
            "Epoch [5/15], batch: [52442/55810, loss: 7.4330]\n",
            "Epoch [5/15], batch: [52443/55810, loss: 7.8000]\n",
            "Epoch [5/15], batch: [52444/55810, loss: 7.8192]\n",
            "Epoch [5/15], batch: [52445/55810, loss: 8.3842]\n",
            "Epoch [5/15], batch: [52446/55810, loss: 8.2571]\n",
            "Epoch [5/15], batch: [52447/55810, loss: 7.4767]\n",
            "Epoch [5/15], batch: [52448/55810, loss: 8.3095]\n",
            "Epoch [5/15], batch: [52449/55810, loss: 7.6757]\n",
            "Epoch [5/15], batch: [52450/55810, loss: 8.0851]\n",
            "Epoch [5/15], batch: [52451/55810, loss: 6.6452]\n",
            "Epoch [5/15], batch: [52452/55810, loss: 5.8916]\n",
            "Epoch [5/15], batch: [52453/55810, loss: 6.9999]\n",
            "Epoch [5/15], batch: [52454/55810, loss: 6.8573]\n",
            "Epoch [5/15], batch: [52455/55810, loss: 8.2648]\n",
            "Epoch [5/15], batch: [52456/55810, loss: 5.5352]\n",
            "Epoch [5/15], batch: [52457/55810, loss: 6.7065]\n",
            "Epoch [5/15], batch: [52458/55810, loss: 7.5027]\n",
            "Epoch [5/15], batch: [52459/55810, loss: 5.0766]\n",
            "Epoch [5/15], batch: [52460/55810, loss: 4.6677]\n",
            "Epoch [5/15], batch: [52461/55810, loss: 5.3065]\n",
            "Epoch [5/15], batch: [52462/55810, loss: 7.5961]\n",
            "Epoch [5/15], batch: [52463/55810, loss: 7.7208]\n",
            "Epoch [5/15], batch: [52464/55810, loss: 5.9054]\n",
            "Epoch [5/15], batch: [52465/55810, loss: 5.0136]\n",
            "Epoch [5/15], batch: [52466/55810, loss: 5.7397]\n",
            "Epoch [5/15], batch: [52467/55810, loss: 7.0318]\n",
            "Epoch [5/15], batch: [52468/55810, loss: 4.9218]\n",
            "Epoch [5/15], batch: [52469/55810, loss: 6.0508]\n",
            "Epoch [5/15], batch: [52470/55810, loss: 4.0491]\n",
            "Epoch [5/15], batch: [52471/55810, loss: 5.9970]\n",
            "Epoch [5/15], batch: [52472/55810, loss: 4.3017]\n",
            "Epoch [5/15], batch: [52473/55810, loss: 5.4930]\n",
            "Epoch [5/15], batch: [52474/55810, loss: 5.8711]\n",
            "Epoch [5/15], batch: [52475/55810, loss: 5.5544]\n",
            "Epoch [5/15], batch: [52476/55810, loss: 6.5085]\n",
            "Epoch [5/15], batch: [52477/55810, loss: 6.6975]\n",
            "Epoch [5/15], batch: [52478/55810, loss: 4.8458]\n",
            "Epoch [5/15], batch: [52479/55810, loss: 6.0940]\n",
            "Epoch [5/15], batch: [52480/55810, loss: 5.6840]\n",
            "Epoch [5/15], batch: [52481/55810, loss: 4.7869]\n",
            "Epoch [5/15], batch: [52482/55810, loss: 6.0379]\n",
            "Epoch [5/15], batch: [52483/55810, loss: 4.8672]\n",
            "Epoch [5/15], batch: [52484/55810, loss: 6.3230]\n",
            "Epoch [5/15], batch: [52485/55810, loss: 7.2605]\n",
            "Epoch [5/15], batch: [52486/55810, loss: 6.8673]\n",
            "Epoch [5/15], batch: [52487/55810, loss: 6.2094]\n",
            "Epoch [5/15], batch: [52488/55810, loss: 6.9679]\n",
            "Epoch [5/15], batch: [52489/55810, loss: 6.1081]\n",
            "Epoch [5/15], batch: [52490/55810, loss: 7.5609]\n",
            "Epoch [5/15], batch: [52491/55810, loss: 7.7855]\n",
            "Epoch [5/15], batch: [52492/55810, loss: 6.9660]\n",
            "Epoch [5/15], batch: [52493/55810, loss: 7.4466]\n",
            "Epoch [5/15], batch: [52494/55810, loss: 4.7086]\n",
            "Epoch [5/15], batch: [52495/55810, loss: 7.5388]\n",
            "Epoch [5/15], batch: [52496/55810, loss: 5.5175]\n",
            "Epoch [5/15], batch: [52497/55810, loss: 6.4486]\n",
            "Epoch [5/15], batch: [52498/55810, loss: 5.9668]\n",
            "Epoch [5/15], batch: [52499/55810, loss: 5.4261]\n",
            "Epoch [5/15], batch: [52500/55810, loss: 4.0300]\n",
            "Epoch [5/15], batch: [52501/55810, loss: 6.2341]\n",
            "Epoch [5/15], batch: [52502/55810, loss: 6.5209]\n",
            "Epoch [5/15], batch: [52503/55810, loss: 6.6525]\n",
            "Epoch [5/15], batch: [52504/55810, loss: 5.6329]\n",
            "Epoch [5/15], batch: [52505/55810, loss: 5.3352]\n",
            "Epoch [5/15], batch: [52506/55810, loss: 5.2679]\n",
            "Epoch [5/15], batch: [52507/55810, loss: 6.7195]\n",
            "Epoch [5/15], batch: [52508/55810, loss: 6.0465]\n",
            "Epoch [5/15], batch: [52509/55810, loss: 6.5908]\n",
            "Epoch [5/15], batch: [52510/55810, loss: 7.7507]\n",
            "Epoch [5/15], batch: [52511/55810, loss: 7.7464]\n",
            "Epoch [5/15], batch: [52512/55810, loss: 7.0865]\n",
            "Epoch [5/15], batch: [52513/55810, loss: 6.3418]\n",
            "Epoch [5/15], batch: [52514/55810, loss: 5.7802]\n",
            "Epoch [5/15], batch: [52515/55810, loss: 8.0187]\n",
            "Epoch [5/15], batch: [52516/55810, loss: 6.6985]\n",
            "Epoch [5/15], batch: [52517/55810, loss: 6.4675]\n",
            "Epoch [5/15], batch: [52518/55810, loss: 6.5436]\n",
            "Epoch [5/15], batch: [52519/55810, loss: 6.8566]\n",
            "Epoch [5/15], batch: [52520/55810, loss: 7.5172]\n",
            "Epoch [5/15], batch: [52521/55810, loss: 7.3179]\n",
            "Epoch [5/15], batch: [52522/55810, loss: 7.8039]\n",
            "Epoch [5/15], batch: [52523/55810, loss: 6.0743]\n",
            "Epoch [5/15], batch: [52524/55810, loss: 5.7233]\n",
            "Epoch [5/15], batch: [52525/55810, loss: 6.2672]\n",
            "Epoch [5/15], batch: [52526/55810, loss: 5.3778]\n",
            "Epoch [5/15], batch: [52527/55810, loss: 3.1380]\n",
            "Epoch [5/15], batch: [52528/55810, loss: 5.8194]\n",
            "Epoch [5/15], batch: [52529/55810, loss: 8.4422]\n",
            "Epoch [5/15], batch: [52530/55810, loss: 6.7708]\n",
            "Epoch [5/15], batch: [52531/55810, loss: 6.4685]\n",
            "Epoch [5/15], batch: [52532/55810, loss: 7.5171]\n",
            "Epoch [5/15], batch: [52533/55810, loss: 6.2321]\n",
            "Epoch [5/15], batch: [52534/55810, loss: 5.9488]\n",
            "Epoch [5/15], batch: [52535/55810, loss: 6.3196]\n",
            "Epoch [5/15], batch: [52536/55810, loss: 6.1299]\n",
            "Epoch [5/15], batch: [52537/55810, loss: 4.7978]\n",
            "Epoch [5/15], batch: [52538/55810, loss: 6.4724]\n",
            "Epoch [5/15], batch: [52539/55810, loss: 7.5334]\n",
            "Epoch [5/15], batch: [52540/55810, loss: 6.9416]\n",
            "Epoch [5/15], batch: [52541/55810, loss: 7.0620]\n",
            "Epoch [5/15], batch: [52542/55810, loss: 6.0540]\n",
            "Epoch [5/15], batch: [52543/55810, loss: 8.0252]\n",
            "Epoch [5/15], batch: [52544/55810, loss: 6.9557]\n",
            "Epoch [5/15], batch: [52545/55810, loss: 6.4920]\n",
            "Epoch [5/15], batch: [52546/55810, loss: 7.2800]\n",
            "Epoch [5/15], batch: [52547/55810, loss: 6.8328]\n",
            "Epoch [5/15], batch: [52548/55810, loss: 7.7790]\n",
            "Epoch [5/15], batch: [52549/55810, loss: 7.6322]\n",
            "Epoch [5/15], batch: [52550/55810, loss: 4.5182]\n",
            "Epoch [5/15], batch: [52551/55810, loss: 5.0625]\n",
            "Epoch [5/15], batch: [52552/55810, loss: 6.6734]\n",
            "Epoch [5/15], batch: [52553/55810, loss: 6.4084]\n",
            "Epoch [5/15], batch: [52554/55810, loss: 7.4471]\n",
            "Epoch [5/15], batch: [52555/55810, loss: 5.0683]\n",
            "Epoch [5/15], batch: [52556/55810, loss: 5.7161]\n",
            "Epoch [5/15], batch: [52557/55810, loss: 5.6774]\n",
            "Epoch [5/15], batch: [52558/55810, loss: 5.7964]\n",
            "Epoch [5/15], batch: [52559/55810, loss: 5.4115]\n",
            "Epoch [5/15], batch: [52560/55810, loss: 6.0281]\n",
            "Epoch [5/15], batch: [52561/55810, loss: 5.3705]\n",
            "Epoch [5/15], batch: [52562/55810, loss: 6.1210]\n",
            "Epoch [5/15], batch: [52563/55810, loss: 3.7978]\n",
            "Epoch [5/15], batch: [52564/55810, loss: 5.8165]\n",
            "Epoch [5/15], batch: [52565/55810, loss: 7.0498]\n",
            "Epoch [5/15], batch: [52566/55810, loss: 5.7599]\n",
            "Epoch [5/15], batch: [52567/55810, loss: 7.4862]\n",
            "Epoch [5/15], batch: [52568/55810, loss: 5.5486]\n",
            "Epoch [5/15], batch: [52569/55810, loss: 6.6621]\n",
            "Epoch [5/15], batch: [52570/55810, loss: 5.9775]\n",
            "Epoch [5/15], batch: [52571/55810, loss: 7.4463]\n",
            "Epoch [5/15], batch: [52572/55810, loss: 7.9088]\n",
            "Epoch [5/15], batch: [52573/55810, loss: 6.5646]\n",
            "Epoch [5/15], batch: [52574/55810, loss: 6.7567]\n",
            "Epoch [5/15], batch: [52575/55810, loss: 4.9621]\n",
            "Epoch [5/15], batch: [52576/55810, loss: 5.6343]\n",
            "Epoch [5/15], batch: [52577/55810, loss: 7.4787]\n",
            "Epoch [5/15], batch: [52578/55810, loss: 4.9620]\n",
            "Epoch [5/15], batch: [52579/55810, loss: 5.9701]\n",
            "Epoch [5/15], batch: [52580/55810, loss: 6.3023]\n",
            "Epoch [5/15], batch: [52581/55810, loss: 4.8277]\n",
            "Epoch [5/15], batch: [52582/55810, loss: 7.9571]\n",
            "Epoch [5/15], batch: [52583/55810, loss: 6.8920]\n",
            "Epoch [5/15], batch: [52584/55810, loss: 4.2678]\n",
            "Epoch [5/15], batch: [52585/55810, loss: 7.8156]\n",
            "Epoch [5/15], batch: [52586/55810, loss: 6.0103]\n",
            "Epoch [5/15], batch: [52587/55810, loss: 6.1535]\n",
            "Epoch [5/15], batch: [52588/55810, loss: 6.8501]\n",
            "Epoch [5/15], batch: [52589/55810, loss: 5.1546]\n",
            "Epoch [5/15], batch: [52590/55810, loss: 5.9200]\n",
            "Epoch [5/15], batch: [52591/55810, loss: 5.9708]\n",
            "Epoch [5/15], batch: [52592/55810, loss: 7.4130]\n",
            "Epoch [5/15], batch: [52593/55810, loss: 7.7229]\n",
            "Epoch [5/15], batch: [52594/55810, loss: 6.6193]\n",
            "Epoch [5/15], batch: [52595/55810, loss: 7.0572]\n",
            "Epoch [5/15], batch: [52596/55810, loss: 6.9252]\n",
            "Epoch [5/15], batch: [52597/55810, loss: 7.4289]\n",
            "Epoch [5/15], batch: [52598/55810, loss: 5.5526]\n",
            "Epoch [5/15], batch: [52599/55810, loss: 6.3303]\n",
            "Epoch [5/15], batch: [52600/55810, loss: 7.3364]\n",
            "Epoch [5/15], batch: [52601/55810, loss: 7.6778]\n",
            "Epoch [5/15], batch: [52602/55810, loss: 6.9294]\n",
            "Epoch [5/15], batch: [52603/55810, loss: 6.4587]\n",
            "Epoch [5/15], batch: [52604/55810, loss: 7.9156]\n",
            "Epoch [5/15], batch: [52605/55810, loss: 7.3742]\n",
            "Epoch [5/15], batch: [52606/55810, loss: 5.8756]\n",
            "Epoch [5/15], batch: [52607/55810, loss: 7.9628]\n",
            "Epoch [5/15], batch: [52608/55810, loss: 5.1871]\n",
            "Epoch [5/15], batch: [52609/55810, loss: 7.6361]\n",
            "Epoch [5/15], batch: [52610/55810, loss: 7.1478]\n",
            "Epoch [5/15], batch: [52611/55810, loss: 7.2229]\n",
            "Epoch [5/15], batch: [52612/55810, loss: 5.9137]\n",
            "Epoch [5/15], batch: [52613/55810, loss: 6.1850]\n",
            "Epoch [5/15], batch: [52614/55810, loss: 6.9125]\n",
            "Epoch [5/15], batch: [52615/55810, loss: 6.1555]\n",
            "Epoch [5/15], batch: [52616/55810, loss: 6.7821]\n",
            "Epoch [5/15], batch: [52617/55810, loss: 6.2028]\n",
            "Epoch [5/15], batch: [52618/55810, loss: 6.0761]\n",
            "Epoch [5/15], batch: [52619/55810, loss: 6.8982]\n",
            "Epoch [5/15], batch: [52620/55810, loss: 6.2467]\n",
            "Epoch [5/15], batch: [52621/55810, loss: 5.2006]\n",
            "Epoch [5/15], batch: [52622/55810, loss: 5.3704]\n",
            "Epoch [5/15], batch: [52623/55810, loss: 6.2183]\n",
            "Epoch [5/15], batch: [52624/55810, loss: 5.7589]\n",
            "Epoch [5/15], batch: [52625/55810, loss: 6.5135]\n",
            "Epoch [5/15], batch: [52626/55810, loss: 6.1588]\n",
            "Epoch [5/15], batch: [52627/55810, loss: 8.5380]\n",
            "Epoch [5/15], batch: [52628/55810, loss: 6.4473]\n",
            "Epoch [5/15], batch: [52629/55810, loss: 6.3932]\n",
            "Epoch [5/15], batch: [52630/55810, loss: 7.3572]\n",
            "Epoch [5/15], batch: [52631/55810, loss: 6.9491]\n",
            "Epoch [5/15], batch: [52632/55810, loss: 4.3346]\n",
            "Epoch [5/15], batch: [52633/55810, loss: 5.3369]\n",
            "Epoch [5/15], batch: [52634/55810, loss: 7.5735]\n",
            "Epoch [5/15], batch: [52635/55810, loss: 4.8685]\n",
            "Epoch [5/15], batch: [52636/55810, loss: 4.9381]\n",
            "Epoch [5/15], batch: [52637/55810, loss: 5.5969]\n",
            "Epoch [5/15], batch: [52638/55810, loss: 7.0545]\n",
            "Epoch [5/15], batch: [52639/55810, loss: 6.4409]\n",
            "Epoch [5/15], batch: [52640/55810, loss: 6.1575]\n",
            "Epoch [5/15], batch: [52641/55810, loss: 5.7727]\n",
            "Epoch [5/15], batch: [52642/55810, loss: 6.9645]\n",
            "Epoch [5/15], batch: [52643/55810, loss: 6.5037]\n",
            "Epoch [5/15], batch: [52644/55810, loss: 5.6074]\n",
            "Epoch [5/15], batch: [52645/55810, loss: 7.2384]\n",
            "Epoch [5/15], batch: [52646/55810, loss: 6.1943]\n",
            "Epoch [5/15], batch: [52647/55810, loss: 7.8875]\n",
            "Epoch [5/15], batch: [52648/55810, loss: 6.9376]\n",
            "Epoch [5/15], batch: [52649/55810, loss: 7.7973]\n",
            "Epoch [5/15], batch: [52650/55810, loss: 7.2627]\n",
            "Epoch [5/15], batch: [52651/55810, loss: 6.2172]\n",
            "Epoch [5/15], batch: [52652/55810, loss: 7.9071]\n",
            "Epoch [5/15], batch: [52653/55810, loss: 7.9534]\n",
            "Epoch [5/15], batch: [52654/55810, loss: 6.9029]\n",
            "Epoch [5/15], batch: [52655/55810, loss: 7.4981]\n",
            "Epoch [5/15], batch: [52656/55810, loss: 5.9737]\n",
            "Epoch [5/15], batch: [52657/55810, loss: 6.4731]\n",
            "Epoch [5/15], batch: [52658/55810, loss: 4.5449]\n",
            "Epoch [5/15], batch: [52659/55810, loss: 6.9184]\n",
            "Epoch [5/15], batch: [52660/55810, loss: 8.8036]\n",
            "Epoch [5/15], batch: [52661/55810, loss: 5.3390]\n",
            "Epoch [5/15], batch: [52662/55810, loss: 6.0184]\n",
            "Epoch [5/15], batch: [52663/55810, loss: 6.3482]\n",
            "Epoch [5/15], batch: [52664/55810, loss: 5.9918]\n",
            "Epoch [5/15], batch: [52665/55810, loss: 7.0932]\n",
            "Epoch [5/15], batch: [52666/55810, loss: 5.7675]\n",
            "Epoch [5/15], batch: [52667/55810, loss: 6.3931]\n",
            "Epoch [5/15], batch: [52668/55810, loss: 6.7730]\n",
            "Epoch [5/15], batch: [52669/55810, loss: 6.3666]\n",
            "Epoch [5/15], batch: [52670/55810, loss: 6.9772]\n",
            "Epoch [5/15], batch: [52671/55810, loss: 5.9395]\n",
            "Epoch [5/15], batch: [52672/55810, loss: 7.8668]\n",
            "Epoch [5/15], batch: [52673/55810, loss: 6.9766]\n",
            "Epoch [5/15], batch: [52674/55810, loss: 7.1213]\n",
            "Epoch [5/15], batch: [52675/55810, loss: 5.9819]\n",
            "Epoch [5/15], batch: [52676/55810, loss: 7.4596]\n",
            "Epoch [5/15], batch: [52677/55810, loss: 7.0358]\n",
            "Epoch [5/15], batch: [52678/55810, loss: 7.3383]\n",
            "Epoch [5/15], batch: [52679/55810, loss: 6.9438]\n",
            "Epoch [5/15], batch: [52680/55810, loss: 5.8455]\n",
            "Epoch [5/15], batch: [52681/55810, loss: 8.0454]\n",
            "Epoch [5/15], batch: [52682/55810, loss: 8.7234]\n",
            "Epoch [5/15], batch: [52683/55810, loss: 7.1870]\n",
            "Epoch [5/15], batch: [52684/55810, loss: 5.9313]\n",
            "Epoch [5/15], batch: [52685/55810, loss: 7.5758]\n",
            "Epoch [5/15], batch: [52686/55810, loss: 7.9419]\n",
            "Epoch [5/15], batch: [52687/55810, loss: 9.4157]\n",
            "Epoch [5/15], batch: [52688/55810, loss: 6.9950]\n",
            "Epoch [5/15], batch: [52689/55810, loss: 7.3182]\n",
            "Epoch [5/15], batch: [52690/55810, loss: 7.3573]\n",
            "Epoch [5/15], batch: [52691/55810, loss: 6.3031]\n",
            "Epoch [5/15], batch: [52692/55810, loss: 7.4105]\n",
            "Epoch [5/15], batch: [52693/55810, loss: 6.2713]\n",
            "Epoch [5/15], batch: [52694/55810, loss: 8.0098]\n",
            "Epoch [5/15], batch: [52695/55810, loss: 6.6118]\n",
            "Epoch [5/15], batch: [52696/55810, loss: 7.1007]\n",
            "Epoch [5/15], batch: [52697/55810, loss: 7.5927]\n",
            "Epoch [5/15], batch: [52698/55810, loss: 7.1385]\n",
            "Epoch [5/15], batch: [52699/55810, loss: 8.0539]\n",
            "Epoch [5/15], batch: [52700/55810, loss: 5.9526]\n",
            "Epoch [5/15], batch: [52701/55810, loss: 6.9532]\n",
            "Epoch [5/15], batch: [52702/55810, loss: 7.3901]\n",
            "Epoch [5/15], batch: [52703/55810, loss: 4.7458]\n",
            "Epoch [5/15], batch: [52704/55810, loss: 5.9641]\n",
            "Epoch [5/15], batch: [52705/55810, loss: 6.0243]\n",
            "Epoch [5/15], batch: [52706/55810, loss: 6.9993]\n",
            "Epoch [5/15], batch: [52707/55810, loss: 7.8707]\n",
            "Epoch [5/15], batch: [52708/55810, loss: 7.3796]\n",
            "Epoch [5/15], batch: [52709/55810, loss: 6.6970]\n",
            "Epoch [5/15], batch: [52710/55810, loss: 6.6850]\n",
            "Epoch [5/15], batch: [52711/55810, loss: 7.1882]\n",
            "Epoch [5/15], batch: [52712/55810, loss: 6.1800]\n",
            "Epoch [5/15], batch: [52713/55810, loss: 7.2685]\n",
            "Epoch [5/15], batch: [52714/55810, loss: 7.3247]\n",
            "Epoch [5/15], batch: [52715/55810, loss: 6.5689]\n",
            "Epoch [5/15], batch: [52716/55810, loss: 6.8443]\n",
            "Epoch [5/15], batch: [52717/55810, loss: 6.1421]\n",
            "Epoch [5/15], batch: [52718/55810, loss: 6.7356]\n",
            "Epoch [5/15], batch: [52719/55810, loss: 6.3257]\n",
            "Epoch [5/15], batch: [52720/55810, loss: 7.6649]\n",
            "Epoch [5/15], batch: [52721/55810, loss: 8.3347]\n",
            "Epoch [5/15], batch: [52722/55810, loss: 7.5456]\n",
            "Epoch [5/15], batch: [52723/55810, loss: 7.8293]\n",
            "Epoch [5/15], batch: [52724/55810, loss: 7.5222]\n",
            "Epoch [5/15], batch: [52725/55810, loss: 7.0894]\n",
            "Epoch [5/15], batch: [52726/55810, loss: 7.7322]\n",
            "Epoch [5/15], batch: [52727/55810, loss: 7.0998]\n",
            "Epoch [5/15], batch: [52728/55810, loss: 7.3527]\n",
            "Epoch [5/15], batch: [52729/55810, loss: 5.5667]\n",
            "Epoch [5/15], batch: [52730/55810, loss: 6.1433]\n",
            "Epoch [5/15], batch: [52731/55810, loss: 7.4402]\n",
            "Epoch [5/15], batch: [52732/55810, loss: 8.5557]\n",
            "Epoch [5/15], batch: [52733/55810, loss: 5.2511]\n",
            "Epoch [5/15], batch: [52734/55810, loss: 4.9788]\n",
            "Epoch [5/15], batch: [52735/55810, loss: 6.7017]\n",
            "Epoch [5/15], batch: [52736/55810, loss: 6.3331]\n",
            "Epoch [5/15], batch: [52737/55810, loss: 6.5573]\n",
            "Epoch [5/15], batch: [52738/55810, loss: 5.3654]\n",
            "Epoch [5/15], batch: [52739/55810, loss: 5.8801]\n",
            "Epoch [5/15], batch: [52740/55810, loss: 7.2660]\n",
            "Epoch [5/15], batch: [52741/55810, loss: 5.7940]\n",
            "Epoch [5/15], batch: [52742/55810, loss: 6.4850]\n",
            "Epoch [5/15], batch: [52743/55810, loss: 6.2754]\n",
            "Epoch [5/15], batch: [52744/55810, loss: 4.5293]\n",
            "Epoch [5/15], batch: [52745/55810, loss: 6.4292]\n",
            "Epoch [5/15], batch: [52746/55810, loss: 4.9975]\n",
            "Epoch [5/15], batch: [52747/55810, loss: 6.1179]\n",
            "Epoch [5/15], batch: [52748/55810, loss: 4.9913]\n",
            "Epoch [5/15], batch: [52749/55810, loss: 7.2289]\n",
            "Epoch [5/15], batch: [52750/55810, loss: 5.5083]\n",
            "Epoch [5/15], batch: [52751/55810, loss: 5.0793]\n",
            "Epoch [5/15], batch: [52752/55810, loss: 6.8117]\n",
            "Epoch [5/15], batch: [52753/55810, loss: 5.6202]\n",
            "Epoch [5/15], batch: [52754/55810, loss: 7.4811]\n",
            "Epoch [5/15], batch: [52755/55810, loss: 5.3162]\n",
            "Epoch [5/15], batch: [52756/55810, loss: 5.9600]\n",
            "Epoch [5/15], batch: [52757/55810, loss: 6.7167]\n",
            "Epoch [5/15], batch: [52758/55810, loss: 6.5578]\n",
            "Epoch [5/15], batch: [52759/55810, loss: 6.7898]\n",
            "Epoch [5/15], batch: [52760/55810, loss: 5.8575]\n",
            "Epoch [5/15], batch: [52761/55810, loss: 5.0314]\n",
            "Epoch [5/15], batch: [52762/55810, loss: 5.9870]\n",
            "Epoch [5/15], batch: [52763/55810, loss: 6.0352]\n",
            "Epoch [5/15], batch: [52764/55810, loss: 5.5815]\n",
            "Epoch [5/15], batch: [52765/55810, loss: 6.8297]\n",
            "Epoch [5/15], batch: [52766/55810, loss: 5.9720]\n",
            "Epoch [5/15], batch: [52767/55810, loss: 8.0049]\n",
            "Epoch [5/15], batch: [52768/55810, loss: 6.5905]\n",
            "Epoch [5/15], batch: [52769/55810, loss: 5.6472]\n",
            "Epoch [5/15], batch: [52770/55810, loss: 6.8893]\n",
            "Epoch [5/15], batch: [52771/55810, loss: 6.3009]\n",
            "Epoch [5/15], batch: [52772/55810, loss: 4.8715]\n",
            "Epoch [5/15], batch: [52773/55810, loss: 5.5740]\n",
            "Epoch [5/15], batch: [52774/55810, loss: 6.8028]\n",
            "Epoch [5/15], batch: [52775/55810, loss: 6.5376]\n",
            "Epoch [5/15], batch: [52776/55810, loss: 6.0709]\n",
            "Epoch [5/15], batch: [52777/55810, loss: 4.7115]\n",
            "Epoch [5/15], batch: [52778/55810, loss: 5.8849]\n",
            "Epoch [5/15], batch: [52779/55810, loss: 7.9550]\n",
            "Epoch [5/15], batch: [52780/55810, loss: 5.4241]\n",
            "Epoch [5/15], batch: [52781/55810, loss: 7.7529]\n",
            "Epoch [5/15], batch: [52782/55810, loss: 7.5361]\n",
            "Epoch [5/15], batch: [52783/55810, loss: 7.6443]\n",
            "Epoch [5/15], batch: [52784/55810, loss: 9.1631]\n",
            "Epoch [5/15], batch: [52785/55810, loss: 7.2663]\n",
            "Epoch [5/15], batch: [52786/55810, loss: 7.9706]\n",
            "Epoch [5/15], batch: [52787/55810, loss: 6.3749]\n",
            "Epoch [5/15], batch: [52788/55810, loss: 5.3231]\n",
            "Epoch [5/15], batch: [52789/55810, loss: 5.8030]\n",
            "Epoch [5/15], batch: [52790/55810, loss: 4.9255]\n",
            "Epoch [5/15], batch: [52791/55810, loss: 5.4856]\n",
            "Epoch [5/15], batch: [52792/55810, loss: 5.4803]\n",
            "Epoch [5/15], batch: [52793/55810, loss: 5.2437]\n",
            "Epoch [5/15], batch: [52794/55810, loss: 3.1923]\n",
            "Epoch [5/15], batch: [52795/55810, loss: 6.8732]\n",
            "Epoch [5/15], batch: [52796/55810, loss: 7.7858]\n",
            "Epoch [5/15], batch: [52797/55810, loss: 8.7599]\n",
            "Epoch [5/15], batch: [52798/55810, loss: 8.0777]\n",
            "Epoch [5/15], batch: [52799/55810, loss: 7.1642]\n",
            "Epoch [5/15], batch: [52800/55810, loss: 4.9607]\n",
            "Epoch [5/15], batch: [52801/55810, loss: 8.0508]\n",
            "Epoch [5/15], batch: [52802/55810, loss: 5.4720]\n",
            "Epoch [5/15], batch: [52803/55810, loss: 5.7259]\n",
            "Epoch [5/15], batch: [52804/55810, loss: 5.3715]\n",
            "Epoch [5/15], batch: [52805/55810, loss: 7.1608]\n",
            "Epoch [5/15], batch: [52806/55810, loss: 6.7927]\n",
            "Epoch [5/15], batch: [52807/55810, loss: 6.3743]\n",
            "Epoch [5/15], batch: [52808/55810, loss: 8.0715]\n",
            "Epoch [5/15], batch: [52809/55810, loss: 5.8374]\n",
            "Epoch [5/15], batch: [52810/55810, loss: 6.9789]\n",
            "Epoch [5/15], batch: [52811/55810, loss: 6.6831]\n",
            "Epoch [5/15], batch: [52812/55810, loss: 7.8627]\n",
            "Epoch [5/15], batch: [52813/55810, loss: 6.5373]\n",
            "Epoch [5/15], batch: [52814/55810, loss: 6.7773]\n",
            "Epoch [5/15], batch: [52815/55810, loss: 7.4629]\n",
            "Epoch [5/15], batch: [52816/55810, loss: 8.9358]\n",
            "Epoch [5/15], batch: [52817/55810, loss: 7.6903]\n",
            "Epoch [5/15], batch: [52818/55810, loss: 8.1453]\n",
            "Epoch [5/15], batch: [52819/55810, loss: 8.6291]\n",
            "Epoch [5/15], batch: [52820/55810, loss: 7.4582]\n",
            "Epoch [5/15], batch: [52821/55810, loss: 8.0314]\n",
            "Epoch [5/15], batch: [52822/55810, loss: 8.4104]\n",
            "Epoch [5/15], batch: [52823/55810, loss: 6.0692]\n",
            "Epoch [5/15], batch: [52824/55810, loss: 7.7238]\n",
            "Epoch [5/15], batch: [52825/55810, loss: 6.0705]\n",
            "Epoch [5/15], batch: [52826/55810, loss: 6.1960]\n",
            "Epoch [5/15], batch: [52827/55810, loss: 7.6336]\n",
            "Epoch [5/15], batch: [52828/55810, loss: 7.4019]\n",
            "Epoch [5/15], batch: [52829/55810, loss: 7.3919]\n",
            "Epoch [5/15], batch: [52830/55810, loss: 8.7312]\n",
            "Epoch [5/15], batch: [52831/55810, loss: 7.5880]\n",
            "Epoch [5/15], batch: [52832/55810, loss: 8.8113]\n",
            "Epoch [5/15], batch: [52833/55810, loss: 7.8225]\n",
            "Epoch [5/15], batch: [52834/55810, loss: 7.6615]\n",
            "Epoch [5/15], batch: [52835/55810, loss: 7.5606]\n",
            "Epoch [5/15], batch: [52836/55810, loss: 7.7857]\n",
            "Epoch [5/15], batch: [52837/55810, loss: 7.8950]\n",
            "Epoch [5/15], batch: [52838/55810, loss: 6.9778]\n",
            "Epoch [5/15], batch: [52839/55810, loss: 6.8826]\n",
            "Epoch [5/15], batch: [52840/55810, loss: 6.2035]\n",
            "Epoch [5/15], batch: [52841/55810, loss: 5.9055]\n",
            "Epoch [5/15], batch: [52842/55810, loss: 6.2452]\n",
            "Epoch [5/15], batch: [52843/55810, loss: 6.9861]\n",
            "Epoch [5/15], batch: [52844/55810, loss: 7.0851]\n",
            "Epoch [5/15], batch: [52845/55810, loss: 7.4744]\n",
            "Epoch [5/15], batch: [52846/55810, loss: 9.2545]\n",
            "Epoch [5/15], batch: [52847/55810, loss: 6.0038]\n",
            "Epoch [5/15], batch: [52848/55810, loss: 6.4932]\n",
            "Epoch [5/15], batch: [52849/55810, loss: 6.6889]\n",
            "Epoch [5/15], batch: [52850/55810, loss: 5.3239]\n",
            "Epoch [5/15], batch: [52851/55810, loss: 4.0446]\n",
            "Epoch [5/15], batch: [52852/55810, loss: 7.2974]\n",
            "Epoch [5/15], batch: [52853/55810, loss: 8.0623]\n",
            "Epoch [5/15], batch: [52854/55810, loss: 6.6009]\n",
            "Epoch [5/15], batch: [52855/55810, loss: 7.3144]\n",
            "Epoch [5/15], batch: [52856/55810, loss: 5.2697]\n",
            "Epoch [5/15], batch: [52857/55810, loss: 6.7497]\n",
            "Epoch [5/15], batch: [52858/55810, loss: 6.4813]\n",
            "Epoch [5/15], batch: [52859/55810, loss: 6.7768]\n",
            "Epoch [5/15], batch: [52860/55810, loss: 8.7388]\n",
            "Epoch [5/15], batch: [52861/55810, loss: 5.6209]\n",
            "Epoch [5/15], batch: [52862/55810, loss: 6.9354]\n",
            "Epoch [5/15], batch: [52863/55810, loss: 6.4847]\n",
            "Epoch [5/15], batch: [52864/55810, loss: 6.9773]\n",
            "Epoch [5/15], batch: [52865/55810, loss: 6.3968]\n",
            "Epoch [5/15], batch: [52866/55810, loss: 5.9723]\n",
            "Epoch [5/15], batch: [52867/55810, loss: 6.2340]\n",
            "Epoch [5/15], batch: [52868/55810, loss: 5.7567]\n",
            "Epoch [5/15], batch: [52869/55810, loss: 6.5994]\n",
            "Epoch [5/15], batch: [52870/55810, loss: 5.8958]\n",
            "Epoch [5/15], batch: [52871/55810, loss: 9.0872]\n",
            "Epoch [5/15], batch: [52872/55810, loss: 5.0049]\n",
            "Epoch [5/15], batch: [52873/55810, loss: 6.1117]\n",
            "Epoch [5/15], batch: [52874/55810, loss: 6.0456]\n",
            "Epoch [5/15], batch: [52875/55810, loss: 7.7486]\n",
            "Epoch [5/15], batch: [52876/55810, loss: 5.9185]\n",
            "Epoch [5/15], batch: [52877/55810, loss: 6.6436]\n",
            "Epoch [5/15], batch: [52878/55810, loss: 6.6469]\n",
            "Epoch [5/15], batch: [52879/55810, loss: 7.5321]\n",
            "Epoch [5/15], batch: [52880/55810, loss: 7.2719]\n",
            "Epoch [5/15], batch: [52881/55810, loss: 4.9648]\n",
            "Epoch [5/15], batch: [52882/55810, loss: 6.1485]\n",
            "Epoch [5/15], batch: [52883/55810, loss: 5.3545]\n",
            "Epoch [5/15], batch: [52884/55810, loss: 5.2230]\n",
            "Epoch [5/15], batch: [52885/55810, loss: 4.8566]\n",
            "Epoch [5/15], batch: [52886/55810, loss: 4.3451]\n",
            "Epoch [5/15], batch: [52887/55810, loss: 5.3740]\n",
            "Epoch [5/15], batch: [52888/55810, loss: 3.7212]\n",
            "Epoch [5/15], batch: [52889/55810, loss: 5.9712]\n",
            "Epoch [5/15], batch: [52890/55810, loss: 6.2745]\n",
            "Epoch [5/15], batch: [52891/55810, loss: 6.0433]\n",
            "Epoch [5/15], batch: [52892/55810, loss: 7.3565]\n",
            "Epoch [5/15], batch: [52893/55810, loss: 5.0627]\n",
            "Epoch [5/15], batch: [52894/55810, loss: 5.7368]\n",
            "Epoch [5/15], batch: [52895/55810, loss: 5.9163]\n",
            "Epoch [5/15], batch: [52896/55810, loss: 6.6683]\n",
            "Epoch [5/15], batch: [52897/55810, loss: 4.7759]\n",
            "Epoch [5/15], batch: [52898/55810, loss: 6.4251]\n",
            "Epoch [5/15], batch: [52899/55810, loss: 6.3630]\n",
            "Epoch [5/15], batch: [52900/55810, loss: 6.9101]\n",
            "Epoch [5/15], batch: [52901/55810, loss: 6.2755]\n",
            "Epoch [5/15], batch: [52902/55810, loss: 7.6953]\n",
            "Epoch [5/15], batch: [52903/55810, loss: 6.9099]\n",
            "Epoch [5/15], batch: [52904/55810, loss: 5.4739]\n",
            "Epoch [5/15], batch: [52905/55810, loss: 5.7925]\n",
            "Epoch [5/15], batch: [52906/55810, loss: 4.5799]\n",
            "Epoch [5/15], batch: [52907/55810, loss: 5.0183]\n",
            "Epoch [5/15], batch: [52908/55810, loss: 6.6925]\n",
            "Epoch [5/15], batch: [52909/55810, loss: 5.8580]\n",
            "Epoch [5/15], batch: [52910/55810, loss: 4.5915]\n",
            "Epoch [5/15], batch: [52911/55810, loss: 5.0421]\n",
            "Epoch [5/15], batch: [52912/55810, loss: 7.0308]\n",
            "Epoch [5/15], batch: [52913/55810, loss: 4.6705]\n",
            "Epoch [5/15], batch: [52914/55810, loss: 6.0205]\n",
            "Epoch [5/15], batch: [52915/55810, loss: 5.5368]\n",
            "Epoch [5/15], batch: [52916/55810, loss: 5.8433]\n",
            "Epoch [5/15], batch: [52917/55810, loss: 5.8147]\n",
            "Epoch [5/15], batch: [52918/55810, loss: 5.3908]\n",
            "Epoch [5/15], batch: [52919/55810, loss: 6.8579]\n",
            "Epoch [5/15], batch: [52920/55810, loss: 6.6055]\n",
            "Epoch [5/15], batch: [52921/55810, loss: 4.8462]\n",
            "Epoch [5/15], batch: [52922/55810, loss: 7.4030]\n",
            "Epoch [5/15], batch: [52923/55810, loss: 8.1184]\n",
            "Epoch [5/15], batch: [52924/55810, loss: 6.8769]\n",
            "Epoch [5/15], batch: [52925/55810, loss: 6.9479]\n",
            "Epoch [5/15], batch: [52926/55810, loss: 7.5806]\n",
            "Epoch [5/15], batch: [52927/55810, loss: 7.9974]\n",
            "Epoch [5/15], batch: [52928/55810, loss: 6.0046]\n",
            "Epoch [5/15], batch: [52929/55810, loss: 8.2799]\n",
            "Epoch [5/15], batch: [52930/55810, loss: 8.3280]\n",
            "Epoch [5/15], batch: [52931/55810, loss: 4.9364]\n",
            "Epoch [5/15], batch: [52932/55810, loss: 6.2262]\n",
            "Epoch [5/15], batch: [52933/55810, loss: 6.9171]\n",
            "Epoch [5/15], batch: [52934/55810, loss: 7.0901]\n",
            "Epoch [5/15], batch: [52935/55810, loss: 5.4289]\n",
            "Epoch [5/15], batch: [52936/55810, loss: 6.0496]\n",
            "Epoch [5/15], batch: [52937/55810, loss: 4.8045]\n",
            "Epoch [5/15], batch: [52938/55810, loss: 7.6329]\n",
            "Epoch [5/15], batch: [52939/55810, loss: 4.8624]\n",
            "Epoch [5/15], batch: [52940/55810, loss: 6.1459]\n",
            "Epoch [5/15], batch: [52941/55810, loss: 6.1989]\n",
            "Epoch [5/15], batch: [52942/55810, loss: 5.9112]\n",
            "Epoch [5/15], batch: [52943/55810, loss: 7.1126]\n",
            "Epoch [5/15], batch: [52944/55810, loss: 6.8286]\n",
            "Epoch [5/15], batch: [52945/55810, loss: 7.2111]\n",
            "Epoch [5/15], batch: [52946/55810, loss: 7.1713]\n",
            "Epoch [5/15], batch: [52947/55810, loss: 5.5491]\n",
            "Epoch [5/15], batch: [52948/55810, loss: 6.8690]\n",
            "Epoch [5/15], batch: [52949/55810, loss: 7.8760]\n",
            "Epoch [5/15], batch: [52950/55810, loss: 8.0801]\n",
            "Epoch [5/15], batch: [52951/55810, loss: 8.3874]\n",
            "Epoch [5/15], batch: [52952/55810, loss: 6.5206]\n",
            "Epoch [5/15], batch: [52953/55810, loss: 7.2713]\n",
            "Epoch [5/15], batch: [52954/55810, loss: 6.7760]\n",
            "Epoch [5/15], batch: [52955/55810, loss: 7.4592]\n",
            "Epoch [5/15], batch: [52956/55810, loss: 7.9899]\n",
            "Epoch [5/15], batch: [52957/55810, loss: 5.6375]\n",
            "Epoch [5/15], batch: [52958/55810, loss: 5.2062]\n",
            "Epoch [5/15], batch: [52959/55810, loss: 5.4114]\n",
            "Epoch [5/15], batch: [52960/55810, loss: 5.9029]\n",
            "Epoch [5/15], batch: [52961/55810, loss: 5.0620]\n",
            "Epoch [5/15], batch: [52962/55810, loss: 8.8195]\n",
            "Epoch [5/15], batch: [52963/55810, loss: 6.3074]\n",
            "Epoch [5/15], batch: [52964/55810, loss: 5.0985]\n",
            "Epoch [5/15], batch: [52965/55810, loss: 6.9134]\n",
            "Epoch [5/15], batch: [52966/55810, loss: 5.1151]\n",
            "Epoch [5/15], batch: [52967/55810, loss: 6.0248]\n",
            "Epoch [5/15], batch: [52968/55810, loss: 5.0961]\n",
            "Epoch [5/15], batch: [52969/55810, loss: 4.9096]\n",
            "Epoch [5/15], batch: [52970/55810, loss: 5.2236]\n",
            "Epoch [5/15], batch: [52971/55810, loss: 8.0897]\n",
            "Epoch [5/15], batch: [52972/55810, loss: 7.3649]\n",
            "Epoch [5/15], batch: [52973/55810, loss: 7.6301]\n",
            "Epoch [5/15], batch: [52974/55810, loss: 7.8663]\n",
            "Epoch [5/15], batch: [52975/55810, loss: 6.4226]\n",
            "Epoch [5/15], batch: [52976/55810, loss: 6.7234]\n",
            "Epoch [5/15], batch: [52977/55810, loss: 7.6866]\n",
            "Epoch [5/15], batch: [52978/55810, loss: 6.9377]\n",
            "Epoch [5/15], batch: [52979/55810, loss: 5.4527]\n",
            "Epoch [5/15], batch: [52980/55810, loss: 8.2532]\n",
            "Epoch [5/15], batch: [52981/55810, loss: 7.1554]\n",
            "Epoch [5/15], batch: [52982/55810, loss: 7.1409]\n",
            "Epoch [5/15], batch: [52983/55810, loss: 6.5126]\n",
            "Epoch [5/15], batch: [52984/55810, loss: 6.1436]\n",
            "Epoch [5/15], batch: [52985/55810, loss: 6.4888]\n",
            "Epoch [5/15], batch: [52986/55810, loss: 7.1755]\n",
            "Epoch [5/15], batch: [52987/55810, loss: 6.6786]\n",
            "Epoch [5/15], batch: [52988/55810, loss: 8.0303]\n",
            "Epoch [5/15], batch: [52989/55810, loss: 7.3880]\n",
            "Epoch [5/15], batch: [52990/55810, loss: 6.3950]\n",
            "Epoch [5/15], batch: [52991/55810, loss: 8.2451]\n",
            "Epoch [5/15], batch: [52992/55810, loss: 6.9979]\n",
            "Epoch [5/15], batch: [52993/55810, loss: 5.2639]\n",
            "Epoch [5/15], batch: [52994/55810, loss: 6.3804]\n",
            "Epoch [5/15], batch: [52995/55810, loss: 8.1749]\n",
            "Epoch [5/15], batch: [52996/55810, loss: 8.0520]\n",
            "Epoch [5/15], batch: [52997/55810, loss: 6.9096]\n",
            "Epoch [5/15], batch: [52998/55810, loss: 6.2329]\n",
            "Epoch [5/15], batch: [52999/55810, loss: 6.2387]\n",
            "Epoch [5/15], batch: [53000/55810, loss: 7.2412]\n",
            "Epoch [5/15], batch: [53001/55810, loss: 4.0162]\n",
            "Epoch [5/15], batch: [53002/55810, loss: 5.7164]\n",
            "Epoch [5/15], batch: [53003/55810, loss: 6.4719]\n",
            "Epoch [5/15], batch: [53004/55810, loss: 5.7446]\n",
            "Epoch [5/15], batch: [53005/55810, loss: 6.9389]\n",
            "Epoch [5/15], batch: [53006/55810, loss: 6.1701]\n",
            "Epoch [5/15], batch: [53007/55810, loss: 7.3275]\n",
            "Epoch [5/15], batch: [53008/55810, loss: 8.8339]\n",
            "Epoch [5/15], batch: [53009/55810, loss: 6.1482]\n",
            "Epoch [5/15], batch: [53010/55810, loss: 6.1993]\n",
            "Epoch [5/15], batch: [53011/55810, loss: 6.4626]\n",
            "Epoch [5/15], batch: [53012/55810, loss: 6.7274]\n",
            "Epoch [5/15], batch: [53013/55810, loss: 6.9308]\n",
            "Epoch [5/15], batch: [53014/55810, loss: 6.8680]\n",
            "Epoch [5/15], batch: [53015/55810, loss: 7.4632]\n",
            "Epoch [5/15], batch: [53016/55810, loss: 7.2022]\n",
            "Epoch [5/15], batch: [53017/55810, loss: 6.3010]\n",
            "Epoch [5/15], batch: [53018/55810, loss: 7.4622]\n",
            "Epoch [5/15], batch: [53019/55810, loss: 8.9789]\n",
            "Epoch [5/15], batch: [53020/55810, loss: 7.1159]\n",
            "Epoch [5/15], batch: [53021/55810, loss: 6.5938]\n",
            "Epoch [5/15], batch: [53022/55810, loss: 7.4009]\n",
            "Epoch [5/15], batch: [53023/55810, loss: 5.9985]\n",
            "Epoch [5/15], batch: [53024/55810, loss: 6.3798]\n",
            "Epoch [5/15], batch: [53025/55810, loss: 6.1575]\n",
            "Epoch [5/15], batch: [53026/55810, loss: 6.2947]\n",
            "Epoch [5/15], batch: [53027/55810, loss: 5.2758]\n",
            "Epoch [5/15], batch: [53028/55810, loss: 7.0350]\n",
            "Epoch [5/15], batch: [53029/55810, loss: 6.7130]\n",
            "Epoch [5/15], batch: [53030/55810, loss: 5.5124]\n",
            "Epoch [5/15], batch: [53031/55810, loss: 7.3918]\n",
            "Epoch [5/15], batch: [53032/55810, loss: 8.7313]\n",
            "Epoch [5/15], batch: [53033/55810, loss: 5.6185]\n",
            "Epoch [5/15], batch: [53034/55810, loss: 4.6455]\n",
            "Epoch [5/15], batch: [53035/55810, loss: 6.7528]\n",
            "Epoch [5/15], batch: [53036/55810, loss: 7.7626]\n",
            "Epoch [5/15], batch: [53037/55810, loss: 6.3478]\n",
            "Epoch [5/15], batch: [53038/55810, loss: 6.2545]\n",
            "Epoch [5/15], batch: [53039/55810, loss: 5.1780]\n",
            "Epoch [5/15], batch: [53040/55810, loss: 8.1363]\n",
            "Epoch [5/15], batch: [53041/55810, loss: 6.6524]\n",
            "Epoch [5/15], batch: [53042/55810, loss: 4.4312]\n",
            "Epoch [5/15], batch: [53043/55810, loss: 4.4691]\n",
            "Epoch [5/15], batch: [53044/55810, loss: 6.4512]\n",
            "Epoch [5/15], batch: [53045/55810, loss: 5.2350]\n",
            "Epoch [5/15], batch: [53046/55810, loss: 6.9443]\n",
            "Epoch [5/15], batch: [53047/55810, loss: 8.4212]\n",
            "Epoch [5/15], batch: [53048/55810, loss: 6.2521]\n",
            "Epoch [5/15], batch: [53049/55810, loss: 6.5063]\n",
            "Epoch [5/15], batch: [53050/55810, loss: 7.3583]\n",
            "Epoch [5/15], batch: [53051/55810, loss: 8.0080]\n",
            "Epoch [5/15], batch: [53052/55810, loss: 5.5159]\n",
            "Epoch [5/15], batch: [53053/55810, loss: 5.2091]\n",
            "Epoch [5/15], batch: [53054/55810, loss: 5.2647]\n",
            "Epoch [5/15], batch: [53055/55810, loss: 7.1346]\n",
            "Epoch [5/15], batch: [53056/55810, loss: 4.0609]\n",
            "Epoch [5/15], batch: [53057/55810, loss: 4.5183]\n",
            "Epoch [5/15], batch: [53058/55810, loss: 6.1790]\n",
            "Epoch [5/15], batch: [53059/55810, loss: 6.9850]\n",
            "Epoch [5/15], batch: [53060/55810, loss: 5.2315]\n",
            "Epoch [5/15], batch: [53061/55810, loss: 7.8235]\n",
            "Epoch [5/15], batch: [53062/55810, loss: 5.9574]\n",
            "Epoch [5/15], batch: [53063/55810, loss: 5.4588]\n",
            "Epoch [5/15], batch: [53064/55810, loss: 7.1141]\n",
            "Epoch [5/15], batch: [53065/55810, loss: 4.5736]\n",
            "Epoch [5/15], batch: [53066/55810, loss: 6.0045]\n",
            "Epoch [5/15], batch: [53067/55810, loss: 5.8072]\n",
            "Epoch [5/15], batch: [53068/55810, loss: 6.6636]\n",
            "Epoch [5/15], batch: [53069/55810, loss: 6.2888]\n",
            "Epoch [5/15], batch: [53070/55810, loss: 6.9947]\n",
            "Epoch [5/15], batch: [53071/55810, loss: 6.6058]\n",
            "Epoch [5/15], batch: [53072/55810, loss: 5.3903]\n",
            "Epoch [5/15], batch: [53073/55810, loss: 5.5802]\n",
            "Epoch [5/15], batch: [53074/55810, loss: 7.5804]\n",
            "Epoch [5/15], batch: [53075/55810, loss: 6.5937]\n",
            "Epoch [5/15], batch: [53076/55810, loss: 4.9115]\n",
            "Epoch [5/15], batch: [53077/55810, loss: 7.0708]\n",
            "Epoch [5/15], batch: [53078/55810, loss: 5.6845]\n",
            "Epoch [5/15], batch: [53079/55810, loss: 6.7725]\n",
            "Epoch [5/15], batch: [53080/55810, loss: 8.1382]\n",
            "Epoch [5/15], batch: [53081/55810, loss: 7.5081]\n",
            "Epoch [5/15], batch: [53082/55810, loss: 8.1845]\n",
            "Epoch [5/15], batch: [53083/55810, loss: 5.2932]\n",
            "Epoch [5/15], batch: [53084/55810, loss: 7.6520]\n",
            "Epoch [5/15], batch: [53085/55810, loss: 5.6403]\n",
            "Epoch [5/15], batch: [53086/55810, loss: 8.2180]\n",
            "Epoch [5/15], batch: [53087/55810, loss: 6.3353]\n",
            "Epoch [5/15], batch: [53088/55810, loss: 6.0835]\n",
            "Epoch [5/15], batch: [53089/55810, loss: 7.7383]\n",
            "Epoch [5/15], batch: [53090/55810, loss: 4.9879]\n",
            "Epoch [5/15], batch: [53091/55810, loss: 6.3730]\n",
            "Epoch [5/15], batch: [53092/55810, loss: 6.4469]\n",
            "Epoch [5/15], batch: [53093/55810, loss: 7.4175]\n",
            "Epoch [5/15], batch: [53094/55810, loss: 6.1516]\n",
            "Epoch [5/15], batch: [53095/55810, loss: 7.1173]\n",
            "Epoch [5/15], batch: [53096/55810, loss: 7.2056]\n",
            "Epoch [5/15], batch: [53097/55810, loss: 6.8007]\n",
            "Epoch [5/15], batch: [53098/55810, loss: 6.0467]\n",
            "Epoch [5/15], batch: [53099/55810, loss: 7.1554]\n",
            "Epoch [5/15], batch: [53100/55810, loss: 6.6129]\n",
            "Epoch [5/15], batch: [53101/55810, loss: 6.0947]\n",
            "Epoch [5/15], batch: [53102/55810, loss: 5.8336]\n",
            "Epoch [5/15], batch: [53103/55810, loss: 7.3807]\n",
            "Epoch [5/15], batch: [53104/55810, loss: 6.6314]\n",
            "Epoch [5/15], batch: [53105/55810, loss: 6.9798]\n",
            "Epoch [5/15], batch: [53106/55810, loss: 6.2458]\n",
            "Epoch [5/15], batch: [53107/55810, loss: 7.7240]\n",
            "Epoch [5/15], batch: [53108/55810, loss: 6.0578]\n",
            "Epoch [5/15], batch: [53109/55810, loss: 6.2995]\n",
            "Epoch [5/15], batch: [53110/55810, loss: 6.4552]\n",
            "Epoch [5/15], batch: [53111/55810, loss: 6.5789]\n",
            "Epoch [5/15], batch: [53112/55810, loss: 6.7396]\n",
            "Epoch [5/15], batch: [53113/55810, loss: 6.6601]\n",
            "Epoch [5/15], batch: [53114/55810, loss: 6.9406]\n",
            "Epoch [5/15], batch: [53115/55810, loss: 7.1130]\n",
            "Epoch [5/15], batch: [53116/55810, loss: 7.2131]\n",
            "Epoch [5/15], batch: [53117/55810, loss: 7.7173]\n",
            "Epoch [5/15], batch: [53118/55810, loss: 7.6661]\n",
            "Epoch [5/15], batch: [53119/55810, loss: 7.5515]\n",
            "Epoch [5/15], batch: [53120/55810, loss: 5.4650]\n",
            "Epoch [5/15], batch: [53121/55810, loss: 4.5264]\n",
            "Epoch [5/15], batch: [53122/55810, loss: 5.4083]\n",
            "Epoch [5/15], batch: [53123/55810, loss: 5.0093]\n",
            "Epoch [5/15], batch: [53124/55810, loss: 5.3703]\n",
            "Epoch [5/15], batch: [53125/55810, loss: 6.0248]\n",
            "Epoch [5/15], batch: [53126/55810, loss: 4.8028]\n",
            "Epoch [5/15], batch: [53127/55810, loss: 6.2878]\n",
            "Epoch [5/15], batch: [53128/55810, loss: 4.7925]\n",
            "Epoch [5/15], batch: [53129/55810, loss: 6.0395]\n",
            "Epoch [5/15], batch: [53130/55810, loss: 5.1326]\n",
            "Epoch [5/15], batch: [53131/55810, loss: 6.6598]\n",
            "Epoch [5/15], batch: [53132/55810, loss: 5.7840]\n",
            "Epoch [5/15], batch: [53133/55810, loss: 7.4044]\n",
            "Epoch [5/15], batch: [53134/55810, loss: 7.2262]\n",
            "Epoch [5/15], batch: [53135/55810, loss: 7.4320]\n",
            "Epoch [5/15], batch: [53136/55810, loss: 6.0048]\n",
            "Epoch [5/15], batch: [53137/55810, loss: 6.4182]\n",
            "Epoch [5/15], batch: [53138/55810, loss: 6.8291]\n",
            "Epoch [5/15], batch: [53139/55810, loss: 6.8185]\n",
            "Epoch [5/15], batch: [53140/55810, loss: 6.9346]\n",
            "Epoch [5/15], batch: [53141/55810, loss: 7.3969]\n",
            "Epoch [5/15], batch: [53142/55810, loss: 5.5915]\n",
            "Epoch [5/15], batch: [53143/55810, loss: 6.7980]\n",
            "Epoch [5/15], batch: [53144/55810, loss: 7.5688]\n",
            "Epoch [5/15], batch: [53145/55810, loss: 7.1599]\n",
            "Epoch [5/15], batch: [53146/55810, loss: 6.0275]\n",
            "Epoch [5/15], batch: [53147/55810, loss: 5.7950]\n",
            "Epoch [5/15], batch: [53148/55810, loss: 6.2029]\n",
            "Epoch [5/15], batch: [53149/55810, loss: 7.9637]\n",
            "Epoch [5/15], batch: [53150/55810, loss: 6.1838]\n",
            "Epoch [5/15], batch: [53151/55810, loss: 7.0781]\n",
            "Epoch [5/15], batch: [53152/55810, loss: 6.6993]\n",
            "Epoch [5/15], batch: [53153/55810, loss: 6.9973]\n",
            "Epoch [5/15], batch: [53154/55810, loss: 5.4573]\n",
            "Epoch [5/15], batch: [53155/55810, loss: 7.5893]\n",
            "Epoch [5/15], batch: [53156/55810, loss: 6.8222]\n",
            "Epoch [5/15], batch: [53157/55810, loss: 5.8633]\n",
            "Epoch [5/15], batch: [53158/55810, loss: 5.9272]\n",
            "Epoch [5/15], batch: [53159/55810, loss: 4.6797]\n",
            "Epoch [5/15], batch: [53160/55810, loss: 5.2847]\n",
            "Epoch [5/15], batch: [53161/55810, loss: 3.8067]\n",
            "Epoch [5/15], batch: [53162/55810, loss: 6.9654]\n",
            "Epoch [5/15], batch: [53163/55810, loss: 5.4758]\n",
            "Epoch [5/15], batch: [53164/55810, loss: 7.7126]\n",
            "Epoch [5/15], batch: [53165/55810, loss: 6.0645]\n",
            "Epoch [5/15], batch: [53166/55810, loss: 6.0645]\n",
            "Epoch [5/15], batch: [53167/55810, loss: 5.9761]\n",
            "Epoch [5/15], batch: [53168/55810, loss: 7.3138]\n",
            "Epoch [5/15], batch: [53169/55810, loss: 8.1609]\n",
            "Epoch [5/15], batch: [53170/55810, loss: 6.7673]\n",
            "Epoch [5/15], batch: [53171/55810, loss: 6.8385]\n",
            "Epoch [5/15], batch: [53172/55810, loss: 7.1210]\n",
            "Epoch [5/15], batch: [53173/55810, loss: 5.7572]\n",
            "Epoch [5/15], batch: [53174/55810, loss: 5.3311]\n",
            "Epoch [5/15], batch: [53175/55810, loss: 5.9051]\n",
            "Epoch [5/15], batch: [53176/55810, loss: 6.6481]\n",
            "Epoch [5/15], batch: [53177/55810, loss: 7.5159]\n",
            "Epoch [5/15], batch: [53178/55810, loss: 7.3452]\n",
            "Epoch [5/15], batch: [53179/55810, loss: 6.2733]\n",
            "Epoch [5/15], batch: [53180/55810, loss: 7.3657]\n",
            "Epoch [5/15], batch: [53181/55810, loss: 4.8184]\n",
            "Epoch [5/15], batch: [53182/55810, loss: 5.5702]\n",
            "Epoch [5/15], batch: [53183/55810, loss: 6.6508]\n",
            "Epoch [5/15], batch: [53184/55810, loss: 6.0256]\n",
            "Epoch [5/15], batch: [53185/55810, loss: 5.9532]\n",
            "Epoch [5/15], batch: [53186/55810, loss: 6.7210]\n",
            "Epoch [5/15], batch: [53187/55810, loss: 6.8135]\n",
            "Epoch [5/15], batch: [53188/55810, loss: 4.6069]\n",
            "Epoch [5/15], batch: [53189/55810, loss: 7.1389]\n",
            "Epoch [5/15], batch: [53190/55810, loss: 6.2217]\n",
            "Epoch [5/15], batch: [53191/55810, loss: 5.4906]\n",
            "Epoch [5/15], batch: [53192/55810, loss: 7.0527]\n",
            "Epoch [5/15], batch: [53193/55810, loss: 5.0799]\n",
            "Epoch [5/15], batch: [53194/55810, loss: 4.7838]\n",
            "Epoch [5/15], batch: [53195/55810, loss: 5.6997]\n",
            "Epoch [5/15], batch: [53196/55810, loss: 7.7814]\n",
            "Epoch [5/15], batch: [53197/55810, loss: 5.2018]\n",
            "Epoch [5/15], batch: [53198/55810, loss: 5.8652]\n",
            "Epoch [5/15], batch: [53199/55810, loss: 5.9561]\n",
            "Epoch [5/15], batch: [53200/55810, loss: 5.4851]\n",
            "Epoch [5/15], batch: [53201/55810, loss: 4.2986]\n",
            "Epoch [5/15], batch: [53202/55810, loss: 6.3611]\n",
            "Epoch [5/15], batch: [53203/55810, loss: 7.9094]\n",
            "Epoch [5/15], batch: [53204/55810, loss: 7.0521]\n",
            "Epoch [5/15], batch: [53205/55810, loss: 5.5585]\n",
            "Epoch [5/15], batch: [53206/55810, loss: 5.0060]\n",
            "Epoch [5/15], batch: [53207/55810, loss: 5.3034]\n",
            "Epoch [5/15], batch: [53208/55810, loss: 5.7703]\n",
            "Epoch [5/15], batch: [53209/55810, loss: 6.3837]\n",
            "Epoch [5/15], batch: [53210/55810, loss: 6.7755]\n",
            "Epoch [5/15], batch: [53211/55810, loss: 7.6663]\n",
            "Epoch [5/15], batch: [53212/55810, loss: 7.9041]\n",
            "Epoch [5/15], batch: [53213/55810, loss: 6.9450]\n",
            "Epoch [5/15], batch: [53214/55810, loss: 7.2531]\n",
            "Epoch [5/15], batch: [53215/55810, loss: 8.3476]\n",
            "Epoch [5/15], batch: [53216/55810, loss: 7.5793]\n",
            "Epoch [5/15], batch: [53217/55810, loss: 7.3346]\n",
            "Epoch [5/15], batch: [53218/55810, loss: 8.1354]\n",
            "Epoch [5/15], batch: [53219/55810, loss: 5.6618]\n",
            "Epoch [5/15], batch: [53220/55810, loss: 8.9174]\n",
            "Epoch [5/15], batch: [53221/55810, loss: 7.2204]\n",
            "Epoch [5/15], batch: [53222/55810, loss: 6.0106]\n",
            "Epoch [5/15], batch: [53223/55810, loss: 7.7603]\n",
            "Epoch [5/15], batch: [53224/55810, loss: 7.2376]\n",
            "Epoch [5/15], batch: [53225/55810, loss: 7.9669]\n",
            "Epoch [5/15], batch: [53226/55810, loss: 7.8323]\n",
            "Epoch [5/15], batch: [53227/55810, loss: 7.4176]\n",
            "Epoch [5/15], batch: [53228/55810, loss: 7.0281]\n",
            "Epoch [5/15], batch: [53229/55810, loss: 6.6854]\n",
            "Epoch [5/15], batch: [53230/55810, loss: 6.5231]\n",
            "Epoch [5/15], batch: [53231/55810, loss: 5.6906]\n",
            "Epoch [5/15], batch: [53232/55810, loss: 5.1053]\n",
            "Epoch [5/15], batch: [53233/55810, loss: 6.7525]\n",
            "Epoch [5/15], batch: [53234/55810, loss: 7.0249]\n",
            "Epoch [5/15], batch: [53235/55810, loss: 6.7983]\n",
            "Epoch [5/15], batch: [53236/55810, loss: 5.6132]\n",
            "Epoch [5/15], batch: [53237/55810, loss: 5.8727]\n",
            "Epoch [5/15], batch: [53238/55810, loss: 6.5852]\n",
            "Epoch [5/15], batch: [53239/55810, loss: 5.8715]\n",
            "Epoch [5/15], batch: [53240/55810, loss: 6.7760]\n",
            "Epoch [5/15], batch: [53241/55810, loss: 5.3546]\n",
            "Epoch [5/15], batch: [53242/55810, loss: 6.2725]\n",
            "Epoch [5/15], batch: [53243/55810, loss: 6.7771]\n",
            "Epoch [5/15], batch: [53244/55810, loss: 6.4949]\n",
            "Epoch [5/15], batch: [53245/55810, loss: 7.1170]\n",
            "Epoch [5/15], batch: [53246/55810, loss: 5.5098]\n",
            "Epoch [5/15], batch: [53247/55810, loss: 9.0340]\n",
            "Epoch [5/15], batch: [53248/55810, loss: 6.4270]\n",
            "Epoch [5/15], batch: [53249/55810, loss: 6.5719]\n",
            "Epoch [5/15], batch: [53250/55810, loss: 6.2397]\n",
            "Epoch [5/15], batch: [53251/55810, loss: 6.9683]\n",
            "Epoch [5/15], batch: [53252/55810, loss: 7.6709]\n",
            "Epoch [5/15], batch: [53253/55810, loss: 9.0298]\n",
            "Epoch [5/15], batch: [53254/55810, loss: 6.6883]\n",
            "Epoch [5/15], batch: [53255/55810, loss: 6.4231]\n",
            "Epoch [5/15], batch: [53256/55810, loss: 5.9874]\n",
            "Epoch [5/15], batch: [53257/55810, loss: 6.3063]\n",
            "Epoch [5/15], batch: [53258/55810, loss: 5.6642]\n",
            "Epoch [5/15], batch: [53259/55810, loss: 5.8573]\n",
            "Epoch [5/15], batch: [53260/55810, loss: 6.0604]\n",
            "Epoch [5/15], batch: [53261/55810, loss: 5.9798]\n",
            "Epoch [5/15], batch: [53262/55810, loss: 4.8694]\n",
            "Epoch [5/15], batch: [53263/55810, loss: 8.4484]\n",
            "Epoch [5/15], batch: [53264/55810, loss: 8.7914]\n",
            "Epoch [5/15], batch: [53265/55810, loss: 7.7767]\n",
            "Epoch [5/15], batch: [53266/55810, loss: 5.9324]\n",
            "Epoch [5/15], batch: [53267/55810, loss: 6.6566]\n",
            "Epoch [5/15], batch: [53268/55810, loss: 6.5575]\n",
            "Epoch [5/15], batch: [53269/55810, loss: 6.6701]\n",
            "Epoch [5/15], batch: [53270/55810, loss: 5.6209]\n",
            "Epoch [5/15], batch: [53271/55810, loss: 6.4385]\n",
            "Epoch [5/15], batch: [53272/55810, loss: 8.3144]\n",
            "Epoch [5/15], batch: [53273/55810, loss: 7.0916]\n",
            "Epoch [5/15], batch: [53274/55810, loss: 6.4510]\n",
            "Epoch [5/15], batch: [53275/55810, loss: 7.1502]\n",
            "Epoch [5/15], batch: [53276/55810, loss: 5.6632]\n",
            "Epoch [5/15], batch: [53277/55810, loss: 7.0090]\n",
            "Epoch [5/15], batch: [53278/55810, loss: 7.7277]\n",
            "Epoch [5/15], batch: [53279/55810, loss: 6.7863]\n",
            "Epoch [5/15], batch: [53280/55810, loss: 5.6911]\n",
            "Epoch [5/15], batch: [53281/55810, loss: 4.0594]\n",
            "Epoch [5/15], batch: [53282/55810, loss: 5.9941]\n",
            "Epoch [5/15], batch: [53283/55810, loss: 6.3588]\n",
            "Epoch [5/15], batch: [53284/55810, loss: 4.2968]\n",
            "Epoch [5/15], batch: [53285/55810, loss: 6.6632]\n",
            "Epoch [5/15], batch: [53286/55810, loss: 6.3230]\n",
            "Epoch [5/15], batch: [53287/55810, loss: 4.9120]\n",
            "Epoch [5/15], batch: [53288/55810, loss: 5.9548]\n",
            "Epoch [5/15], batch: [53289/55810, loss: 5.7092]\n",
            "Epoch [5/15], batch: [53290/55810, loss: 7.0581]\n",
            "Epoch [5/15], batch: [53291/55810, loss: 6.5195]\n",
            "Epoch [5/15], batch: [53292/55810, loss: 6.7273]\n",
            "Epoch [5/15], batch: [53293/55810, loss: 7.0785]\n",
            "Epoch [5/15], batch: [53294/55810, loss: 8.5823]\n",
            "Epoch [5/15], batch: [53295/55810, loss: 6.8418]\n",
            "Epoch [5/15], batch: [53296/55810, loss: 5.6163]\n",
            "Epoch [5/15], batch: [53297/55810, loss: 6.1693]\n",
            "Epoch [5/15], batch: [53298/55810, loss: 6.5924]\n",
            "Epoch [5/15], batch: [53299/55810, loss: 7.1989]\n",
            "Epoch [5/15], batch: [53300/55810, loss: 6.0786]\n",
            "Epoch [5/15], batch: [53301/55810, loss: 4.5421]\n",
            "Epoch [5/15], batch: [53302/55810, loss: 6.1103]\n",
            "Epoch [5/15], batch: [53303/55810, loss: 6.6233]\n",
            "Epoch [5/15], batch: [53304/55810, loss: 4.3747]\n",
            "Epoch [5/15], batch: [53305/55810, loss: 5.8301]\n",
            "Epoch [5/15], batch: [53306/55810, loss: 6.7554]\n",
            "Epoch [5/15], batch: [53307/55810, loss: 6.0462]\n",
            "Epoch [5/15], batch: [53308/55810, loss: 8.3665]\n",
            "Epoch [5/15], batch: [53309/55810, loss: 6.2204]\n",
            "Epoch [5/15], batch: [53310/55810, loss: 8.0208]\n",
            "Epoch [5/15], batch: [53311/55810, loss: 6.5012]\n",
            "Epoch [5/15], batch: [53312/55810, loss: 5.4297]\n",
            "Epoch [5/15], batch: [53313/55810, loss: 6.4707]\n",
            "Epoch [5/15], batch: [53314/55810, loss: 5.0418]\n",
            "Epoch [5/15], batch: [53315/55810, loss: 5.6908]\n",
            "Epoch [5/15], batch: [53316/55810, loss: 5.3146]\n",
            "Epoch [5/15], batch: [53317/55810, loss: 6.1817]\n",
            "Epoch [5/15], batch: [53318/55810, loss: 7.3550]\n",
            "Epoch [5/15], batch: [53319/55810, loss: 5.8202]\n",
            "Epoch [5/15], batch: [53320/55810, loss: 7.1415]\n",
            "Epoch [5/15], batch: [53321/55810, loss: 7.9599]\n",
            "Epoch [5/15], batch: [53322/55810, loss: 7.4737]\n",
            "Epoch [5/15], batch: [53323/55810, loss: 8.5275]\n",
            "Epoch [5/15], batch: [53324/55810, loss: 9.1143]\n",
            "Epoch [5/15], batch: [53325/55810, loss: 6.9814]\n",
            "Epoch [5/15], batch: [53326/55810, loss: 7.0017]\n",
            "Epoch [5/15], batch: [53327/55810, loss: 6.0646]\n",
            "Epoch [5/15], batch: [53328/55810, loss: 6.2315]\n",
            "Epoch [5/15], batch: [53329/55810, loss: 6.5976]\n",
            "Epoch [5/15], batch: [53330/55810, loss: 6.8188]\n",
            "Epoch [5/15], batch: [53331/55810, loss: 6.2411]\n",
            "Epoch [5/15], batch: [53332/55810, loss: 6.1514]\n",
            "Epoch [5/15], batch: [53333/55810, loss: 7.1595]\n",
            "Epoch [5/15], batch: [53334/55810, loss: 6.6534]\n",
            "Epoch [5/15], batch: [53335/55810, loss: 7.2713]\n",
            "Epoch [5/15], batch: [53336/55810, loss: 7.7709]\n",
            "Epoch [5/15], batch: [53337/55810, loss: 6.6798]\n",
            "Epoch [5/15], batch: [53338/55810, loss: 7.2175]\n",
            "Epoch [5/15], batch: [53339/55810, loss: 8.5928]\n",
            "Epoch [5/15], batch: [53340/55810, loss: 6.0253]\n",
            "Epoch [5/15], batch: [53341/55810, loss: 8.0994]\n",
            "Epoch [5/15], batch: [53342/55810, loss: 6.2333]\n",
            "Epoch [5/15], batch: [53343/55810, loss: 8.0789]\n",
            "Epoch [5/15], batch: [53344/55810, loss: 8.8475]\n",
            "Epoch [5/15], batch: [53345/55810, loss: 7.0540]\n",
            "Epoch [5/15], batch: [53346/55810, loss: 6.3998]\n",
            "Epoch [5/15], batch: [53347/55810, loss: 5.7245]\n",
            "Epoch [5/15], batch: [53348/55810, loss: 5.7575]\n",
            "Epoch [5/15], batch: [53349/55810, loss: 6.8713]\n",
            "Epoch [5/15], batch: [53350/55810, loss: 7.3811]\n",
            "Epoch [5/15], batch: [53351/55810, loss: 6.8334]\n",
            "Epoch [5/15], batch: [53352/55810, loss: 7.4744]\n",
            "Epoch [5/15], batch: [53353/55810, loss: 6.2764]\n",
            "Epoch [5/15], batch: [53354/55810, loss: 6.1142]\n",
            "Epoch [5/15], batch: [53355/55810, loss: 6.2925]\n",
            "Epoch [5/15], batch: [53356/55810, loss: 5.4541]\n",
            "Epoch [5/15], batch: [53357/55810, loss: 7.0810]\n",
            "Epoch [5/15], batch: [53358/55810, loss: 3.8899]\n",
            "Epoch [5/15], batch: [53359/55810, loss: 6.0244]\n",
            "Epoch [5/15], batch: [53360/55810, loss: 5.9976]\n",
            "Epoch [5/15], batch: [53361/55810, loss: 7.6611]\n",
            "Epoch [5/15], batch: [53362/55810, loss: 6.3386]\n",
            "Epoch [5/15], batch: [53363/55810, loss: 7.6059]\n",
            "Epoch [5/15], batch: [53364/55810, loss: 6.8639]\n",
            "Epoch [5/15], batch: [53365/55810, loss: 6.6359]\n",
            "Epoch [5/15], batch: [53366/55810, loss: 4.7414]\n",
            "Epoch [5/15], batch: [53367/55810, loss: 5.0844]\n",
            "Epoch [5/15], batch: [53368/55810, loss: 4.4270]\n",
            "Epoch [5/15], batch: [53369/55810, loss: 6.3866]\n",
            "Epoch [5/15], batch: [53370/55810, loss: 8.1218]\n",
            "Epoch [5/15], batch: [53371/55810, loss: 6.6275]\n",
            "Epoch [5/15], batch: [53372/55810, loss: 8.2648]\n",
            "Epoch [5/15], batch: [53373/55810, loss: 7.8098]\n",
            "Epoch [5/15], batch: [53374/55810, loss: 6.3658]\n",
            "Epoch [5/15], batch: [53375/55810, loss: 5.5486]\n",
            "Epoch [5/15], batch: [53376/55810, loss: 6.6778]\n",
            "Epoch [5/15], batch: [53377/55810, loss: 6.6091]\n",
            "Epoch [5/15], batch: [53378/55810, loss: 7.9031]\n",
            "Epoch [5/15], batch: [53379/55810, loss: 6.6705]\n",
            "Epoch [5/15], batch: [53380/55810, loss: 7.3650]\n",
            "Epoch [5/15], batch: [53381/55810, loss: 7.3261]\n",
            "Epoch [5/15], batch: [53382/55810, loss: 7.5073]\n",
            "Epoch [5/15], batch: [53383/55810, loss: 6.9529]\n",
            "Epoch [5/15], batch: [53384/55810, loss: 6.6049]\n",
            "Epoch [5/15], batch: [53385/55810, loss: 6.9154]\n",
            "Epoch [5/15], batch: [53386/55810, loss: 7.4258]\n",
            "Epoch [5/15], batch: [53387/55810, loss: 7.4751]\n",
            "Epoch [5/15], batch: [53388/55810, loss: 8.5070]\n",
            "Epoch [5/15], batch: [53389/55810, loss: 4.7898]\n",
            "Epoch [5/15], batch: [53390/55810, loss: 6.1698]\n",
            "Epoch [5/15], batch: [53391/55810, loss: 6.2237]\n",
            "Epoch [5/15], batch: [53392/55810, loss: 8.0351]\n",
            "Epoch [5/15], batch: [53393/55810, loss: 8.2177]\n",
            "Epoch [5/15], batch: [53394/55810, loss: 7.6219]\n",
            "Epoch [5/15], batch: [53395/55810, loss: 7.0485]\n",
            "Epoch [5/15], batch: [53396/55810, loss: 6.2347]\n",
            "Epoch [5/15], batch: [53397/55810, loss: 7.1355]\n",
            "Epoch [5/15], batch: [53398/55810, loss: 6.9247]\n",
            "Epoch [5/15], batch: [53399/55810, loss: 5.8668]\n",
            "Epoch [5/15], batch: [53400/55810, loss: 6.5926]\n",
            "Epoch [5/15], batch: [53401/55810, loss: 5.1219]\n",
            "Epoch [5/15], batch: [53402/55810, loss: 6.4576]\n",
            "Epoch [5/15], batch: [53403/55810, loss: 6.9282]\n",
            "Epoch [5/15], batch: [53404/55810, loss: 6.7459]\n",
            "Epoch [5/15], batch: [53405/55810, loss: 8.2053]\n",
            "Epoch [5/15], batch: [53406/55810, loss: 6.8300]\n",
            "Epoch [5/15], batch: [53407/55810, loss: 7.5559]\n",
            "Epoch [5/15], batch: [53408/55810, loss: 7.5603]\n",
            "Epoch [5/15], batch: [53409/55810, loss: 6.9482]\n",
            "Epoch [5/15], batch: [53410/55810, loss: 7.8444]\n",
            "Epoch [5/15], batch: [53411/55810, loss: 7.5117]\n",
            "Epoch [5/15], batch: [53412/55810, loss: 7.0307]\n",
            "Epoch [5/15], batch: [53413/55810, loss: 8.4382]\n",
            "Epoch [5/15], batch: [53414/55810, loss: 5.7241]\n",
            "Epoch [5/15], batch: [53415/55810, loss: 5.0475]\n",
            "Epoch [5/15], batch: [53416/55810, loss: 4.9226]\n",
            "Epoch [5/15], batch: [53417/55810, loss: 7.8443]\n",
            "Epoch [5/15], batch: [53418/55810, loss: 6.7461]\n",
            "Epoch [5/15], batch: [53419/55810, loss: 6.6336]\n",
            "Epoch [5/15], batch: [53420/55810, loss: 7.1977]\n",
            "Epoch [5/15], batch: [53421/55810, loss: 4.4705]\n",
            "Epoch [5/15], batch: [53422/55810, loss: 7.4543]\n",
            "Epoch [5/15], batch: [53423/55810, loss: 5.5504]\n",
            "Epoch [5/15], batch: [53424/55810, loss: 5.6354]\n",
            "Epoch [5/15], batch: [53425/55810, loss: 6.0448]\n",
            "Epoch [5/15], batch: [53426/55810, loss: 4.8153]\n",
            "Epoch [5/15], batch: [53427/55810, loss: 6.7833]\n",
            "Epoch [5/15], batch: [53428/55810, loss: 6.5572]\n",
            "Epoch [5/15], batch: [53429/55810, loss: 7.6491]\n",
            "Epoch [5/15], batch: [53430/55810, loss: 6.3339]\n",
            "Epoch [5/15], batch: [53431/55810, loss: 6.5520]\n",
            "Epoch [5/15], batch: [53432/55810, loss: 6.4357]\n",
            "Epoch [5/15], batch: [53433/55810, loss: 6.5313]\n",
            "Epoch [5/15], batch: [53434/55810, loss: 7.4168]\n",
            "Epoch [5/15], batch: [53435/55810, loss: 7.5582]\n",
            "Epoch [5/15], batch: [53436/55810, loss: 6.2311]\n",
            "Epoch [5/15], batch: [53437/55810, loss: 7.8961]\n",
            "Epoch [5/15], batch: [53438/55810, loss: 6.6553]\n",
            "Epoch [5/15], batch: [53439/55810, loss: 4.1464]\n",
            "Epoch [5/15], batch: [53440/55810, loss: 5.0268]\n",
            "Epoch [5/15], batch: [53441/55810, loss: 4.7245]\n",
            "Epoch [5/15], batch: [53442/55810, loss: 6.3110]\n",
            "Epoch [5/15], batch: [53443/55810, loss: 3.2871]\n",
            "Epoch [5/15], batch: [53444/55810, loss: 6.4497]\n",
            "Epoch [5/15], batch: [53445/55810, loss: 6.3327]\n",
            "Epoch [5/15], batch: [53446/55810, loss: 7.1156]\n",
            "Epoch [5/15], batch: [53447/55810, loss: 6.5508]\n",
            "Epoch [5/15], batch: [53448/55810, loss: 7.6480]\n",
            "Epoch [5/15], batch: [53449/55810, loss: 7.7800]\n",
            "Epoch [5/15], batch: [53450/55810, loss: 5.9646]\n",
            "Epoch [5/15], batch: [53451/55810, loss: 5.8532]\n",
            "Epoch [5/15], batch: [53452/55810, loss: 6.3676]\n",
            "Epoch [5/15], batch: [53453/55810, loss: 5.8519]\n",
            "Epoch [5/15], batch: [53454/55810, loss: 5.8709]\n",
            "Epoch [5/15], batch: [53455/55810, loss: 5.4319]\n",
            "Epoch [5/15], batch: [53456/55810, loss: 6.9631]\n",
            "Epoch [5/15], batch: [53457/55810, loss: 6.2044]\n",
            "Epoch [5/15], batch: [53458/55810, loss: 6.6109]\n",
            "Epoch [5/15], batch: [53459/55810, loss: 5.5876]\n",
            "Epoch [5/15], batch: [53460/55810, loss: 6.3759]\n",
            "Epoch [5/15], batch: [53461/55810, loss: 6.8753]\n",
            "Epoch [5/15], batch: [53462/55810, loss: 6.4425]\n",
            "Epoch [5/15], batch: [53463/55810, loss: 5.6537]\n",
            "Epoch [5/15], batch: [53464/55810, loss: 3.8508]\n",
            "Epoch [5/15], batch: [53465/55810, loss: 5.2933]\n",
            "Epoch [5/15], batch: [53466/55810, loss: 7.2053]\n",
            "Epoch [5/15], batch: [53467/55810, loss: 7.4040]\n",
            "Epoch [5/15], batch: [53468/55810, loss: 5.9443]\n",
            "Epoch [5/15], batch: [53469/55810, loss: 6.3471]\n",
            "Epoch [5/15], batch: [53470/55810, loss: 7.4553]\n",
            "Epoch [5/15], batch: [53471/55810, loss: 6.1360]\n",
            "Epoch [5/15], batch: [53472/55810, loss: 7.7822]\n",
            "Epoch [5/15], batch: [53473/55810, loss: 7.0185]\n",
            "Epoch [5/15], batch: [53474/55810, loss: 6.8908]\n",
            "Epoch [5/15], batch: [53475/55810, loss: 5.5630]\n",
            "Epoch [5/15], batch: [53476/55810, loss: 5.8859]\n",
            "Epoch [5/15], batch: [53477/55810, loss: 4.9675]\n",
            "Epoch [5/15], batch: [53478/55810, loss: 5.6189]\n",
            "Epoch [5/15], batch: [53479/55810, loss: 6.5550]\n",
            "Epoch [5/15], batch: [53480/55810, loss: 5.5047]\n",
            "Epoch [5/15], batch: [53481/55810, loss: 5.6379]\n",
            "Epoch [5/15], batch: [53482/55810, loss: 4.5017]\n",
            "Epoch [5/15], batch: [53483/55810, loss: 6.6069]\n",
            "Epoch [5/15], batch: [53484/55810, loss: 7.2957]\n",
            "Epoch [5/15], batch: [53485/55810, loss: 6.3650]\n",
            "Epoch [5/15], batch: [53486/55810, loss: 7.4700]\n",
            "Epoch [5/15], batch: [53487/55810, loss: 5.5210]\n",
            "Epoch [5/15], batch: [53488/55810, loss: 8.1246]\n",
            "Epoch [5/15], batch: [53489/55810, loss: 6.9424]\n",
            "Epoch [5/15], batch: [53490/55810, loss: 6.0240]\n",
            "Epoch [5/15], batch: [53491/55810, loss: 7.2288]\n",
            "Epoch [5/15], batch: [53492/55810, loss: 7.5924]\n",
            "Epoch [5/15], batch: [53493/55810, loss: 6.5599]\n",
            "Epoch [5/15], batch: [53494/55810, loss: 6.3042]\n",
            "Epoch [5/15], batch: [53495/55810, loss: 5.6892]\n",
            "Epoch [5/15], batch: [53496/55810, loss: 7.5549]\n",
            "Epoch [5/15], batch: [53497/55810, loss: 7.3295]\n",
            "Epoch [5/15], batch: [53498/55810, loss: 7.0172]\n",
            "Epoch [5/15], batch: [53499/55810, loss: 4.9467]\n",
            "Epoch [5/15], batch: [53500/55810, loss: 6.6646]\n",
            "Epoch [5/15], batch: [53501/55810, loss: 8.6395]\n",
            "Epoch [5/15], batch: [53502/55810, loss: 6.1970]\n",
            "Epoch [5/15], batch: [53503/55810, loss: 7.0969]\n",
            "Epoch [5/15], batch: [53504/55810, loss: 6.3113]\n",
            "Epoch [5/15], batch: [53505/55810, loss: 6.2026]\n",
            "Epoch [5/15], batch: [53506/55810, loss: 7.0639]\n",
            "Epoch [5/15], batch: [53507/55810, loss: 7.4735]\n",
            "Epoch [5/15], batch: [53508/55810, loss: 7.0923]\n",
            "Epoch [5/15], batch: [53509/55810, loss: 6.6435]\n",
            "Epoch [5/15], batch: [53510/55810, loss: 6.1203]\n",
            "Epoch [5/15], batch: [53511/55810, loss: 5.0556]\n",
            "Epoch [5/15], batch: [53512/55810, loss: 7.7732]\n",
            "Epoch [5/15], batch: [53513/55810, loss: 7.4940]\n",
            "Epoch [5/15], batch: [53514/55810, loss: 7.9329]\n",
            "Epoch [5/15], batch: [53515/55810, loss: 5.6896]\n",
            "Epoch [5/15], batch: [53516/55810, loss: 6.1117]\n",
            "Epoch [5/15], batch: [53517/55810, loss: 8.0600]\n",
            "Epoch [5/15], batch: [53518/55810, loss: 5.6102]\n",
            "Epoch [5/15], batch: [53519/55810, loss: 6.1372]\n",
            "Epoch [5/15], batch: [53520/55810, loss: 5.9982]\n",
            "Epoch [5/15], batch: [53521/55810, loss: 5.0626]\n",
            "Epoch [5/15], batch: [53522/55810, loss: 7.6898]\n",
            "Epoch [5/15], batch: [53523/55810, loss: 8.2270]\n",
            "Epoch [5/15], batch: [53524/55810, loss: 6.4667]\n",
            "Epoch [5/15], batch: [53525/55810, loss: 6.5906]\n",
            "Epoch [5/15], batch: [53526/55810, loss: 8.2583]\n",
            "Epoch [5/15], batch: [53527/55810, loss: 7.1429]\n",
            "Epoch [5/15], batch: [53528/55810, loss: 6.2112]\n",
            "Epoch [5/15], batch: [53529/55810, loss: 7.6983]\n",
            "Epoch [5/15], batch: [53530/55810, loss: 6.6167]\n",
            "Epoch [5/15], batch: [53531/55810, loss: 5.1568]\n",
            "Epoch [5/15], batch: [53532/55810, loss: 4.5295]\n",
            "Epoch [5/15], batch: [53533/55810, loss: 7.9505]\n",
            "Epoch [5/15], batch: [53534/55810, loss: 7.1628]\n",
            "Epoch [5/15], batch: [53535/55810, loss: 5.1771]\n",
            "Epoch [5/15], batch: [53536/55810, loss: 7.7862]\n",
            "Epoch [5/15], batch: [53537/55810, loss: 6.0248]\n",
            "Epoch [5/15], batch: [53538/55810, loss: 5.7855]\n",
            "Epoch [5/15], batch: [53539/55810, loss: 6.7460]\n",
            "Epoch [5/15], batch: [53540/55810, loss: 7.2278]\n",
            "Epoch [5/15], batch: [53541/55810, loss: 7.1146]\n",
            "Epoch [5/15], batch: [53542/55810, loss: 6.3310]\n",
            "Epoch [5/15], batch: [53543/55810, loss: 6.9662]\n",
            "Epoch [5/15], batch: [53544/55810, loss: 7.7096]\n",
            "Epoch [5/15], batch: [53545/55810, loss: 7.2845]\n",
            "Epoch [5/15], batch: [53546/55810, loss: 7.5640]\n",
            "Epoch [5/15], batch: [53547/55810, loss: 5.2176]\n",
            "Epoch [5/15], batch: [53548/55810, loss: 6.7780]\n",
            "Epoch [5/15], batch: [53549/55810, loss: 8.5138]\n",
            "Epoch [5/15], batch: [53550/55810, loss: 6.6560]\n",
            "Epoch [5/15], batch: [53551/55810, loss: 8.2912]\n",
            "Epoch [5/15], batch: [53552/55810, loss: 8.0111]\n",
            "Epoch [5/15], batch: [53553/55810, loss: 5.0484]\n",
            "Epoch [5/15], batch: [53554/55810, loss: 5.9123]\n",
            "Epoch [5/15], batch: [53555/55810, loss: 7.2185]\n",
            "Epoch [5/15], batch: [53556/55810, loss: 8.1285]\n",
            "Epoch [5/15], batch: [53557/55810, loss: 8.3370]\n",
            "Epoch [5/15], batch: [53558/55810, loss: 8.2703]\n",
            "Epoch [5/15], batch: [53559/55810, loss: 6.3349]\n",
            "Epoch [5/15], batch: [53560/55810, loss: 6.9397]\n",
            "Epoch [5/15], batch: [53561/55810, loss: 8.1360]\n",
            "Epoch [5/15], batch: [53562/55810, loss: 7.4775]\n",
            "Epoch [5/15], batch: [53563/55810, loss: 6.1857]\n",
            "Epoch [5/15], batch: [53564/55810, loss: 7.0257]\n",
            "Epoch [5/15], batch: [53565/55810, loss: 6.5035]\n",
            "Epoch [5/15], batch: [53566/55810, loss: 6.9923]\n",
            "Epoch [5/15], batch: [53567/55810, loss: 7.2323]\n",
            "Epoch [5/15], batch: [53568/55810, loss: 6.4483]\n",
            "Epoch [5/15], batch: [53569/55810, loss: 7.2883]\n",
            "Epoch [5/15], batch: [53570/55810, loss: 6.1211]\n",
            "Epoch [5/15], batch: [53571/55810, loss: 6.4284]\n",
            "Epoch [5/15], batch: [53572/55810, loss: 7.5738]\n",
            "Epoch [5/15], batch: [53573/55810, loss: 5.6359]\n",
            "Epoch [5/15], batch: [53574/55810, loss: 6.3505]\n",
            "Epoch [5/15], batch: [53575/55810, loss: 7.1601]\n",
            "Epoch [5/15], batch: [53576/55810, loss: 7.2358]\n",
            "Epoch [5/15], batch: [53577/55810, loss: 6.4293]\n",
            "Epoch [5/15], batch: [53578/55810, loss: 7.7181]\n",
            "Epoch [5/15], batch: [53579/55810, loss: 5.9525]\n",
            "Epoch [5/15], batch: [53580/55810, loss: 7.9895]\n",
            "Epoch [5/15], batch: [53581/55810, loss: 6.9008]\n",
            "Epoch [5/15], batch: [53582/55810, loss: 7.4746]\n",
            "Epoch [5/15], batch: [53583/55810, loss: 6.5826]\n",
            "Epoch [5/15], batch: [53584/55810, loss: 7.7787]\n",
            "Epoch [5/15], batch: [53585/55810, loss: 8.4958]\n",
            "Epoch [5/15], batch: [53586/55810, loss: 7.5417]\n",
            "Epoch [5/15], batch: [53587/55810, loss: 6.9370]\n",
            "Epoch [5/15], batch: [53588/55810, loss: 6.3432]\n",
            "Epoch [5/15], batch: [53589/55810, loss: 7.9132]\n",
            "Epoch [5/15], batch: [53590/55810, loss: 6.6889]\n",
            "Epoch [5/15], batch: [53591/55810, loss: 6.0705]\n",
            "Epoch [5/15], batch: [53592/55810, loss: 6.8188]\n",
            "Epoch [5/15], batch: [53593/55810, loss: 8.3039]\n",
            "Epoch [5/15], batch: [53594/55810, loss: 5.8247]\n",
            "Epoch [5/15], batch: [53595/55810, loss: 7.8383]\n",
            "Epoch [5/15], batch: [53596/55810, loss: 5.9076]\n",
            "Epoch [5/15], batch: [53597/55810, loss: 5.1133]\n",
            "Epoch [5/15], batch: [53598/55810, loss: 7.5582]\n",
            "Epoch [5/15], batch: [53599/55810, loss: 5.3511]\n",
            "Epoch [5/15], batch: [53600/55810, loss: 6.7388]\n",
            "Epoch [5/15], batch: [53601/55810, loss: 7.2351]\n",
            "Epoch [5/15], batch: [53602/55810, loss: 8.2826]\n",
            "Epoch [5/15], batch: [53603/55810, loss: 7.0999]\n",
            "Epoch [5/15], batch: [53604/55810, loss: 8.7719]\n",
            "Epoch [5/15], batch: [53605/55810, loss: 6.3270]\n",
            "Epoch [5/15], batch: [53606/55810, loss: 8.5351]\n",
            "Epoch [5/15], batch: [53607/55810, loss: 7.0268]\n",
            "Epoch [5/15], batch: [53608/55810, loss: 8.7198]\n",
            "Epoch [5/15], batch: [53609/55810, loss: 8.1681]\n",
            "Epoch [5/15], batch: [53610/55810, loss: 6.8818]\n",
            "Epoch [5/15], batch: [53611/55810, loss: 5.8372]\n",
            "Epoch [5/15], batch: [53612/55810, loss: 5.1452]\n",
            "Epoch [5/15], batch: [53613/55810, loss: 6.0817]\n",
            "Epoch [5/15], batch: [53614/55810, loss: 7.0581]\n",
            "Epoch [5/15], batch: [53615/55810, loss: 6.8428]\n",
            "Epoch [5/15], batch: [53616/55810, loss: 5.8141]\n",
            "Epoch [5/15], batch: [53617/55810, loss: 7.1900]\n",
            "Epoch [5/15], batch: [53618/55810, loss: 7.6534]\n",
            "Epoch [5/15], batch: [53619/55810, loss: 8.6063]\n",
            "Epoch [5/15], batch: [53620/55810, loss: 7.1296]\n",
            "Epoch [5/15], batch: [53621/55810, loss: 6.9431]\n",
            "Epoch [5/15], batch: [53622/55810, loss: 6.3492]\n",
            "Epoch [5/15], batch: [53623/55810, loss: 6.3735]\n",
            "Epoch [5/15], batch: [53624/55810, loss: 6.4469]\n",
            "Epoch [5/15], batch: [53625/55810, loss: 5.5916]\n",
            "Epoch [5/15], batch: [53626/55810, loss: 6.9795]\n",
            "Epoch [5/15], batch: [53627/55810, loss: 4.2446]\n",
            "Epoch [5/15], batch: [53628/55810, loss: 5.5384]\n",
            "Epoch [5/15], batch: [53629/55810, loss: 8.7779]\n",
            "Epoch [5/15], batch: [53630/55810, loss: 5.7581]\n",
            "Epoch [5/15], batch: [53631/55810, loss: 7.9832]\n",
            "Epoch [5/15], batch: [53632/55810, loss: 7.1588]\n",
            "Epoch [5/15], batch: [53633/55810, loss: 6.8344]\n",
            "Epoch [5/15], batch: [53634/55810, loss: 5.5321]\n",
            "Epoch [5/15], batch: [53635/55810, loss: 5.5204]\n",
            "Epoch [5/15], batch: [53636/55810, loss: 6.4001]\n",
            "Epoch [5/15], batch: [53637/55810, loss: 8.4284]\n",
            "Epoch [5/15], batch: [53638/55810, loss: 5.8005]\n",
            "Epoch [5/15], batch: [53639/55810, loss: 5.6488]\n",
            "Epoch [5/15], batch: [53640/55810, loss: 5.4284]\n",
            "Epoch [5/15], batch: [53641/55810, loss: 6.9809]\n",
            "Epoch [5/15], batch: [53642/55810, loss: 7.1257]\n",
            "Epoch [5/15], batch: [53643/55810, loss: 5.4724]\n",
            "Epoch [5/15], batch: [53644/55810, loss: 7.4670]\n",
            "Epoch [5/15], batch: [53645/55810, loss: 5.8963]\n",
            "Epoch [5/15], batch: [53646/55810, loss: 6.5453]\n",
            "Epoch [5/15], batch: [53647/55810, loss: 6.2009]\n",
            "Epoch [5/15], batch: [53648/55810, loss: 7.0287]\n",
            "Epoch [5/15], batch: [53649/55810, loss: 6.7218]\n",
            "Epoch [5/15], batch: [53650/55810, loss: 5.7996]\n",
            "Epoch [5/15], batch: [53651/55810, loss: 8.0586]\n",
            "Epoch [5/15], batch: [53652/55810, loss: 5.7091]\n",
            "Epoch [5/15], batch: [53653/55810, loss: 7.1331]\n",
            "Epoch [5/15], batch: [53654/55810, loss: 7.7258]\n",
            "Epoch [5/15], batch: [53655/55810, loss: 6.3901]\n",
            "Epoch [5/15], batch: [53656/55810, loss: 6.4273]\n",
            "Epoch [5/15], batch: [53657/55810, loss: 5.7280]\n",
            "Epoch [5/15], batch: [53658/55810, loss: 6.3268]\n",
            "Epoch [5/15], batch: [53659/55810, loss: 8.0481]\n",
            "Epoch [5/15], batch: [53660/55810, loss: 5.3618]\n",
            "Epoch [5/15], batch: [53661/55810, loss: 7.2766]\n",
            "Epoch [5/15], batch: [53662/55810, loss: 7.2422]\n",
            "Epoch [5/15], batch: [53663/55810, loss: 6.7219]\n",
            "Epoch [5/15], batch: [53664/55810, loss: 6.0571]\n",
            "Epoch [5/15], batch: [53665/55810, loss: 5.9881]\n",
            "Epoch [5/15], batch: [53666/55810, loss: 7.7554]\n",
            "Epoch [5/15], batch: [53667/55810, loss: 6.3253]\n",
            "Epoch [5/15], batch: [53668/55810, loss: 6.2420]\n",
            "Epoch [5/15], batch: [53669/55810, loss: 6.3094]\n",
            "Epoch [5/15], batch: [53670/55810, loss: 5.7743]\n",
            "Epoch [5/15], batch: [53671/55810, loss: 7.4053]\n",
            "Epoch [5/15], batch: [53672/55810, loss: 6.3904]\n",
            "Epoch [5/15], batch: [53673/55810, loss: 6.4817]\n",
            "Epoch [5/15], batch: [53674/55810, loss: 5.2952]\n",
            "Epoch [5/15], batch: [53675/55810, loss: 8.3646]\n",
            "Epoch [5/15], batch: [53676/55810, loss: 7.0922]\n",
            "Epoch [5/15], batch: [53677/55810, loss: 5.8383]\n",
            "Epoch [5/15], batch: [53678/55810, loss: 7.2390]\n",
            "Epoch [5/15], batch: [53679/55810, loss: 6.6138]\n",
            "Epoch [5/15], batch: [53680/55810, loss: 5.4913]\n",
            "Epoch [5/15], batch: [53681/55810, loss: 5.3571]\n",
            "Epoch [5/15], batch: [53682/55810, loss: 7.2078]\n",
            "Epoch [5/15], batch: [53683/55810, loss: 5.6221]\n",
            "Epoch [5/15], batch: [53684/55810, loss: 7.1419]\n",
            "Epoch [5/15], batch: [53685/55810, loss: 6.7018]\n",
            "Epoch [5/15], batch: [53686/55810, loss: 6.6360]\n",
            "Epoch [5/15], batch: [53687/55810, loss: 6.3166]\n",
            "Epoch [5/15], batch: [53688/55810, loss: 6.0381]\n",
            "Epoch [5/15], batch: [53689/55810, loss: 6.6423]\n",
            "Epoch [5/15], batch: [53690/55810, loss: 7.1528]\n",
            "Epoch [5/15], batch: [53691/55810, loss: 8.0209]\n",
            "Epoch [5/15], batch: [53692/55810, loss: 5.9949]\n",
            "Epoch [5/15], batch: [53693/55810, loss: 5.9216]\n",
            "Epoch [5/15], batch: [53694/55810, loss: 6.5550]\n",
            "Epoch [5/15], batch: [53695/55810, loss: 6.5733]\n",
            "Epoch [5/15], batch: [53696/55810, loss: 7.0789]\n",
            "Epoch [5/15], batch: [53697/55810, loss: 6.8649]\n",
            "Epoch [5/15], batch: [53698/55810, loss: 6.6862]\n",
            "Epoch [5/15], batch: [53699/55810, loss: 5.3316]\n",
            "Epoch [5/15], batch: [53700/55810, loss: 6.3267]\n",
            "Epoch [5/15], batch: [53701/55810, loss: 7.3737]\n",
            "Epoch [5/15], batch: [53702/55810, loss: 6.7764]\n",
            "Epoch [5/15], batch: [53703/55810, loss: 6.6418]\n",
            "Epoch [5/15], batch: [53704/55810, loss: 5.7137]\n",
            "Epoch [5/15], batch: [53705/55810, loss: 7.2520]\n",
            "Epoch [5/15], batch: [53706/55810, loss: 8.8183]\n",
            "Epoch [5/15], batch: [53707/55810, loss: 7.5858]\n",
            "Epoch [5/15], batch: [53708/55810, loss: 7.1958]\n",
            "Epoch [5/15], batch: [53709/55810, loss: 7.2286]\n",
            "Epoch [5/15], batch: [53710/55810, loss: 6.0091]\n",
            "Epoch [5/15], batch: [53711/55810, loss: 5.4174]\n",
            "Epoch [5/15], batch: [53712/55810, loss: 8.1891]\n",
            "Epoch [5/15], batch: [53713/55810, loss: 6.7408]\n",
            "Epoch [5/15], batch: [53714/55810, loss: 7.3308]\n",
            "Epoch [5/15], batch: [53715/55810, loss: 7.2831]\n",
            "Epoch [5/15], batch: [53716/55810, loss: 7.1931]\n",
            "Epoch [5/15], batch: [53717/55810, loss: 6.3458]\n",
            "Epoch [5/15], batch: [53718/55810, loss: 5.2785]\n",
            "Epoch [5/15], batch: [53719/55810, loss: 6.8513]\n",
            "Epoch [5/15], batch: [53720/55810, loss: 8.1869]\n",
            "Epoch [5/15], batch: [53721/55810, loss: 6.4412]\n",
            "Epoch [5/15], batch: [53722/55810, loss: 6.9448]\n",
            "Epoch [5/15], batch: [53723/55810, loss: 5.9192]\n",
            "Epoch [5/15], batch: [53724/55810, loss: 5.3100]\n",
            "Epoch [5/15], batch: [53725/55810, loss: 6.6656]\n",
            "Epoch [5/15], batch: [53726/55810, loss: 5.8758]\n",
            "Epoch [5/15], batch: [53727/55810, loss: 5.4637]\n",
            "Epoch [5/15], batch: [53728/55810, loss: 7.3134]\n",
            "Epoch [5/15], batch: [53729/55810, loss: 5.8269]\n",
            "Epoch [5/15], batch: [53730/55810, loss: 4.0084]\n",
            "Epoch [5/15], batch: [53731/55810, loss: 6.0964]\n",
            "Epoch [5/15], batch: [53732/55810, loss: 4.3177]\n",
            "Epoch [5/15], batch: [53733/55810, loss: 7.3992]\n",
            "Epoch [5/15], batch: [53734/55810, loss: 7.4852]\n",
            "Epoch [5/15], batch: [53735/55810, loss: 6.1663]\n",
            "Epoch [5/15], batch: [53736/55810, loss: 5.1875]\n",
            "Epoch [5/15], batch: [53737/55810, loss: 6.7018]\n",
            "Epoch [5/15], batch: [53738/55810, loss: 7.4104]\n",
            "Epoch [5/15], batch: [53739/55810, loss: 6.3206]\n",
            "Epoch [5/15], batch: [53740/55810, loss: 7.3037]\n",
            "Epoch [5/15], batch: [53741/55810, loss: 7.8744]\n",
            "Epoch [5/15], batch: [53742/55810, loss: 6.9231]\n",
            "Epoch [5/15], batch: [53743/55810, loss: 5.1401]\n",
            "Epoch [5/15], batch: [53744/55810, loss: 4.8540]\n",
            "Epoch [5/15], batch: [53745/55810, loss: 7.3087]\n",
            "Epoch [5/15], batch: [53746/55810, loss: 7.4844]\n",
            "Epoch [5/15], batch: [53747/55810, loss: 6.3094]\n",
            "Epoch [5/15], batch: [53748/55810, loss: 6.1205]\n",
            "Epoch [5/15], batch: [53749/55810, loss: 6.9675]\n",
            "Epoch [5/15], batch: [53750/55810, loss: 7.9973]\n",
            "Epoch [5/15], batch: [53751/55810, loss: 6.0388]\n",
            "Epoch [5/15], batch: [53752/55810, loss: 5.5790]\n",
            "Epoch [5/15], batch: [53753/55810, loss: 6.3068]\n",
            "Epoch [5/15], batch: [53754/55810, loss: 5.9283]\n",
            "Epoch [5/15], batch: [53755/55810, loss: 5.2304]\n",
            "Epoch [5/15], batch: [53756/55810, loss: 5.2017]\n",
            "Epoch [5/15], batch: [53757/55810, loss: 8.6885]\n",
            "Epoch [5/15], batch: [53758/55810, loss: 7.1688]\n",
            "Epoch [5/15], batch: [53759/55810, loss: 7.0148]\n",
            "Epoch [5/15], batch: [53760/55810, loss: 7.9629]\n",
            "Epoch [5/15], batch: [53761/55810, loss: 7.4337]\n",
            "Epoch [5/15], batch: [53762/55810, loss: 5.9213]\n",
            "Epoch [5/15], batch: [53763/55810, loss: 7.6129]\n",
            "Epoch [5/15], batch: [53764/55810, loss: 5.5877]\n",
            "Epoch [5/15], batch: [53765/55810, loss: 5.9146]\n",
            "Epoch [5/15], batch: [53766/55810, loss: 5.2092]\n",
            "Epoch [5/15], batch: [53767/55810, loss: 6.9972]\n",
            "Epoch [5/15], batch: [53768/55810, loss: 5.6013]\n",
            "Epoch [5/15], batch: [53769/55810, loss: 6.6938]\n",
            "Epoch [5/15], batch: [53770/55810, loss: 6.2872]\n",
            "Epoch [5/15], batch: [53771/55810, loss: 6.7700]\n",
            "Epoch [5/15], batch: [53772/55810, loss: 6.0427]\n",
            "Epoch [5/15], batch: [53773/55810, loss: 5.1932]\n",
            "Epoch [5/15], batch: [53774/55810, loss: 7.2595]\n",
            "Epoch [5/15], batch: [53775/55810, loss: 6.9329]\n",
            "Epoch [5/15], batch: [53776/55810, loss: 7.8509]\n",
            "Epoch [5/15], batch: [53777/55810, loss: 7.6905]\n",
            "Epoch [5/15], batch: [53778/55810, loss: 7.2754]\n",
            "Epoch [5/15], batch: [53779/55810, loss: 7.8429]\n",
            "Epoch [5/15], batch: [53780/55810, loss: 5.8273]\n",
            "Epoch [5/15], batch: [53781/55810, loss: 6.3453]\n",
            "Epoch [5/15], batch: [53782/55810, loss: 5.8211]\n",
            "Epoch [5/15], batch: [53783/55810, loss: 7.6963]\n",
            "Epoch [5/15], batch: [53784/55810, loss: 4.0550]\n",
            "Epoch [5/15], batch: [53785/55810, loss: 5.1002]\n",
            "Epoch [5/15], batch: [53786/55810, loss: 5.6453]\n",
            "Epoch [5/15], batch: [53787/55810, loss: 8.0216]\n",
            "Epoch [5/15], batch: [53788/55810, loss: 7.6365]\n",
            "Epoch [5/15], batch: [53789/55810, loss: 6.4651]\n",
            "Epoch [5/15], batch: [53790/55810, loss: 5.3649]\n",
            "Epoch [5/15], batch: [53791/55810, loss: 7.4552]\n",
            "Epoch [5/15], batch: [53792/55810, loss: 6.7975]\n",
            "Epoch [5/15], batch: [53793/55810, loss: 5.9017]\n",
            "Epoch [5/15], batch: [53794/55810, loss: 7.2045]\n",
            "Epoch [5/15], batch: [53795/55810, loss: 6.5874]\n",
            "Epoch [5/15], batch: [53796/55810, loss: 5.5369]\n",
            "Epoch [5/15], batch: [53797/55810, loss: 6.3341]\n",
            "Epoch [5/15], batch: [53798/55810, loss: 5.5695]\n",
            "Epoch [5/15], batch: [53799/55810, loss: 6.9567]\n",
            "Epoch [5/15], batch: [53800/55810, loss: 7.4949]\n",
            "Epoch [5/15], batch: [53801/55810, loss: 5.0839]\n",
            "Epoch [5/15], batch: [53802/55810, loss: 6.0358]\n",
            "Epoch [5/15], batch: [53803/55810, loss: 3.8727]\n",
            "Epoch [5/15], batch: [53804/55810, loss: 7.6005]\n",
            "Epoch [5/15], batch: [53805/55810, loss: 6.8807]\n",
            "Epoch [5/15], batch: [53806/55810, loss: 5.5964]\n",
            "Epoch [5/15], batch: [53807/55810, loss: 6.1145]\n",
            "Epoch [5/15], batch: [53808/55810, loss: 6.2821]\n",
            "Epoch [5/15], batch: [53809/55810, loss: 6.9887]\n",
            "Epoch [5/15], batch: [53810/55810, loss: 7.1142]\n",
            "Epoch [5/15], batch: [53811/55810, loss: 6.5935]\n",
            "Epoch [5/15], batch: [53812/55810, loss: 7.0920]\n",
            "Epoch [5/15], batch: [53813/55810, loss: 7.7388]\n",
            "Epoch [5/15], batch: [53814/55810, loss: 7.2619]\n",
            "Epoch [5/15], batch: [53815/55810, loss: 6.5565]\n",
            "Epoch [5/15], batch: [53816/55810, loss: 7.6104]\n",
            "Epoch [5/15], batch: [53817/55810, loss: 5.4889]\n",
            "Epoch [5/15], batch: [53818/55810, loss: 6.2698]\n",
            "Epoch [5/15], batch: [53819/55810, loss: 5.2215]\n",
            "Epoch [5/15], batch: [53820/55810, loss: 6.6533]\n",
            "Epoch [5/15], batch: [53821/55810, loss: 5.3892]\n",
            "Epoch [5/15], batch: [53822/55810, loss: 6.6964]\n",
            "Epoch [5/15], batch: [53823/55810, loss: 5.3377]\n",
            "Epoch [5/15], batch: [53824/55810, loss: 6.3703]\n",
            "Epoch [5/15], batch: [53825/55810, loss: 6.6149]\n",
            "Epoch [5/15], batch: [53826/55810, loss: 6.7040]\n",
            "Epoch [5/15], batch: [53827/55810, loss: 5.2879]\n",
            "Epoch [5/15], batch: [53828/55810, loss: 6.6968]\n",
            "Epoch [5/15], batch: [53829/55810, loss: 6.0699]\n",
            "Epoch [5/15], batch: [53830/55810, loss: 8.0419]\n",
            "Epoch [5/15], batch: [53831/55810, loss: 7.6350]\n",
            "Epoch [5/15], batch: [53832/55810, loss: 7.4010]\n",
            "Epoch [5/15], batch: [53833/55810, loss: 6.2706]\n",
            "Epoch [5/15], batch: [53834/55810, loss: 8.0728]\n",
            "Epoch [5/15], batch: [53835/55810, loss: 6.8518]\n",
            "Epoch [5/15], batch: [53836/55810, loss: 5.2985]\n",
            "Epoch [5/15], batch: [53837/55810, loss: 8.2030]\n",
            "Epoch [5/15], batch: [53838/55810, loss: 5.7450]\n",
            "Epoch [5/15], batch: [53839/55810, loss: 5.5526]\n",
            "Epoch [5/15], batch: [53840/55810, loss: 6.6517]\n",
            "Epoch [5/15], batch: [53841/55810, loss: 7.9315]\n",
            "Epoch [5/15], batch: [53842/55810, loss: 6.1576]\n",
            "Epoch [5/15], batch: [53843/55810, loss: 6.1967]\n",
            "Epoch [5/15], batch: [53844/55810, loss: 6.6332]\n",
            "Epoch [5/15], batch: [53845/55810, loss: 6.8708]\n",
            "Epoch [5/15], batch: [53846/55810, loss: 6.9837]\n",
            "Epoch [5/15], batch: [53847/55810, loss: 6.8686]\n",
            "Epoch [5/15], batch: [53848/55810, loss: 7.5153]\n",
            "Epoch [5/15], batch: [53849/55810, loss: 4.8451]\n",
            "Epoch [5/15], batch: [53850/55810, loss: 5.9593]\n",
            "Epoch [5/15], batch: [53851/55810, loss: 8.5065]\n",
            "Epoch [5/15], batch: [53852/55810, loss: 6.8993]\n",
            "Epoch [5/15], batch: [53853/55810, loss: 5.9994]\n",
            "Epoch [5/15], batch: [53854/55810, loss: 6.4592]\n",
            "Epoch [5/15], batch: [53855/55810, loss: 7.3118]\n",
            "Epoch [5/15], batch: [53856/55810, loss: 6.5784]\n",
            "Epoch [5/15], batch: [53857/55810, loss: 5.6023]\n",
            "Epoch [5/15], batch: [53858/55810, loss: 7.2357]\n",
            "Epoch [5/15], batch: [53859/55810, loss: 6.5390]\n",
            "Epoch [5/15], batch: [53860/55810, loss: 6.7215]\n",
            "Epoch [5/15], batch: [53861/55810, loss: 6.1028]\n",
            "Epoch [5/15], batch: [53862/55810, loss: 7.5795]\n",
            "Epoch [5/15], batch: [53863/55810, loss: 5.8285]\n",
            "Epoch [5/15], batch: [53864/55810, loss: 6.7372]\n",
            "Epoch [5/15], batch: [53865/55810, loss: 7.0416]\n",
            "Epoch [5/15], batch: [53866/55810, loss: 5.5291]\n",
            "Epoch [5/15], batch: [53867/55810, loss: 4.2157]\n",
            "Epoch [5/15], batch: [53868/55810, loss: 6.5466]\n",
            "Epoch [5/15], batch: [53869/55810, loss: 7.0252]\n",
            "Epoch [5/15], batch: [53870/55810, loss: 6.9706]\n",
            "Epoch [5/15], batch: [53871/55810, loss: 6.6984]\n",
            "Epoch [5/15], batch: [53872/55810, loss: 5.0090]\n",
            "Epoch [5/15], batch: [53873/55810, loss: 7.4953]\n",
            "Epoch [5/15], batch: [53874/55810, loss: 5.5674]\n",
            "Epoch [5/15], batch: [53875/55810, loss: 5.0184]\n",
            "Epoch [5/15], batch: [53876/55810, loss: 6.7879]\n",
            "Epoch [5/15], batch: [53877/55810, loss: 4.0603]\n",
            "Epoch [5/15], batch: [53878/55810, loss: 7.1218]\n",
            "Epoch [5/15], batch: [53879/55810, loss: 6.9463]\n",
            "Epoch [5/15], batch: [53880/55810, loss: 4.0860]\n",
            "Epoch [5/15], batch: [53881/55810, loss: 5.5793]\n",
            "Epoch [5/15], batch: [53882/55810, loss: 5.2986]\n",
            "Epoch [5/15], batch: [53883/55810, loss: 5.1258]\n",
            "Epoch [5/15], batch: [53884/55810, loss: 5.5023]\n",
            "Epoch [5/15], batch: [53885/55810, loss: 6.7314]\n",
            "Epoch [5/15], batch: [53886/55810, loss: 5.5828]\n",
            "Epoch [5/15], batch: [53887/55810, loss: 7.4083]\n",
            "Epoch [5/15], batch: [53888/55810, loss: 8.4033]\n",
            "Epoch [5/15], batch: [53889/55810, loss: 9.3898]\n",
            "Epoch [5/15], batch: [53890/55810, loss: 7.3396]\n",
            "Epoch [5/15], batch: [53891/55810, loss: 7.8746]\n",
            "Epoch [5/15], batch: [53892/55810, loss: 6.6492]\n",
            "Epoch [5/15], batch: [53893/55810, loss: 8.7824]\n",
            "Epoch [5/15], batch: [53894/55810, loss: 8.0180]\n",
            "Epoch [5/15], batch: [53895/55810, loss: 6.8571]\n",
            "Epoch [5/15], batch: [53896/55810, loss: 7.4279]\n",
            "Epoch [5/15], batch: [53897/55810, loss: 6.4079]\n",
            "Epoch [5/15], batch: [53898/55810, loss: 6.7112]\n",
            "Epoch [5/15], batch: [53899/55810, loss: 6.0258]\n",
            "Epoch [5/15], batch: [53900/55810, loss: 6.4545]\n",
            "Epoch [5/15], batch: [53901/55810, loss: 7.4844]\n",
            "Epoch [5/15], batch: [53902/55810, loss: 7.2615]\n",
            "Epoch [5/15], batch: [53903/55810, loss: 8.3758]\n",
            "Epoch [5/15], batch: [53904/55810, loss: 4.6110]\n",
            "Epoch [5/15], batch: [53905/55810, loss: 4.9120]\n",
            "Epoch [5/15], batch: [53906/55810, loss: 6.4225]\n",
            "Epoch [5/15], batch: [53907/55810, loss: 5.9960]\n",
            "Epoch [5/15], batch: [53908/55810, loss: 6.2545]\n",
            "Epoch [5/15], batch: [53909/55810, loss: 6.0544]\n",
            "Epoch [5/15], batch: [53910/55810, loss: 7.2324]\n",
            "Epoch [5/15], batch: [53911/55810, loss: 8.2397]\n",
            "Epoch [5/15], batch: [53912/55810, loss: 6.4483]\n",
            "Epoch [5/15], batch: [53913/55810, loss: 6.3507]\n",
            "Epoch [5/15], batch: [53914/55810, loss: 5.4073]\n",
            "Epoch [5/15], batch: [53915/55810, loss: 7.8218]\n",
            "Epoch [5/15], batch: [53916/55810, loss: 7.9042]\n",
            "Epoch [5/15], batch: [53917/55810, loss: 7.0174]\n",
            "Epoch [5/15], batch: [53918/55810, loss: 8.2725]\n",
            "Epoch [5/15], batch: [53919/55810, loss: 6.1929]\n",
            "Epoch [5/15], batch: [53920/55810, loss: 8.5141]\n",
            "Epoch [5/15], batch: [53921/55810, loss: 4.8245]\n",
            "Epoch [5/15], batch: [53922/55810, loss: 6.9527]\n",
            "Epoch [5/15], batch: [53923/55810, loss: 6.6903]\n",
            "Epoch [5/15], batch: [53924/55810, loss: 6.7078]\n",
            "Epoch [5/15], batch: [53925/55810, loss: 7.2311]\n",
            "Epoch [5/15], batch: [53926/55810, loss: 7.4725]\n",
            "Epoch [5/15], batch: [53927/55810, loss: 7.9822]\n",
            "Epoch [5/15], batch: [53928/55810, loss: 9.1454]\n",
            "Epoch [5/15], batch: [53929/55810, loss: 6.6860]\n",
            "Epoch [5/15], batch: [53930/55810, loss: 8.0861]\n",
            "Epoch [5/15], batch: [53931/55810, loss: 6.9430]\n",
            "Epoch [5/15], batch: [53932/55810, loss: 9.3449]\n",
            "Epoch [5/15], batch: [53933/55810, loss: 6.9401]\n",
            "Epoch [5/15], batch: [53934/55810, loss: 8.5243]\n",
            "Epoch [5/15], batch: [53935/55810, loss: 7.9366]\n",
            "Epoch [5/15], batch: [53936/55810, loss: 7.7095]\n",
            "Epoch [5/15], batch: [53937/55810, loss: 8.3920]\n",
            "Epoch [5/15], batch: [53938/55810, loss: 9.1508]\n",
            "Epoch [5/15], batch: [53939/55810, loss: 7.2136]\n",
            "Epoch [5/15], batch: [53940/55810, loss: 8.4626]\n",
            "Epoch [5/15], batch: [53941/55810, loss: 8.1274]\n",
            "Epoch [5/15], batch: [53942/55810, loss: 6.9706]\n",
            "Epoch [5/15], batch: [53943/55810, loss: 8.3300]\n",
            "Epoch [5/15], batch: [53944/55810, loss: 7.4101]\n",
            "Epoch [5/15], batch: [53945/55810, loss: 8.1750]\n",
            "Epoch [5/15], batch: [53946/55810, loss: 6.8187]\n",
            "Epoch [5/15], batch: [53947/55810, loss: 7.0709]\n",
            "Epoch [5/15], batch: [53948/55810, loss: 7.7584]\n",
            "Epoch [5/15], batch: [53949/55810, loss: 8.2910]\n",
            "Epoch [5/15], batch: [53950/55810, loss: 6.6810]\n",
            "Epoch [5/15], batch: [53951/55810, loss: 6.8688]\n",
            "Epoch [5/15], batch: [53952/55810, loss: 8.1096]\n",
            "Epoch [5/15], batch: [53953/55810, loss: 8.9767]\n",
            "Epoch [5/15], batch: [53954/55810, loss: 6.6321]\n",
            "Epoch [5/15], batch: [53955/55810, loss: 7.4144]\n",
            "Epoch [5/15], batch: [53956/55810, loss: 8.7689]\n",
            "Epoch [5/15], batch: [53957/55810, loss: 7.9359]\n",
            "Epoch [5/15], batch: [53958/55810, loss: 8.2141]\n",
            "Epoch [5/15], batch: [53959/55810, loss: 5.4125]\n",
            "Epoch [5/15], batch: [53960/55810, loss: 6.1830]\n",
            "Epoch [5/15], batch: [53961/55810, loss: 5.9523]\n",
            "Epoch [5/15], batch: [53962/55810, loss: 7.1167]\n",
            "Epoch [5/15], batch: [53963/55810, loss: 8.0172]\n",
            "Epoch [5/15], batch: [53964/55810, loss: 6.8446]\n",
            "Epoch [5/15], batch: [53965/55810, loss: 7.3956]\n",
            "Epoch [5/15], batch: [53966/55810, loss: 9.0350]\n",
            "Epoch [5/15], batch: [53967/55810, loss: 7.0673]\n",
            "Epoch [5/15], batch: [53968/55810, loss: 7.1316]\n",
            "Epoch [5/15], batch: [53969/55810, loss: 8.8450]\n",
            "Epoch [5/15], batch: [53970/55810, loss: 5.3487]\n",
            "Epoch [5/15], batch: [53971/55810, loss: 6.9603]\n",
            "Epoch [5/15], batch: [53972/55810, loss: 7.9831]\n",
            "Epoch [5/15], batch: [53973/55810, loss: 7.9049]\n",
            "Epoch [5/15], batch: [53974/55810, loss: 7.3042]\n",
            "Epoch [5/15], batch: [53975/55810, loss: 7.5718]\n",
            "Epoch [5/15], batch: [53976/55810, loss: 6.8487]\n",
            "Epoch [5/15], batch: [53977/55810, loss: 7.9345]\n",
            "Epoch [5/15], batch: [53978/55810, loss: 9.2577]\n",
            "Epoch [5/15], batch: [53979/55810, loss: 8.6247]\n",
            "Epoch [5/15], batch: [53980/55810, loss: 7.0005]\n",
            "Epoch [5/15], batch: [53981/55810, loss: 7.9592]\n",
            "Epoch [5/15], batch: [53982/55810, loss: 7.5076]\n",
            "Epoch [5/15], batch: [53983/55810, loss: 5.2789]\n",
            "Epoch [5/15], batch: [53984/55810, loss: 6.5273]\n",
            "Epoch [5/15], batch: [53985/55810, loss: 8.1882]\n",
            "Epoch [5/15], batch: [53986/55810, loss: 7.0496]\n",
            "Epoch [5/15], batch: [53987/55810, loss: 7.1203]\n",
            "Epoch [5/15], batch: [53988/55810, loss: 5.4008]\n",
            "Epoch [5/15], batch: [53989/55810, loss: 4.3441]\n",
            "Epoch [5/15], batch: [53990/55810, loss: 7.2746]\n",
            "Epoch [5/15], batch: [53991/55810, loss: 7.2973]\n",
            "Epoch [5/15], batch: [53992/55810, loss: 6.2493]\n",
            "Epoch [5/15], batch: [53993/55810, loss: 6.8947]\n",
            "Epoch [5/15], batch: [53994/55810, loss: 6.4999]\n",
            "Epoch [5/15], batch: [53995/55810, loss: 6.6012]\n",
            "Epoch [5/15], batch: [53996/55810, loss: 5.7064]\n",
            "Epoch [5/15], batch: [53997/55810, loss: 8.7290]\n",
            "Epoch [5/15], batch: [53998/55810, loss: 7.1193]\n",
            "Epoch [5/15], batch: [53999/55810, loss: 7.4627]\n",
            "Epoch [5/15], batch: [54000/55810, loss: 7.1171]\n",
            "Epoch [5/15], batch: [54001/55810, loss: 6.7420]\n",
            "Epoch [5/15], batch: [54002/55810, loss: 7.1462]\n",
            "Epoch [5/15], batch: [54003/55810, loss: 5.5685]\n",
            "Epoch [5/15], batch: [54004/55810, loss: 6.9537]\n",
            "Epoch [5/15], batch: [54005/55810, loss: 8.3692]\n",
            "Epoch [5/15], batch: [54006/55810, loss: 5.9841]\n",
            "Epoch [5/15], batch: [54007/55810, loss: 4.8490]\n",
            "Epoch [5/15], batch: [54008/55810, loss: 8.3655]\n",
            "Epoch [5/15], batch: [54009/55810, loss: 5.7277]\n",
            "Epoch [5/15], batch: [54010/55810, loss: 7.4006]\n",
            "Epoch [5/15], batch: [54011/55810, loss: 8.1427]\n",
            "Epoch [5/15], batch: [54012/55810, loss: 6.8013]\n",
            "Epoch [5/15], batch: [54013/55810, loss: 6.3391]\n",
            "Epoch [5/15], batch: [54014/55810, loss: 6.3395]\n",
            "Epoch [5/15], batch: [54015/55810, loss: 6.6605]\n",
            "Epoch [5/15], batch: [54016/55810, loss: 5.9525]\n",
            "Epoch [5/15], batch: [54017/55810, loss: 5.6876]\n",
            "Epoch [5/15], batch: [54018/55810, loss: 6.0955]\n",
            "Epoch [5/15], batch: [54019/55810, loss: 7.4029]\n",
            "Epoch [5/15], batch: [54020/55810, loss: 6.7951]\n",
            "Epoch [5/15], batch: [54021/55810, loss: 6.4531]\n",
            "Epoch [5/15], batch: [54022/55810, loss: 5.8710]\n",
            "Epoch [5/15], batch: [54023/55810, loss: 6.1738]\n",
            "Epoch [5/15], batch: [54024/55810, loss: 7.2796]\n",
            "Epoch [5/15], batch: [54025/55810, loss: 6.1001]\n",
            "Epoch [5/15], batch: [54026/55810, loss: 6.3627]\n",
            "Epoch [5/15], batch: [54027/55810, loss: 7.6839]\n",
            "Epoch [5/15], batch: [54028/55810, loss: 4.6976]\n",
            "Epoch [5/15], batch: [54029/55810, loss: 5.9738]\n",
            "Epoch [5/15], batch: [54030/55810, loss: 6.0433]\n",
            "Epoch [5/15], batch: [54031/55810, loss: 4.1992]\n",
            "Epoch [5/15], batch: [54032/55810, loss: 7.0177]\n",
            "Epoch [5/15], batch: [54033/55810, loss: 5.0511]\n",
            "Epoch [5/15], batch: [54034/55810, loss: 7.8807]\n",
            "Epoch [5/15], batch: [54035/55810, loss: 7.0865]\n",
            "Epoch [5/15], batch: [54036/55810, loss: 5.3393]\n",
            "Epoch [5/15], batch: [54037/55810, loss: 7.5993]\n",
            "Epoch [5/15], batch: [54038/55810, loss: 7.0643]\n",
            "Epoch [5/15], batch: [54039/55810, loss: 6.0902]\n",
            "Epoch [5/15], batch: [54040/55810, loss: 5.6405]\n",
            "Epoch [5/15], batch: [54041/55810, loss: 7.2978]\n",
            "Epoch [5/15], batch: [54042/55810, loss: 7.2729]\n",
            "Epoch [5/15], batch: [54043/55810, loss: 7.6640]\n",
            "Epoch [5/15], batch: [54044/55810, loss: 8.0609]\n",
            "Epoch [5/15], batch: [54045/55810, loss: 7.5746]\n",
            "Epoch [5/15], batch: [54046/55810, loss: 7.7889]\n",
            "Epoch [5/15], batch: [54047/55810, loss: 7.0343]\n",
            "Epoch [5/15], batch: [54048/55810, loss: 8.0561]\n",
            "Epoch [5/15], batch: [54049/55810, loss: 6.9486]\n",
            "Epoch [5/15], batch: [54050/55810, loss: 6.0787]\n",
            "Epoch [5/15], batch: [54051/55810, loss: 8.2827]\n",
            "Epoch [5/15], batch: [54052/55810, loss: 7.6367]\n",
            "Epoch [5/15], batch: [54053/55810, loss: 7.7971]\n",
            "Epoch [5/15], batch: [54054/55810, loss: 6.8516]\n",
            "Epoch [5/15], batch: [54055/55810, loss: 5.8509]\n",
            "Epoch [5/15], batch: [54056/55810, loss: 8.0875]\n",
            "Epoch [5/15], batch: [54057/55810, loss: 3.5843]\n",
            "Epoch [5/15], batch: [54058/55810, loss: 6.5647]\n",
            "Epoch [5/15], batch: [54059/55810, loss: 5.6682]\n",
            "Epoch [5/15], batch: [54060/55810, loss: 7.5090]\n",
            "Epoch [5/15], batch: [54061/55810, loss: 7.3261]\n",
            "Epoch [5/15], batch: [54062/55810, loss: 7.2564]\n",
            "Epoch [5/15], batch: [54063/55810, loss: 6.2850]\n",
            "Epoch [5/15], batch: [54064/55810, loss: 7.0818]\n",
            "Epoch [5/15], batch: [54065/55810, loss: 6.0207]\n",
            "Epoch [5/15], batch: [54066/55810, loss: 6.4493]\n",
            "Epoch [5/15], batch: [54067/55810, loss: 8.0274]\n",
            "Epoch [5/15], batch: [54068/55810, loss: 7.7530]\n",
            "Epoch [5/15], batch: [54069/55810, loss: 5.9103]\n",
            "Epoch [5/15], batch: [54070/55810, loss: 6.6569]\n",
            "Epoch [5/15], batch: [54071/55810, loss: 7.6530]\n",
            "Epoch [5/15], batch: [54072/55810, loss: 7.7066]\n",
            "Epoch [5/15], batch: [54073/55810, loss: 7.1038]\n",
            "Epoch [5/15], batch: [54074/55810, loss: 8.1120]\n",
            "Epoch [5/15], batch: [54075/55810, loss: 6.8675]\n",
            "Epoch [5/15], batch: [54076/55810, loss: 7.0979]\n",
            "Epoch [5/15], batch: [54077/55810, loss: 6.4538]\n",
            "Epoch [5/15], batch: [54078/55810, loss: 5.5714]\n",
            "Epoch [5/15], batch: [54079/55810, loss: 6.3269]\n",
            "Epoch [5/15], batch: [54080/55810, loss: 7.6645]\n",
            "Epoch [5/15], batch: [54081/55810, loss: 5.6975]\n",
            "Epoch [5/15], batch: [54082/55810, loss: 7.6814]\n",
            "Epoch [5/15], batch: [54083/55810, loss: 7.1755]\n",
            "Epoch [5/15], batch: [54084/55810, loss: 6.4000]\n",
            "Epoch [5/15], batch: [54085/55810, loss: 5.8476]\n",
            "Epoch [5/15], batch: [54086/55810, loss: 6.8997]\n",
            "Epoch [5/15], batch: [54087/55810, loss: 6.3829]\n",
            "Epoch [5/15], batch: [54088/55810, loss: 7.6186]\n",
            "Epoch [5/15], batch: [54089/55810, loss: 6.4275]\n",
            "Epoch [5/15], batch: [54090/55810, loss: 6.3612]\n",
            "Epoch [5/15], batch: [54091/55810, loss: 5.9735]\n",
            "Epoch [5/15], batch: [54092/55810, loss: 5.8279]\n",
            "Epoch [5/15], batch: [54093/55810, loss: 7.0196]\n",
            "Epoch [5/15], batch: [54094/55810, loss: 5.7738]\n",
            "Epoch [5/15], batch: [54095/55810, loss: 5.7640]\n",
            "Epoch [5/15], batch: [54096/55810, loss: 5.8874]\n",
            "Epoch [5/15], batch: [54097/55810, loss: 6.0593]\n",
            "Epoch [5/15], batch: [54098/55810, loss: 6.7402]\n",
            "Epoch [5/15], batch: [54099/55810, loss: 6.5674]\n",
            "Epoch [5/15], batch: [54100/55810, loss: 6.8620]\n",
            "Epoch [5/15], batch: [54101/55810, loss: 6.0506]\n",
            "Epoch [5/15], batch: [54102/55810, loss: 6.8490]\n",
            "Epoch [5/15], batch: [54103/55810, loss: 6.7247]\n",
            "Epoch [5/15], batch: [54104/55810, loss: 7.7038]\n",
            "Epoch [5/15], batch: [54105/55810, loss: 7.5712]\n",
            "Epoch [5/15], batch: [54106/55810, loss: 6.9998]\n",
            "Epoch [5/15], batch: [54107/55810, loss: 5.1298]\n",
            "Epoch [5/15], batch: [54108/55810, loss: 7.8513]\n",
            "Epoch [5/15], batch: [54109/55810, loss: 7.1395]\n",
            "Epoch [5/15], batch: [54110/55810, loss: 8.7148]\n",
            "Epoch [5/15], batch: [54111/55810, loss: 6.2240]\n",
            "Epoch [5/15], batch: [54112/55810, loss: 5.6384]\n",
            "Epoch [5/15], batch: [54113/55810, loss: 4.8650]\n",
            "Epoch [5/15], batch: [54114/55810, loss: 6.4621]\n",
            "Epoch [5/15], batch: [54115/55810, loss: 7.4282]\n",
            "Epoch [5/15], batch: [54116/55810, loss: 4.4033]\n",
            "Epoch [5/15], batch: [54117/55810, loss: 4.4991]\n",
            "Epoch [5/15], batch: [54118/55810, loss: 6.2850]\n",
            "Epoch [5/15], batch: [54119/55810, loss: 6.1441]\n",
            "Epoch [5/15], batch: [54120/55810, loss: 6.6872]\n",
            "Epoch [5/15], batch: [54121/55810, loss: 7.6213]\n",
            "Epoch [5/15], batch: [54122/55810, loss: 5.4501]\n",
            "Epoch [5/15], batch: [54123/55810, loss: 7.1609]\n",
            "Epoch [5/15], batch: [54124/55810, loss: 7.6508]\n",
            "Epoch [5/15], batch: [54125/55810, loss: 5.8036]\n",
            "Epoch [5/15], batch: [54126/55810, loss: 5.9524]\n",
            "Epoch [5/15], batch: [54127/55810, loss: 4.9034]\n",
            "Epoch [5/15], batch: [54128/55810, loss: 7.3053]\n",
            "Epoch [5/15], batch: [54129/55810, loss: 6.1470]\n",
            "Epoch [5/15], batch: [54130/55810, loss: 6.3760]\n",
            "Epoch [5/15], batch: [54131/55810, loss: 7.3675]\n",
            "Epoch [5/15], batch: [54132/55810, loss: 5.8176]\n",
            "Epoch [5/15], batch: [54133/55810, loss: 5.1444]\n",
            "Epoch [5/15], batch: [54134/55810, loss: 6.1136]\n",
            "Epoch [5/15], batch: [54135/55810, loss: 5.2720]\n",
            "Epoch [5/15], batch: [54136/55810, loss: 7.5397]\n",
            "Epoch [5/15], batch: [54137/55810, loss: 6.4794]\n",
            "Epoch [5/15], batch: [54138/55810, loss: 4.2145]\n",
            "Epoch [5/15], batch: [54139/55810, loss: 5.1277]\n",
            "Epoch [5/15], batch: [54140/55810, loss: 5.8556]\n",
            "Epoch [5/15], batch: [54141/55810, loss: 6.4494]\n",
            "Epoch [5/15], batch: [54142/55810, loss: 7.1376]\n",
            "Epoch [5/15], batch: [54143/55810, loss: 7.6688]\n",
            "Epoch [5/15], batch: [54144/55810, loss: 6.0753]\n",
            "Epoch [5/15], batch: [54145/55810, loss: 5.2479]\n",
            "Epoch [5/15], batch: [54146/55810, loss: 6.5052]\n",
            "Epoch [5/15], batch: [54147/55810, loss: 5.0377]\n",
            "Epoch [5/15], batch: [54148/55810, loss: 6.8992]\n",
            "Epoch [5/15], batch: [54149/55810, loss: 7.3308]\n",
            "Epoch [5/15], batch: [54150/55810, loss: 7.5960]\n",
            "Epoch [5/15], batch: [54151/55810, loss: 6.1812]\n",
            "Epoch [5/15], batch: [54152/55810, loss: 6.7905]\n",
            "Epoch [5/15], batch: [54153/55810, loss: 4.8345]\n",
            "Epoch [5/15], batch: [54154/55810, loss: 7.9989]\n",
            "Epoch [5/15], batch: [54155/55810, loss: 5.9863]\n",
            "Epoch [5/15], batch: [54156/55810, loss: 6.8053]\n",
            "Epoch [5/15], batch: [54157/55810, loss: 5.7265]\n",
            "Epoch [5/15], batch: [54158/55810, loss: 6.2458]\n",
            "Epoch [5/15], batch: [54159/55810, loss: 5.5190]\n",
            "Epoch [5/15], batch: [54160/55810, loss: 6.2978]\n",
            "Epoch [5/15], batch: [54161/55810, loss: 5.7744]\n",
            "Epoch [5/15], batch: [54162/55810, loss: 5.3950]\n",
            "Epoch [5/15], batch: [54163/55810, loss: 6.4483]\n",
            "Epoch [5/15], batch: [54164/55810, loss: 7.3597]\n",
            "Epoch [5/15], batch: [54165/55810, loss: 6.2304]\n",
            "Epoch [5/15], batch: [54166/55810, loss: 8.3501]\n",
            "Epoch [5/15], batch: [54167/55810, loss: 7.5989]\n",
            "Epoch [5/15], batch: [54168/55810, loss: 7.0694]\n",
            "Epoch [5/15], batch: [54169/55810, loss: 5.9334]\n",
            "Epoch [5/15], batch: [54170/55810, loss: 4.4086]\n",
            "Epoch [5/15], batch: [54171/55810, loss: 6.2514]\n",
            "Epoch [5/15], batch: [54172/55810, loss: 6.4101]\n",
            "Epoch [5/15], batch: [54173/55810, loss: 6.7381]\n",
            "Epoch [5/15], batch: [54174/55810, loss: 6.9990]\n",
            "Epoch [5/15], batch: [54175/55810, loss: 6.8709]\n",
            "Epoch [5/15], batch: [54176/55810, loss: 6.8650]\n",
            "Epoch [5/15], batch: [54177/55810, loss: 5.3864]\n",
            "Epoch [5/15], batch: [54178/55810, loss: 7.0040]\n",
            "Epoch [5/15], batch: [54179/55810, loss: 7.2099]\n",
            "Epoch [5/15], batch: [54180/55810, loss: 6.6263]\n",
            "Epoch [5/15], batch: [54181/55810, loss: 6.4492]\n",
            "Epoch [5/15], batch: [54182/55810, loss: 5.4749]\n",
            "Epoch [5/15], batch: [54183/55810, loss: 5.5191]\n",
            "Epoch [5/15], batch: [54184/55810, loss: 6.4095]\n",
            "Epoch [5/15], batch: [54185/55810, loss: 7.3090]\n",
            "Epoch [5/15], batch: [54186/55810, loss: 5.2353]\n",
            "Epoch [5/15], batch: [54187/55810, loss: 7.2992]\n",
            "Epoch [5/15], batch: [54188/55810, loss: 8.0227]\n",
            "Epoch [5/15], batch: [54189/55810, loss: 8.2260]\n",
            "Epoch [5/15], batch: [54190/55810, loss: 7.8715]\n",
            "Epoch [5/15], batch: [54191/55810, loss: 7.8367]\n",
            "Epoch [5/15], batch: [54192/55810, loss: 7.9386]\n",
            "Epoch [5/15], batch: [54193/55810, loss: 7.2338]\n",
            "Epoch [5/15], batch: [54194/55810, loss: 6.4152]\n",
            "Epoch [5/15], batch: [54195/55810, loss: 6.3665]\n",
            "Epoch [5/15], batch: [54196/55810, loss: 6.8882]\n",
            "Epoch [5/15], batch: [54197/55810, loss: 7.1739]\n",
            "Epoch [5/15], batch: [54198/55810, loss: 7.9503]\n",
            "Epoch [5/15], batch: [54199/55810, loss: 8.5751]\n",
            "Epoch [5/15], batch: [54200/55810, loss: 6.9425]\n",
            "Epoch [5/15], batch: [54201/55810, loss: 6.7047]\n",
            "Epoch [5/15], batch: [54202/55810, loss: 7.3723]\n",
            "Epoch [5/15], batch: [54203/55810, loss: 6.9212]\n",
            "Epoch [5/15], batch: [54204/55810, loss: 6.3183]\n",
            "Epoch [5/15], batch: [54205/55810, loss: 7.7102]\n",
            "Epoch [5/15], batch: [54206/55810, loss: 5.9381]\n",
            "Epoch [5/15], batch: [54207/55810, loss: 6.8684]\n",
            "Epoch [5/15], batch: [54208/55810, loss: 5.2664]\n",
            "Epoch [5/15], batch: [54209/55810, loss: 6.0104]\n",
            "Epoch [5/15], batch: [54210/55810, loss: 7.7218]\n",
            "Epoch [5/15], batch: [54211/55810, loss: 7.5110]\n",
            "Epoch [5/15], batch: [54212/55810, loss: 6.5615]\n",
            "Epoch [5/15], batch: [54213/55810, loss: 7.0202]\n",
            "Epoch [5/15], batch: [54214/55810, loss: 8.1527]\n",
            "Epoch [5/15], batch: [54215/55810, loss: 6.2763]\n",
            "Epoch [5/15], batch: [54216/55810, loss: 6.7312]\n",
            "Epoch [5/15], batch: [54217/55810, loss: 7.4572]\n",
            "Epoch [5/15], batch: [54218/55810, loss: 5.8786]\n",
            "Epoch [5/15], batch: [54219/55810, loss: 5.4257]\n",
            "Epoch [5/15], batch: [54220/55810, loss: 6.2697]\n",
            "Epoch [5/15], batch: [54221/55810, loss: 6.6036]\n",
            "Epoch [5/15], batch: [54222/55810, loss: 8.0295]\n",
            "Epoch [5/15], batch: [54223/55810, loss: 4.4589]\n",
            "Epoch [5/15], batch: [54224/55810, loss: 7.3786]\n",
            "Epoch [5/15], batch: [54225/55810, loss: 7.6425]\n",
            "Epoch [5/15], batch: [54226/55810, loss: 5.5971]\n",
            "Epoch [5/15], batch: [54227/55810, loss: 8.1300]\n",
            "Epoch [5/15], batch: [54228/55810, loss: 6.7116]\n",
            "Epoch [5/15], batch: [54229/55810, loss: 6.5133]\n",
            "Epoch [5/15], batch: [54230/55810, loss: 7.1461]\n",
            "Epoch [5/15], batch: [54231/55810, loss: 5.3177]\n",
            "Epoch [5/15], batch: [54232/55810, loss: 5.1623]\n",
            "Epoch [5/15], batch: [54233/55810, loss: 5.6451]\n",
            "Epoch [5/15], batch: [54234/55810, loss: 7.9436]\n",
            "Epoch [5/15], batch: [54235/55810, loss: 7.0623]\n",
            "Epoch [5/15], batch: [54236/55810, loss: 8.1150]\n",
            "Epoch [5/15], batch: [54237/55810, loss: 6.5639]\n",
            "Epoch [5/15], batch: [54238/55810, loss: 6.6949]\n",
            "Epoch [5/15], batch: [54239/55810, loss: 7.4178]\n",
            "Epoch [5/15], batch: [54240/55810, loss: 5.1116]\n",
            "Epoch [5/15], batch: [54241/55810, loss: 7.2077]\n",
            "Epoch [5/15], batch: [54242/55810, loss: 7.1507]\n",
            "Epoch [5/15], batch: [54243/55810, loss: 6.2982]\n",
            "Epoch [5/15], batch: [54244/55810, loss: 6.6549]\n",
            "Epoch [5/15], batch: [54245/55810, loss: 5.8386]\n",
            "Epoch [5/15], batch: [54246/55810, loss: 5.2256]\n",
            "Epoch [5/15], batch: [54247/55810, loss: 5.3845]\n",
            "Epoch [5/15], batch: [54248/55810, loss: 8.0422]\n",
            "Epoch [5/15], batch: [54249/55810, loss: 7.1854]\n",
            "Epoch [5/15], batch: [54250/55810, loss: 7.0341]\n",
            "Epoch [5/15], batch: [54251/55810, loss: 3.3558]\n",
            "Epoch [5/15], batch: [54252/55810, loss: 4.8685]\n",
            "Epoch [5/15], batch: [54253/55810, loss: 6.2168]\n",
            "Epoch [5/15], batch: [54254/55810, loss: 8.3988]\n",
            "Epoch [5/15], batch: [54255/55810, loss: 5.5691]\n",
            "Epoch [5/15], batch: [54256/55810, loss: 5.7951]\n",
            "Epoch [5/15], batch: [54257/55810, loss: 5.7814]\n",
            "Epoch [5/15], batch: [54258/55810, loss: 6.5586]\n",
            "Epoch [5/15], batch: [54259/55810, loss: 7.4664]\n",
            "Epoch [5/15], batch: [54260/55810, loss: 6.9853]\n",
            "Epoch [5/15], batch: [54261/55810, loss: 6.2626]\n",
            "Epoch [5/15], batch: [54262/55810, loss: 7.2385]\n",
            "Epoch [5/15], batch: [54263/55810, loss: 7.4131]\n",
            "Epoch [5/15], batch: [54264/55810, loss: 6.4805]\n",
            "Epoch [5/15], batch: [54265/55810, loss: 5.1695]\n",
            "Epoch [5/15], batch: [54266/55810, loss: 6.7440]\n",
            "Epoch [5/15], batch: [54267/55810, loss: 6.8005]\n",
            "Epoch [5/15], batch: [54268/55810, loss: 7.2582]\n",
            "Epoch [5/15], batch: [54269/55810, loss: 5.5143]\n",
            "Epoch [5/15], batch: [54270/55810, loss: 7.1466]\n",
            "Epoch [5/15], batch: [54271/55810, loss: 7.9221]\n",
            "Epoch [5/15], batch: [54272/55810, loss: 5.5104]\n",
            "Epoch [5/15], batch: [54273/55810, loss: 5.6689]\n",
            "Epoch [5/15], batch: [54274/55810, loss: 5.0869]\n",
            "Epoch [5/15], batch: [54275/55810, loss: 6.9711]\n",
            "Epoch [5/15], batch: [54276/55810, loss: 7.5025]\n",
            "Epoch [5/15], batch: [54277/55810, loss: 5.7170]\n",
            "Epoch [5/15], batch: [54278/55810, loss: 5.8413]\n",
            "Epoch [5/15], batch: [54279/55810, loss: 7.1899]\n",
            "Epoch [5/15], batch: [54280/55810, loss: 6.1509]\n",
            "Epoch [5/15], batch: [54281/55810, loss: 7.2829]\n",
            "Epoch [5/15], batch: [54282/55810, loss: 7.8429]\n",
            "Epoch [5/15], batch: [54283/55810, loss: 5.9818]\n",
            "Epoch [5/15], batch: [54284/55810, loss: 6.0762]\n",
            "Epoch [5/15], batch: [54285/55810, loss: 7.5237]\n",
            "Epoch [5/15], batch: [54286/55810, loss: 8.8682]\n",
            "Epoch [5/15], batch: [54287/55810, loss: 6.1859]\n",
            "Epoch [5/15], batch: [54288/55810, loss: 6.5557]\n",
            "Epoch [5/15], batch: [54289/55810, loss: 7.0460]\n",
            "Epoch [5/15], batch: [54290/55810, loss: 6.5862]\n",
            "Epoch [5/15], batch: [54291/55810, loss: 8.5075]\n",
            "Epoch [5/15], batch: [54292/55810, loss: 6.6991]\n",
            "Epoch [5/15], batch: [54293/55810, loss: 4.9405]\n",
            "Epoch [5/15], batch: [54294/55810, loss: 4.3480]\n",
            "Epoch [5/15], batch: [54295/55810, loss: 6.0915]\n",
            "Epoch [5/15], batch: [54296/55810, loss: 5.3170]\n",
            "Epoch [5/15], batch: [54297/55810, loss: 6.0219]\n",
            "Epoch [5/15], batch: [54298/55810, loss: 7.6439]\n",
            "Epoch [5/15], batch: [54299/55810, loss: 4.8245]\n",
            "Epoch [5/15], batch: [54300/55810, loss: 6.2808]\n",
            "Epoch [5/15], batch: [54301/55810, loss: 7.4579]\n",
            "Epoch [5/15], batch: [54302/55810, loss: 7.7137]\n",
            "Epoch [5/15], batch: [54303/55810, loss: 5.2232]\n",
            "Epoch [5/15], batch: [54304/55810, loss: 5.3109]\n",
            "Epoch [5/15], batch: [54305/55810, loss: 5.9786]\n",
            "Epoch [5/15], batch: [54306/55810, loss: 7.1539]\n",
            "Epoch [5/15], batch: [54307/55810, loss: 7.2880]\n",
            "Epoch [5/15], batch: [54308/55810, loss: 8.0304]\n",
            "Epoch [5/15], batch: [54309/55810, loss: 7.4237]\n",
            "Epoch [5/15], batch: [54310/55810, loss: 7.9979]\n",
            "Epoch [5/15], batch: [54311/55810, loss: 7.4420]\n",
            "Epoch [5/15], batch: [54312/55810, loss: 7.4598]\n",
            "Epoch [5/15], batch: [54313/55810, loss: 7.2013]\n",
            "Epoch [5/15], batch: [54314/55810, loss: 8.5632]\n",
            "Epoch [5/15], batch: [54315/55810, loss: 5.9108]\n",
            "Epoch [5/15], batch: [54316/55810, loss: 6.0804]\n",
            "Epoch [5/15], batch: [54317/55810, loss: 6.9044]\n",
            "Epoch [5/15], batch: [54318/55810, loss: 8.5963]\n",
            "Epoch [5/15], batch: [54319/55810, loss: 8.1767]\n",
            "Epoch [5/15], batch: [54320/55810, loss: 5.1118]\n",
            "Epoch [5/15], batch: [54321/55810, loss: 5.7727]\n",
            "Epoch [5/15], batch: [54322/55810, loss: 6.1160]\n",
            "Epoch [5/15], batch: [54323/55810, loss: 3.6739]\n",
            "Epoch [5/15], batch: [54324/55810, loss: 6.8163]\n",
            "Epoch [5/15], batch: [54325/55810, loss: 4.7931]\n",
            "Epoch [5/15], batch: [54326/55810, loss: 6.2227]\n",
            "Epoch [5/15], batch: [54327/55810, loss: 6.4397]\n",
            "Epoch [5/15], batch: [54328/55810, loss: 5.8310]\n",
            "Epoch [5/15], batch: [54329/55810, loss: 6.0182]\n",
            "Epoch [5/15], batch: [54330/55810, loss: 6.8756]\n",
            "Epoch [5/15], batch: [54331/55810, loss: 5.7187]\n",
            "Epoch [5/15], batch: [54332/55810, loss: 6.7365]\n",
            "Epoch [5/15], batch: [54333/55810, loss: 6.5027]\n",
            "Epoch [5/15], batch: [54334/55810, loss: 5.0927]\n",
            "Epoch [5/15], batch: [54335/55810, loss: 6.2025]\n",
            "Epoch [5/15], batch: [54336/55810, loss: 5.2313]\n",
            "Epoch [5/15], batch: [54337/55810, loss: 7.3456]\n",
            "Epoch [5/15], batch: [54338/55810, loss: 7.2140]\n",
            "Epoch [5/15], batch: [54339/55810, loss: 5.5931]\n",
            "Epoch [5/15], batch: [54340/55810, loss: 7.7016]\n",
            "Epoch [5/15], batch: [54341/55810, loss: 7.5927]\n",
            "Epoch [5/15], batch: [54342/55810, loss: 7.3422]\n",
            "Epoch [5/15], batch: [54343/55810, loss: 7.4888]\n",
            "Epoch [5/15], batch: [54344/55810, loss: 7.7293]\n",
            "Epoch [5/15], batch: [54345/55810, loss: 8.1293]\n",
            "Epoch [5/15], batch: [54346/55810, loss: 7.4486]\n",
            "Epoch [5/15], batch: [54347/55810, loss: 7.1808]\n",
            "Epoch [5/15], batch: [54348/55810, loss: 8.3063]\n",
            "Epoch [5/15], batch: [54349/55810, loss: 5.2266]\n",
            "Epoch [5/15], batch: [54350/55810, loss: 5.9144]\n",
            "Epoch [5/15], batch: [54351/55810, loss: 6.0266]\n",
            "Epoch [5/15], batch: [54352/55810, loss: 6.7954]\n",
            "Epoch [5/15], batch: [54353/55810, loss: 6.4590]\n",
            "Epoch [5/15], batch: [54354/55810, loss: 5.2763]\n",
            "Epoch [5/15], batch: [54355/55810, loss: 6.2144]\n",
            "Epoch [5/15], batch: [54356/55810, loss: 6.5502]\n",
            "Epoch [5/15], batch: [54357/55810, loss: 6.5927]\n",
            "Epoch [5/15], batch: [54358/55810, loss: 6.9805]\n",
            "Epoch [5/15], batch: [54359/55810, loss: 5.1598]\n",
            "Epoch [5/15], batch: [54360/55810, loss: 6.9199]\n",
            "Epoch [5/15], batch: [54361/55810, loss: 5.3738]\n",
            "Epoch [5/15], batch: [54362/55810, loss: 5.4650]\n",
            "Epoch [5/15], batch: [54363/55810, loss: 8.0789]\n",
            "Epoch [5/15], batch: [54364/55810, loss: 6.1080]\n",
            "Epoch [5/15], batch: [54365/55810, loss: 7.3058]\n",
            "Epoch [5/15], batch: [54366/55810, loss: 6.5960]\n",
            "Epoch [5/15], batch: [54367/55810, loss: 7.5730]\n",
            "Epoch [5/15], batch: [54368/55810, loss: 7.0559]\n",
            "Epoch [5/15], batch: [54369/55810, loss: 6.7188]\n",
            "Epoch [5/15], batch: [54370/55810, loss: 3.5907]\n",
            "Epoch [5/15], batch: [54371/55810, loss: 5.5683]\n",
            "Epoch [5/15], batch: [54372/55810, loss: 5.5794]\n",
            "Epoch [5/15], batch: [54373/55810, loss: 6.9596]\n",
            "Epoch [5/15], batch: [54374/55810, loss: 6.2963]\n",
            "Epoch [5/15], batch: [54375/55810, loss: 6.5687]\n",
            "Epoch [5/15], batch: [54376/55810, loss: 6.2812]\n",
            "Epoch [5/15], batch: [54377/55810, loss: 4.0933]\n",
            "Epoch [5/15], batch: [54378/55810, loss: 6.5302]\n",
            "Epoch [5/15], batch: [54379/55810, loss: 5.8023]\n",
            "Epoch [5/15], batch: [54380/55810, loss: 7.1600]\n",
            "Epoch [5/15], batch: [54381/55810, loss: 7.1781]\n",
            "Epoch [5/15], batch: [54382/55810, loss: 5.8380]\n",
            "Epoch [5/15], batch: [54383/55810, loss: 5.7379]\n",
            "Epoch [5/15], batch: [54384/55810, loss: 6.7585]\n",
            "Epoch [5/15], batch: [54385/55810, loss: 7.8060]\n",
            "Epoch [5/15], batch: [54386/55810, loss: 7.6516]\n",
            "Epoch [5/15], batch: [54387/55810, loss: 7.0441]\n",
            "Epoch [5/15], batch: [54388/55810, loss: 5.2678]\n",
            "Epoch [5/15], batch: [54389/55810, loss: 7.1749]\n",
            "Epoch [5/15], batch: [54390/55810, loss: 7.8082]\n",
            "Epoch [5/15], batch: [54391/55810, loss: 7.0341]\n",
            "Epoch [5/15], batch: [54392/55810, loss: 7.1418]\n",
            "Epoch [5/15], batch: [54393/55810, loss: 6.1502]\n",
            "Epoch [5/15], batch: [54394/55810, loss: 5.9503]\n",
            "Epoch [5/15], batch: [54395/55810, loss: 5.9668]\n",
            "Epoch [5/15], batch: [54396/55810, loss: 7.0591]\n",
            "Epoch [5/15], batch: [54397/55810, loss: 6.3227]\n",
            "Epoch [5/15], batch: [54398/55810, loss: 7.3641]\n",
            "Epoch [5/15], batch: [54399/55810, loss: 6.0708]\n",
            "Epoch [5/15], batch: [54400/55810, loss: 6.2934]\n",
            "Epoch [5/15], batch: [54401/55810, loss: 6.5857]\n",
            "Epoch [5/15], batch: [54402/55810, loss: 7.6747]\n",
            "Epoch [5/15], batch: [54403/55810, loss: 5.7196]\n",
            "Epoch [5/15], batch: [54404/55810, loss: 6.2328]\n",
            "Epoch [5/15], batch: [54405/55810, loss: 5.5549]\n",
            "Epoch [5/15], batch: [54406/55810, loss: 6.4080]\n",
            "Epoch [5/15], batch: [54407/55810, loss: 9.1184]\n",
            "Epoch [5/15], batch: [54408/55810, loss: 6.9372]\n",
            "Epoch [5/15], batch: [54409/55810, loss: 6.7101]\n",
            "Epoch [5/15], batch: [54410/55810, loss: 6.5899]\n",
            "Epoch [5/15], batch: [54411/55810, loss: 6.3749]\n",
            "Epoch [5/15], batch: [54412/55810, loss: 6.1889]\n",
            "Epoch [5/15], batch: [54413/55810, loss: 6.8875]\n",
            "Epoch [5/15], batch: [54414/55810, loss: 5.7887]\n",
            "Epoch [5/15], batch: [54415/55810, loss: 5.4381]\n",
            "Epoch [5/15], batch: [54416/55810, loss: 4.5626]\n",
            "Epoch [5/15], batch: [54417/55810, loss: 5.1172]\n",
            "Epoch [5/15], batch: [54418/55810, loss: 6.6095]\n",
            "Epoch [5/15], batch: [54419/55810, loss: 6.8009]\n",
            "Epoch [5/15], batch: [54420/55810, loss: 4.9240]\n",
            "Epoch [5/15], batch: [54421/55810, loss: 5.3375]\n",
            "Epoch [5/15], batch: [54422/55810, loss: 5.8309]\n",
            "Epoch [5/15], batch: [54423/55810, loss: 7.1570]\n",
            "Epoch [5/15], batch: [54424/55810, loss: 6.3513]\n",
            "Epoch [5/15], batch: [54425/55810, loss: 6.9197]\n",
            "Epoch [5/15], batch: [54426/55810, loss: 5.9717]\n",
            "Epoch [5/15], batch: [54427/55810, loss: 7.0268]\n",
            "Epoch [5/15], batch: [54428/55810, loss: 6.3666]\n",
            "Epoch [5/15], batch: [54429/55810, loss: 7.3094]\n",
            "Epoch [5/15], batch: [54430/55810, loss: 5.5934]\n",
            "Epoch [5/15], batch: [54431/55810, loss: 8.4474]\n",
            "Epoch [5/15], batch: [54432/55810, loss: 8.1444]\n",
            "Epoch [5/15], batch: [54433/55810, loss: 7.7107]\n",
            "Epoch [5/15], batch: [54434/55810, loss: 4.5192]\n",
            "Epoch [5/15], batch: [54435/55810, loss: 5.4152]\n",
            "Epoch [5/15], batch: [54436/55810, loss: 5.9408]\n",
            "Epoch [5/15], batch: [54437/55810, loss: 7.6107]\n",
            "Epoch [5/15], batch: [54438/55810, loss: 6.0008]\n",
            "Epoch [5/15], batch: [54439/55810, loss: 7.9925]\n",
            "Epoch [5/15], batch: [54440/55810, loss: 7.3087]\n",
            "Epoch [5/15], batch: [54441/55810, loss: 6.4535]\n",
            "Epoch [5/15], batch: [54442/55810, loss: 4.8076]\n",
            "Epoch [5/15], batch: [54443/55810, loss: 5.3203]\n",
            "Epoch [5/15], batch: [54444/55810, loss: 7.7169]\n",
            "Epoch [5/15], batch: [54445/55810, loss: 7.8498]\n",
            "Epoch [5/15], batch: [54446/55810, loss: 7.0482]\n",
            "Epoch [5/15], batch: [54447/55810, loss: 4.7188]\n",
            "Epoch [5/15], batch: [54448/55810, loss: 5.1456]\n",
            "Epoch [5/15], batch: [54449/55810, loss: 6.1381]\n",
            "Epoch [5/15], batch: [54450/55810, loss: 4.8266]\n",
            "Epoch [5/15], batch: [54451/55810, loss: 5.7258]\n",
            "Epoch [5/15], batch: [54452/55810, loss: 6.5971]\n",
            "Epoch [5/15], batch: [54453/55810, loss: 4.8799]\n",
            "Epoch [5/15], batch: [54454/55810, loss: 6.4424]\n",
            "Epoch [5/15], batch: [54455/55810, loss: 5.8616]\n",
            "Epoch [5/15], batch: [54456/55810, loss: 6.5752]\n",
            "Epoch [5/15], batch: [54457/55810, loss: 7.2066]\n",
            "Epoch [5/15], batch: [54458/55810, loss: 5.0041]\n",
            "Epoch [5/15], batch: [54459/55810, loss: 7.0025]\n",
            "Epoch [5/15], batch: [54460/55810, loss: 6.5315]\n",
            "Epoch [5/15], batch: [54461/55810, loss: 5.4818]\n",
            "Epoch [5/15], batch: [54462/55810, loss: 7.4246]\n",
            "Epoch [5/15], batch: [54463/55810, loss: 4.0374]\n",
            "Epoch [5/15], batch: [54464/55810, loss: 7.0164]\n",
            "Epoch [5/15], batch: [54465/55810, loss: 7.5036]\n",
            "Epoch [5/15], batch: [54466/55810, loss: 6.9731]\n",
            "Epoch [5/15], batch: [54467/55810, loss: 6.5913]\n",
            "Epoch [5/15], batch: [54468/55810, loss: 5.3110]\n",
            "Epoch [5/15], batch: [54469/55810, loss: 5.9714]\n",
            "Epoch [5/15], batch: [54470/55810, loss: 6.6225]\n",
            "Epoch [5/15], batch: [54471/55810, loss: 5.9149]\n",
            "Epoch [5/15], batch: [54472/55810, loss: 6.6306]\n",
            "Epoch [5/15], batch: [54473/55810, loss: 7.9037]\n",
            "Epoch [5/15], batch: [54474/55810, loss: 6.3004]\n",
            "Epoch [5/15], batch: [54475/55810, loss: 7.6595]\n",
            "Epoch [5/15], batch: [54476/55810, loss: 7.1476]\n",
            "Epoch [5/15], batch: [54477/55810, loss: 6.8028]\n",
            "Epoch [5/15], batch: [54478/55810, loss: 6.4490]\n",
            "Epoch [5/15], batch: [54479/55810, loss: 7.1772]\n",
            "Epoch [5/15], batch: [54480/55810, loss: 6.8786]\n",
            "Epoch [5/15], batch: [54481/55810, loss: 6.6602]\n",
            "Epoch [5/15], batch: [54482/55810, loss: 6.1347]\n",
            "Epoch [5/15], batch: [54483/55810, loss: 5.4279]\n",
            "Epoch [5/15], batch: [54484/55810, loss: 4.6659]\n",
            "Epoch [5/15], batch: [54485/55810, loss: 5.9533]\n",
            "Epoch [5/15], batch: [54486/55810, loss: 7.0712]\n",
            "Epoch [5/15], batch: [54487/55810, loss: 6.8062]\n",
            "Epoch [5/15], batch: [54488/55810, loss: 4.6971]\n",
            "Epoch [5/15], batch: [54489/55810, loss: 3.7662]\n",
            "Epoch [5/15], batch: [54490/55810, loss: 4.5707]\n",
            "Epoch [5/15], batch: [54491/55810, loss: 6.4838]\n",
            "Epoch [5/15], batch: [54492/55810, loss: 7.3575]\n",
            "Epoch [5/15], batch: [54493/55810, loss: 6.1792]\n",
            "Epoch [5/15], batch: [54494/55810, loss: 7.0490]\n",
            "Epoch [5/15], batch: [54495/55810, loss: 5.8482]\n",
            "Epoch [5/15], batch: [54496/55810, loss: 6.0928]\n",
            "Epoch [5/15], batch: [54497/55810, loss: 6.7763]\n",
            "Epoch [5/15], batch: [54498/55810, loss: 7.3318]\n",
            "Epoch [5/15], batch: [54499/55810, loss: 8.4342]\n",
            "Epoch [5/15], batch: [54500/55810, loss: 5.8090]\n",
            "Epoch [5/15], batch: [54501/55810, loss: 6.7514]\n",
            "Epoch [5/15], batch: [54502/55810, loss: 8.4114]\n",
            "Epoch [5/15], batch: [54503/55810, loss: 7.7619]\n",
            "Epoch [5/15], batch: [54504/55810, loss: 6.9478]\n",
            "Epoch [5/15], batch: [54505/55810, loss: 7.4088]\n",
            "Epoch [5/15], batch: [54506/55810, loss: 4.9497]\n",
            "Epoch [5/15], batch: [54507/55810, loss: 8.1494]\n",
            "Epoch [5/15], batch: [54508/55810, loss: 8.3072]\n",
            "Epoch [5/15], batch: [54509/55810, loss: 6.8684]\n",
            "Epoch [5/15], batch: [54510/55810, loss: 6.7813]\n",
            "Epoch [5/15], batch: [54511/55810, loss: 5.7572]\n",
            "Epoch [5/15], batch: [54512/55810, loss: 7.4796]\n",
            "Epoch [5/15], batch: [54513/55810, loss: 7.8181]\n",
            "Epoch [5/15], batch: [54514/55810, loss: 4.8242]\n",
            "Epoch [5/15], batch: [54515/55810, loss: 5.1128]\n",
            "Epoch [5/15], batch: [54516/55810, loss: 6.0845]\n",
            "Epoch [5/15], batch: [54517/55810, loss: 7.3347]\n",
            "Epoch [5/15], batch: [54518/55810, loss: 5.0736]\n",
            "Epoch [5/15], batch: [54519/55810, loss: 6.8843]\n",
            "Epoch [5/15], batch: [54520/55810, loss: 5.7685]\n",
            "Epoch [5/15], batch: [54521/55810, loss: 6.0219]\n",
            "Epoch [5/15], batch: [54522/55810, loss: 4.5387]\n",
            "Epoch [5/15], batch: [54523/55810, loss: 7.5144]\n",
            "Epoch [5/15], batch: [54524/55810, loss: 6.0779]\n",
            "Epoch [5/15], batch: [54525/55810, loss: 7.1207]\n",
            "Epoch [5/15], batch: [54526/55810, loss: 6.2930]\n",
            "Epoch [5/15], batch: [54527/55810, loss: 5.4314]\n",
            "Epoch [5/15], batch: [54528/55810, loss: 7.1517]\n",
            "Epoch [5/15], batch: [54529/55810, loss: 5.9305]\n",
            "Epoch [5/15], batch: [54530/55810, loss: 7.4178]\n",
            "Epoch [5/15], batch: [54531/55810, loss: 7.2024]\n",
            "Epoch [5/15], batch: [54532/55810, loss: 6.7003]\n",
            "Epoch [5/15], batch: [54533/55810, loss: 6.2141]\n",
            "Epoch [5/15], batch: [54534/55810, loss: 7.6536]\n",
            "Epoch [5/15], batch: [54535/55810, loss: 7.3887]\n",
            "Epoch [5/15], batch: [54536/55810, loss: 6.5577]\n",
            "Epoch [5/15], batch: [54537/55810, loss: 6.3104]\n",
            "Epoch [5/15], batch: [54538/55810, loss: 6.6766]\n",
            "Epoch [5/15], batch: [54539/55810, loss: 5.9512]\n",
            "Epoch [5/15], batch: [54540/55810, loss: 6.8865]\n",
            "Epoch [5/15], batch: [54541/55810, loss: 4.7459]\n",
            "Epoch [5/15], batch: [54542/55810, loss: 7.7090]\n",
            "Epoch [5/15], batch: [54543/55810, loss: 7.6479]\n",
            "Epoch [5/15], batch: [54544/55810, loss: 7.5414]\n",
            "Epoch [5/15], batch: [54545/55810, loss: 7.6413]\n",
            "Epoch [5/15], batch: [54546/55810, loss: 8.8015]\n",
            "Epoch [5/15], batch: [54547/55810, loss: 7.4982]\n",
            "Epoch [5/15], batch: [54548/55810, loss: 7.2566]\n",
            "Epoch [5/15], batch: [54549/55810, loss: 5.2536]\n",
            "Epoch [5/15], batch: [54550/55810, loss: 6.2023]\n",
            "Epoch [5/15], batch: [54551/55810, loss: 5.4651]\n",
            "Epoch [5/15], batch: [54552/55810, loss: 4.7440]\n",
            "Epoch [5/15], batch: [54553/55810, loss: 6.6854]\n",
            "Epoch [5/15], batch: [54554/55810, loss: 4.3088]\n",
            "Epoch [5/15], batch: [54555/55810, loss: 5.3410]\n",
            "Epoch [5/15], batch: [54556/55810, loss: 7.7099]\n",
            "Epoch [5/15], batch: [54557/55810, loss: 5.9282]\n",
            "Epoch [5/15], batch: [54558/55810, loss: 5.0345]\n",
            "Epoch [5/15], batch: [54559/55810, loss: 4.5224]\n",
            "Epoch [5/15], batch: [54560/55810, loss: 4.9514]\n",
            "Epoch [5/15], batch: [54561/55810, loss: 4.6273]\n",
            "Epoch [5/15], batch: [54562/55810, loss: 5.5931]\n",
            "Epoch [5/15], batch: [54563/55810, loss: 7.7528]\n",
            "Epoch [5/15], batch: [54564/55810, loss: 7.8361]\n",
            "Epoch [5/15], batch: [54565/55810, loss: 7.0006]\n",
            "Epoch [5/15], batch: [54566/55810, loss: 6.8922]\n",
            "Epoch [5/15], batch: [54567/55810, loss: 6.5471]\n",
            "Epoch [5/15], batch: [54568/55810, loss: 4.6351]\n",
            "Epoch [5/15], batch: [54569/55810, loss: 5.3292]\n",
            "Epoch [5/15], batch: [54570/55810, loss: 5.6564]\n",
            "Epoch [5/15], batch: [54571/55810, loss: 6.1558]\n",
            "Epoch [5/15], batch: [54572/55810, loss: 6.8881]\n",
            "Epoch [5/15], batch: [54573/55810, loss: 7.6610]\n",
            "Epoch [5/15], batch: [54574/55810, loss: 7.3112]\n",
            "Epoch [5/15], batch: [54575/55810, loss: 7.1733]\n",
            "Epoch [5/15], batch: [54576/55810, loss: 8.2434]\n",
            "Epoch [5/15], batch: [54577/55810, loss: 7.8234]\n",
            "Epoch [5/15], batch: [54578/55810, loss: 6.9879]\n",
            "Epoch [5/15], batch: [54579/55810, loss: 8.9438]\n",
            "Epoch [5/15], batch: [54580/55810, loss: 6.6744]\n",
            "Epoch [5/15], batch: [54581/55810, loss: 7.3602]\n",
            "Epoch [5/15], batch: [54582/55810, loss: 6.9583]\n",
            "Epoch [5/15], batch: [54583/55810, loss: 7.4305]\n",
            "Epoch [5/15], batch: [54584/55810, loss: 5.9548]\n",
            "Epoch [5/15], batch: [54585/55810, loss: 6.1892]\n",
            "Epoch [5/15], batch: [54586/55810, loss: 5.0246]\n",
            "Epoch [5/15], batch: [54587/55810, loss: 5.5137]\n",
            "Epoch [5/15], batch: [54588/55810, loss: 8.3434]\n",
            "Epoch [5/15], batch: [54589/55810, loss: 7.9766]\n",
            "Epoch [5/15], batch: [54590/55810, loss: 6.3128]\n",
            "Epoch [5/15], batch: [54591/55810, loss: 5.2012]\n",
            "Epoch [5/15], batch: [54592/55810, loss: 6.0943]\n",
            "Epoch [5/15], batch: [54593/55810, loss: 6.3424]\n",
            "Epoch [5/15], batch: [54594/55810, loss: 6.4729]\n",
            "Epoch [5/15], batch: [54595/55810, loss: 6.1922]\n",
            "Epoch [5/15], batch: [54596/55810, loss: 5.6849]\n",
            "Epoch [5/15], batch: [54597/55810, loss: 5.9451]\n",
            "Epoch [5/15], batch: [54598/55810, loss: 6.8386]\n",
            "Epoch [5/15], batch: [54599/55810, loss: 4.6454]\n",
            "Epoch [5/15], batch: [54600/55810, loss: 4.5701]\n",
            "Epoch [5/15], batch: [54601/55810, loss: 4.3804]\n",
            "Epoch [5/15], batch: [54602/55810, loss: 5.7934]\n",
            "Epoch [5/15], batch: [54603/55810, loss: 6.5351]\n",
            "Epoch [5/15], batch: [54604/55810, loss: 5.3904]\n",
            "Epoch [5/15], batch: [54605/55810, loss: 5.3232]\n",
            "Epoch [5/15], batch: [54606/55810, loss: 5.9029]\n",
            "Epoch [5/15], batch: [54607/55810, loss: 6.7407]\n",
            "Epoch [5/15], batch: [54608/55810, loss: 5.6462]\n",
            "Epoch [5/15], batch: [54609/55810, loss: 7.0645]\n",
            "Epoch [5/15], batch: [54610/55810, loss: 4.4676]\n",
            "Epoch [5/15], batch: [54611/55810, loss: 6.3428]\n",
            "Epoch [5/15], batch: [54612/55810, loss: 6.9785]\n",
            "Epoch [5/15], batch: [54613/55810, loss: 7.1141]\n",
            "Epoch [5/15], batch: [54614/55810, loss: 7.6685]\n",
            "Epoch [5/15], batch: [54615/55810, loss: 7.0267]\n",
            "Epoch [5/15], batch: [54616/55810, loss: 5.0919]\n",
            "Epoch [5/15], batch: [54617/55810, loss: 6.2205]\n",
            "Epoch [5/15], batch: [54618/55810, loss: 4.9893]\n",
            "Epoch [5/15], batch: [54619/55810, loss: 4.3573]\n",
            "Epoch [5/15], batch: [54620/55810, loss: 5.7551]\n",
            "Epoch [5/15], batch: [54621/55810, loss: 4.9023]\n",
            "Epoch [5/15], batch: [54622/55810, loss: 5.5279]\n",
            "Epoch [5/15], batch: [54623/55810, loss: 7.2333]\n",
            "Epoch [5/15], batch: [54624/55810, loss: 7.0572]\n",
            "Epoch [5/15], batch: [54625/55810, loss: 6.3683]\n",
            "Epoch [5/15], batch: [54626/55810, loss: 6.1653]\n",
            "Epoch [5/15], batch: [54627/55810, loss: 6.6821]\n",
            "Epoch [5/15], batch: [54628/55810, loss: 7.1760]\n",
            "Epoch [5/15], batch: [54629/55810, loss: 7.4825]\n",
            "Epoch [5/15], batch: [54630/55810, loss: 5.4005]\n",
            "Epoch [5/15], batch: [54631/55810, loss: 6.3330]\n",
            "Epoch [5/15], batch: [54632/55810, loss: 5.3697]\n",
            "Epoch [5/15], batch: [54633/55810, loss: 3.0133]\n",
            "Epoch [5/15], batch: [54634/55810, loss: 6.6309]\n",
            "Epoch [5/15], batch: [54635/55810, loss: 7.0622]\n",
            "Epoch [5/15], batch: [54636/55810, loss: 6.9735]\n",
            "Epoch [5/15], batch: [54637/55810, loss: 6.1102]\n",
            "Epoch [5/15], batch: [54638/55810, loss: 5.8071]\n",
            "Epoch [5/15], batch: [54639/55810, loss: 6.4387]\n",
            "Epoch [5/15], batch: [54640/55810, loss: 6.7681]\n",
            "Epoch [5/15], batch: [54641/55810, loss: 7.4052]\n",
            "Epoch [5/15], batch: [54642/55810, loss: 7.1605]\n",
            "Epoch [5/15], batch: [54643/55810, loss: 7.0425]\n",
            "Epoch [5/15], batch: [54644/55810, loss: 7.5770]\n",
            "Epoch [5/15], batch: [54645/55810, loss: 4.3390]\n",
            "Epoch [5/15], batch: [54646/55810, loss: 5.3532]\n",
            "Epoch [5/15], batch: [54647/55810, loss: 7.3654]\n",
            "Epoch [5/15], batch: [54648/55810, loss: 5.6612]\n",
            "Epoch [5/15], batch: [54649/55810, loss: 5.7707]\n",
            "Epoch [5/15], batch: [54650/55810, loss: 7.7210]\n",
            "Epoch [5/15], batch: [54651/55810, loss: 5.6840]\n",
            "Epoch [5/15], batch: [54652/55810, loss: 4.7118]\n",
            "Epoch [5/15], batch: [54653/55810, loss: 5.9308]\n",
            "Epoch [5/15], batch: [54654/55810, loss: 5.1927]\n",
            "Epoch [5/15], batch: [54655/55810, loss: 5.1941]\n",
            "Epoch [5/15], batch: [54656/55810, loss: 6.0220]\n",
            "Epoch [5/15], batch: [54657/55810, loss: 5.0732]\n",
            "Epoch [5/15], batch: [54658/55810, loss: 4.1857]\n",
            "Epoch [5/15], batch: [54659/55810, loss: 5.1142]\n",
            "Epoch [5/15], batch: [54660/55810, loss: 7.2340]\n",
            "Epoch [5/15], batch: [54661/55810, loss: 4.6647]\n",
            "Epoch [5/15], batch: [54662/55810, loss: 6.8351]\n",
            "Epoch [5/15], batch: [54663/55810, loss: 6.8673]\n",
            "Epoch [5/15], batch: [54664/55810, loss: 7.2885]\n",
            "Epoch [5/15], batch: [54665/55810, loss: 4.2793]\n",
            "Epoch [5/15], batch: [54666/55810, loss: 4.0293]\n",
            "Epoch [5/15], batch: [54667/55810, loss: 5.1912]\n",
            "Epoch [5/15], batch: [54668/55810, loss: 3.8622]\n",
            "Epoch [5/15], batch: [54669/55810, loss: 5.7891]\n",
            "Epoch [5/15], batch: [54670/55810, loss: 6.4916]\n",
            "Epoch [5/15], batch: [54671/55810, loss: 6.8272]\n",
            "Epoch [5/15], batch: [54672/55810, loss: 5.4932]\n",
            "Epoch [5/15], batch: [54673/55810, loss: 4.8336]\n",
            "Epoch [5/15], batch: [54674/55810, loss: 5.9446]\n",
            "Epoch [5/15], batch: [54675/55810, loss: 4.8239]\n",
            "Epoch [5/15], batch: [54676/55810, loss: 5.8099]\n",
            "Epoch [5/15], batch: [54677/55810, loss: 4.7915]\n",
            "Epoch [5/15], batch: [54678/55810, loss: 5.2450]\n",
            "Epoch [5/15], batch: [54679/55810, loss: 5.5299]\n",
            "Epoch [5/15], batch: [54680/55810, loss: 7.6422]\n",
            "Epoch [5/15], batch: [54681/55810, loss: 6.8760]\n",
            "Epoch [5/15], batch: [54682/55810, loss: 6.4322]\n",
            "Epoch [5/15], batch: [54683/55810, loss: 6.4999]\n",
            "Epoch [5/15], batch: [54684/55810, loss: 7.0044]\n",
            "Epoch [5/15], batch: [54685/55810, loss: 6.2631]\n",
            "Epoch [5/15], batch: [54686/55810, loss: 5.2043]\n",
            "Epoch [5/15], batch: [54687/55810, loss: 6.2245]\n",
            "Epoch [5/15], batch: [54688/55810, loss: 6.4270]\n",
            "Epoch [5/15], batch: [54689/55810, loss: 5.6985]\n",
            "Epoch [5/15], batch: [54690/55810, loss: 5.2208]\n",
            "Epoch [5/15], batch: [54691/55810, loss: 4.9989]\n",
            "Epoch [5/15], batch: [54692/55810, loss: 6.9714]\n",
            "Epoch [5/15], batch: [54693/55810, loss: 7.0549]\n",
            "Epoch [5/15], batch: [54694/55810, loss: 4.6499]\n",
            "Epoch [5/15], batch: [54695/55810, loss: 5.9543]\n",
            "Epoch [5/15], batch: [54696/55810, loss: 7.0567]\n",
            "Epoch [5/15], batch: [54697/55810, loss: 7.0347]\n",
            "Epoch [5/15], batch: [54698/55810, loss: 5.8136]\n",
            "Epoch [5/15], batch: [54699/55810, loss: 6.8664]\n",
            "Epoch [5/15], batch: [54700/55810, loss: 6.2208]\n",
            "Epoch [5/15], batch: [54701/55810, loss: 5.7917]\n",
            "Epoch [5/15], batch: [54702/55810, loss: 3.6013]\n",
            "Epoch [5/15], batch: [54703/55810, loss: 6.8194]\n",
            "Epoch [5/15], batch: [54704/55810, loss: 6.9543]\n",
            "Epoch [5/15], batch: [54705/55810, loss: 7.5094]\n",
            "Epoch [5/15], batch: [54706/55810, loss: 6.2320]\n",
            "Epoch [5/15], batch: [54707/55810, loss: 6.4535]\n",
            "Epoch [5/15], batch: [54708/55810, loss: 7.1017]\n",
            "Epoch [5/15], batch: [54709/55810, loss: 6.2033]\n",
            "Epoch [5/15], batch: [54710/55810, loss: 5.7412]\n",
            "Epoch [5/15], batch: [54711/55810, loss: 7.0417]\n",
            "Epoch [5/15], batch: [54712/55810, loss: 7.6614]\n",
            "Epoch [5/15], batch: [54713/55810, loss: 4.4465]\n",
            "Epoch [5/15], batch: [54714/55810, loss: 5.6347]\n",
            "Epoch [5/15], batch: [54715/55810, loss: 5.6064]\n",
            "Epoch [5/15], batch: [54716/55810, loss: 7.6405]\n",
            "Epoch [5/15], batch: [54717/55810, loss: 6.1884]\n",
            "Epoch [5/15], batch: [54718/55810, loss: 6.6267]\n",
            "Epoch [5/15], batch: [54719/55810, loss: 6.1283]\n",
            "Epoch [5/15], batch: [54720/55810, loss: 6.8136]\n",
            "Epoch [5/15], batch: [54721/55810, loss: 7.8529]\n",
            "Epoch [5/15], batch: [54722/55810, loss: 5.7477]\n",
            "Epoch [5/15], batch: [54723/55810, loss: 6.5528]\n",
            "Epoch [5/15], batch: [54724/55810, loss: 5.4058]\n",
            "Epoch [5/15], batch: [54725/55810, loss: 5.1229]\n",
            "Epoch [5/15], batch: [54726/55810, loss: 5.4932]\n",
            "Epoch [5/15], batch: [54727/55810, loss: 5.0137]\n",
            "Epoch [5/15], batch: [54728/55810, loss: 3.8221]\n",
            "Epoch [5/15], batch: [54729/55810, loss: 5.6301]\n",
            "Epoch [5/15], batch: [54730/55810, loss: 5.8729]\n",
            "Epoch [5/15], batch: [54731/55810, loss: 6.8867]\n",
            "Epoch [5/15], batch: [54732/55810, loss: 6.8296]\n",
            "Epoch [5/15], batch: [54733/55810, loss: 4.7569]\n",
            "Epoch [5/15], batch: [54734/55810, loss: 5.5298]\n",
            "Epoch [5/15], batch: [54735/55810, loss: 4.9223]\n",
            "Epoch [5/15], batch: [54736/55810, loss: 6.5685]\n",
            "Epoch [5/15], batch: [54737/55810, loss: 5.5666]\n",
            "Epoch [5/15], batch: [54738/55810, loss: 6.5639]\n",
            "Epoch [5/15], batch: [54739/55810, loss: 6.5395]\n",
            "Epoch [5/15], batch: [54740/55810, loss: 6.6901]\n",
            "Epoch [5/15], batch: [54741/55810, loss: 5.8331]\n",
            "Epoch [5/15], batch: [54742/55810, loss: 4.6238]\n",
            "Epoch [5/15], batch: [54743/55810, loss: 6.9149]\n",
            "Epoch [5/15], batch: [54744/55810, loss: 7.4048]\n",
            "Epoch [5/15], batch: [54745/55810, loss: 6.2668]\n",
            "Epoch [5/15], batch: [54746/55810, loss: 5.4502]\n",
            "Epoch [5/15], batch: [54747/55810, loss: 4.6639]\n",
            "Epoch [5/15], batch: [54748/55810, loss: 6.7208]\n",
            "Epoch [5/15], batch: [54749/55810, loss: 7.1168]\n",
            "Epoch [5/15], batch: [54750/55810, loss: 6.7499]\n",
            "Epoch [5/15], batch: [54751/55810, loss: 6.2265]\n",
            "Epoch [5/15], batch: [54752/55810, loss: 6.5894]\n",
            "Epoch [5/15], batch: [54753/55810, loss: 5.7720]\n",
            "Epoch [5/15], batch: [54754/55810, loss: 4.7177]\n",
            "Epoch [5/15], batch: [54755/55810, loss: 5.9527]\n",
            "Epoch [5/15], batch: [54756/55810, loss: 8.1309]\n",
            "Epoch [5/15], batch: [54757/55810, loss: 6.6371]\n",
            "Epoch [5/15], batch: [54758/55810, loss: 6.8882]\n",
            "Epoch [5/15], batch: [54759/55810, loss: 6.8216]\n",
            "Epoch [5/15], batch: [54760/55810, loss: 5.4369]\n",
            "Epoch [5/15], batch: [54761/55810, loss: 6.8069]\n",
            "Epoch [5/15], batch: [54762/55810, loss: 6.0821]\n",
            "Epoch [5/15], batch: [54763/55810, loss: 6.4669]\n",
            "Epoch [5/15], batch: [54764/55810, loss: 5.3839]\n",
            "Epoch [5/15], batch: [54765/55810, loss: 6.4208]\n",
            "Epoch [5/15], batch: [54766/55810, loss: 6.8177]\n",
            "Epoch [5/15], batch: [54767/55810, loss: 5.2011]\n",
            "Epoch [5/15], batch: [54768/55810, loss: 8.3082]\n",
            "Epoch [5/15], batch: [54769/55810, loss: 7.5278]\n",
            "Epoch [5/15], batch: [54770/55810, loss: 5.4221]\n",
            "Epoch [5/15], batch: [54771/55810, loss: 7.0760]\n",
            "Epoch [5/15], batch: [54772/55810, loss: 8.2719]\n",
            "Epoch [5/15], batch: [54773/55810, loss: 6.8820]\n",
            "Epoch [5/15], batch: [54774/55810, loss: 6.6860]\n",
            "Epoch [5/15], batch: [54775/55810, loss: 5.8228]\n",
            "Epoch [5/15], batch: [54776/55810, loss: 6.9917]\n",
            "Epoch [5/15], batch: [54777/55810, loss: 7.0000]\n",
            "Epoch [5/15], batch: [54778/55810, loss: 6.2478]\n",
            "Epoch [5/15], batch: [54779/55810, loss: 5.2495]\n",
            "Epoch [5/15], batch: [54780/55810, loss: 6.2463]\n",
            "Epoch [5/15], batch: [54781/55810, loss: 5.6423]\n",
            "Epoch [5/15], batch: [54782/55810, loss: 7.1957]\n",
            "Epoch [5/15], batch: [54783/55810, loss: 6.5033]\n",
            "Epoch [5/15], batch: [54784/55810, loss: 5.7062]\n",
            "Epoch [5/15], batch: [54785/55810, loss: 5.6237]\n",
            "Epoch [5/15], batch: [54786/55810, loss: 6.9388]\n",
            "Epoch [5/15], batch: [54787/55810, loss: 5.6110]\n",
            "Epoch [5/15], batch: [54788/55810, loss: 6.4157]\n",
            "Epoch [5/15], batch: [54789/55810, loss: 8.7123]\n",
            "Epoch [5/15], batch: [54790/55810, loss: 5.3318]\n",
            "Epoch [5/15], batch: [54791/55810, loss: 5.1376]\n",
            "Epoch [5/15], batch: [54792/55810, loss: 6.6933]\n",
            "Epoch [5/15], batch: [54793/55810, loss: 6.9881]\n",
            "Epoch [5/15], batch: [54794/55810, loss: 8.0884]\n",
            "Epoch [5/15], batch: [54795/55810, loss: 7.2042]\n",
            "Epoch [5/15], batch: [54796/55810, loss: 7.1352]\n",
            "Epoch [5/15], batch: [54797/55810, loss: 6.4395]\n",
            "Epoch [5/15], batch: [54798/55810, loss: 7.1071]\n",
            "Epoch [5/15], batch: [54799/55810, loss: 7.5347]\n",
            "Epoch [5/15], batch: [54800/55810, loss: 4.9258]\n",
            "Epoch [5/15], batch: [54801/55810, loss: 5.9620]\n",
            "Epoch [5/15], batch: [54802/55810, loss: 6.4552]\n",
            "Epoch [5/15], batch: [54803/55810, loss: 5.3107]\n",
            "Epoch [5/15], batch: [54804/55810, loss: 5.5170]\n",
            "Epoch [5/15], batch: [54805/55810, loss: 5.7927]\n",
            "Epoch [5/15], batch: [54806/55810, loss: 6.4451]\n",
            "Epoch [5/15], batch: [54807/55810, loss: 6.5290]\n",
            "Epoch [5/15], batch: [54808/55810, loss: 7.8518]\n",
            "Epoch [5/15], batch: [54809/55810, loss: 7.4558]\n",
            "Epoch [5/15], batch: [54810/55810, loss: 5.6584]\n",
            "Epoch [5/15], batch: [54811/55810, loss: 5.2861]\n",
            "Epoch [5/15], batch: [54812/55810, loss: 6.3250]\n",
            "Epoch [5/15], batch: [54813/55810, loss: 6.7052]\n",
            "Epoch [5/15], batch: [54814/55810, loss: 6.8009]\n",
            "Epoch [5/15], batch: [54815/55810, loss: 4.1468]\n",
            "Epoch [5/15], batch: [54816/55810, loss: 3.8219]\n",
            "Epoch [5/15], batch: [54817/55810, loss: 4.2633]\n",
            "Epoch [5/15], batch: [54818/55810, loss: 5.4078]\n",
            "Epoch [5/15], batch: [54819/55810, loss: 6.4669]\n",
            "Epoch [5/15], batch: [54820/55810, loss: 4.4651]\n",
            "Epoch [5/15], batch: [54821/55810, loss: 5.5162]\n",
            "Epoch [5/15], batch: [54822/55810, loss: 5.6600]\n",
            "Epoch [5/15], batch: [54823/55810, loss: 5.7653]\n",
            "Epoch [5/15], batch: [54824/55810, loss: 6.3681]\n",
            "Epoch [5/15], batch: [54825/55810, loss: 7.6477]\n",
            "Epoch [5/15], batch: [54826/55810, loss: 5.6373]\n",
            "Epoch [5/15], batch: [54827/55810, loss: 6.6675]\n",
            "Epoch [5/15], batch: [54828/55810, loss: 7.5822]\n",
            "Epoch [5/15], batch: [54829/55810, loss: 8.6060]\n",
            "Epoch [5/15], batch: [54830/55810, loss: 6.4230]\n",
            "Epoch [5/15], batch: [54831/55810, loss: 6.6474]\n",
            "Epoch [5/15], batch: [54832/55810, loss: 6.5635]\n",
            "Epoch [5/15], batch: [54833/55810, loss: 6.4730]\n",
            "Epoch [5/15], batch: [54834/55810, loss: 6.1344]\n",
            "Epoch [5/15], batch: [54835/55810, loss: 6.3905]\n",
            "Epoch [5/15], batch: [54836/55810, loss: 5.9164]\n",
            "Epoch [5/15], batch: [54837/55810, loss: 4.9445]\n",
            "Epoch [5/15], batch: [54838/55810, loss: 5.6335]\n",
            "Epoch [5/15], batch: [54839/55810, loss: 5.4269]\n",
            "Epoch [5/15], batch: [54840/55810, loss: 4.5744]\n",
            "Epoch [5/15], batch: [54841/55810, loss: 6.1712]\n",
            "Epoch [5/15], batch: [54842/55810, loss: 4.8499]\n",
            "Epoch [5/15], batch: [54843/55810, loss: 3.8065]\n",
            "Epoch [5/15], batch: [54844/55810, loss: 6.6416]\n",
            "Epoch [5/15], batch: [54845/55810, loss: 6.3259]\n",
            "Epoch [5/15], batch: [54846/55810, loss: 5.9600]\n",
            "Epoch [5/15], batch: [54847/55810, loss: 6.8187]\n",
            "Epoch [5/15], batch: [54848/55810, loss: 5.0484]\n",
            "Epoch [5/15], batch: [54849/55810, loss: 8.1211]\n",
            "Epoch [5/15], batch: [54850/55810, loss: 6.3487]\n",
            "Epoch [5/15], batch: [54851/55810, loss: 7.8748]\n",
            "Epoch [5/15], batch: [54852/55810, loss: 6.1361]\n",
            "Epoch [5/15], batch: [54853/55810, loss: 7.6936]\n",
            "Epoch [5/15], batch: [54854/55810, loss: 5.5149]\n",
            "Epoch [5/15], batch: [54855/55810, loss: 7.5207]\n",
            "Epoch [5/15], batch: [54856/55810, loss: 6.1596]\n",
            "Epoch [5/15], batch: [54857/55810, loss: 5.8318]\n",
            "Epoch [5/15], batch: [54858/55810, loss: 5.9773]\n",
            "Epoch [5/15], batch: [54859/55810, loss: 7.2803]\n",
            "Epoch [5/15], batch: [54860/55810, loss: 6.4133]\n",
            "Epoch [5/15], batch: [54861/55810, loss: 5.2781]\n",
            "Epoch [5/15], batch: [54862/55810, loss: 7.0700]\n",
            "Epoch [5/15], batch: [54863/55810, loss: 8.5518]\n",
            "Epoch [5/15], batch: [54864/55810, loss: 7.1001]\n",
            "Epoch [5/15], batch: [54865/55810, loss: 6.3501]\n",
            "Epoch [5/15], batch: [54866/55810, loss: 8.3764]\n",
            "Epoch [5/15], batch: [54867/55810, loss: 6.8053]\n",
            "Epoch [5/15], batch: [54868/55810, loss: 6.0796]\n",
            "Epoch [5/15], batch: [54869/55810, loss: 3.7564]\n",
            "Epoch [5/15], batch: [54870/55810, loss: 6.2320]\n",
            "Epoch [5/15], batch: [54871/55810, loss: 7.4768]\n",
            "Epoch [5/15], batch: [54872/55810, loss: 7.4645]\n",
            "Epoch [5/15], batch: [54873/55810, loss: 5.0623]\n",
            "Epoch [5/15], batch: [54874/55810, loss: 6.2580]\n",
            "Epoch [5/15], batch: [54875/55810, loss: 6.6029]\n",
            "Epoch [5/15], batch: [54876/55810, loss: 6.4921]\n",
            "Epoch [5/15], batch: [54877/55810, loss: 5.9543]\n",
            "Epoch [5/15], batch: [54878/55810, loss: 6.8661]\n",
            "Epoch [5/15], batch: [54879/55810, loss: 7.5259]\n",
            "Epoch [5/15], batch: [54880/55810, loss: 7.4077]\n",
            "Epoch [5/15], batch: [54881/55810, loss: 7.4469]\n",
            "Epoch [5/15], batch: [54882/55810, loss: 6.5868]\n",
            "Epoch [5/15], batch: [54883/55810, loss: 6.4216]\n",
            "Epoch [5/15], batch: [54884/55810, loss: 8.0915]\n",
            "Epoch [5/15], batch: [54885/55810, loss: 7.4206]\n",
            "Epoch [5/15], batch: [54886/55810, loss: 6.3568]\n",
            "Epoch [5/15], batch: [54887/55810, loss: 5.8953]\n",
            "Epoch [5/15], batch: [54888/55810, loss: 7.0950]\n",
            "Epoch [5/15], batch: [54889/55810, loss: 5.9651]\n",
            "Epoch [5/15], batch: [54890/55810, loss: 5.9234]\n",
            "Epoch [5/15], batch: [54891/55810, loss: 7.8311]\n",
            "Epoch [5/15], batch: [54892/55810, loss: 7.2934]\n",
            "Epoch [5/15], batch: [54893/55810, loss: 7.3400]\n",
            "Epoch [5/15], batch: [54894/55810, loss: 7.8615]\n",
            "Epoch [5/15], batch: [54895/55810, loss: 6.3081]\n",
            "Epoch [5/15], batch: [54896/55810, loss: 6.0064]\n",
            "Epoch [5/15], batch: [54897/55810, loss: 4.3044]\n",
            "Epoch [5/15], batch: [54898/55810, loss: 5.6499]\n",
            "Epoch [5/15], batch: [54899/55810, loss: 6.7048]\n",
            "Epoch [5/15], batch: [54900/55810, loss: 6.5810]\n",
            "Epoch [5/15], batch: [54901/55810, loss: 6.5142]\n",
            "Epoch [5/15], batch: [54902/55810, loss: 7.7878]\n",
            "Epoch [5/15], batch: [54903/55810, loss: 8.3418]\n",
            "Epoch [5/15], batch: [54904/55810, loss: 5.5466]\n",
            "Epoch [5/15], batch: [54905/55810, loss: 5.6553]\n",
            "Epoch [5/15], batch: [54906/55810, loss: 6.0848]\n",
            "Epoch [5/15], batch: [54907/55810, loss: 4.8826]\n",
            "Epoch [5/15], batch: [54908/55810, loss: 5.8083]\n",
            "Epoch [5/15], batch: [54909/55810, loss: 5.4197]\n",
            "Epoch [5/15], batch: [54910/55810, loss: 6.1637]\n",
            "Epoch [5/15], batch: [54911/55810, loss: 5.1674]\n",
            "Epoch [5/15], batch: [54912/55810, loss: 6.4139]\n",
            "Epoch [5/15], batch: [54913/55810, loss: 4.4727]\n",
            "Epoch [5/15], batch: [54914/55810, loss: 6.6926]\n",
            "Epoch [5/15], batch: [54915/55810, loss: 6.0586]\n",
            "Epoch [5/15], batch: [54916/55810, loss: 7.0058]\n",
            "Epoch [5/15], batch: [54917/55810, loss: 5.3153]\n",
            "Epoch [5/15], batch: [54918/55810, loss: 5.0023]\n",
            "Epoch [5/15], batch: [54919/55810, loss: 5.5520]\n",
            "Epoch [5/15], batch: [54920/55810, loss: 7.5476]\n",
            "Epoch [5/15], batch: [54921/55810, loss: 5.2933]\n",
            "Epoch [5/15], batch: [54922/55810, loss: 5.8586]\n",
            "Epoch [5/15], batch: [54923/55810, loss: 7.1128]\n",
            "Epoch [5/15], batch: [54924/55810, loss: 6.3551]\n",
            "Epoch [5/15], batch: [54925/55810, loss: 6.1106]\n",
            "Epoch [5/15], batch: [54926/55810, loss: 7.3761]\n",
            "Epoch [5/15], batch: [54927/55810, loss: 8.2554]\n",
            "Epoch [5/15], batch: [54928/55810, loss: 6.3071]\n",
            "Epoch [5/15], batch: [54929/55810, loss: 6.6155]\n",
            "Epoch [5/15], batch: [54930/55810, loss: 5.9564]\n",
            "Epoch [5/15], batch: [54931/55810, loss: 8.3357]\n",
            "Epoch [5/15], batch: [54932/55810, loss: 5.2231]\n",
            "Epoch [5/15], batch: [54933/55810, loss: 6.3830]\n",
            "Epoch [5/15], batch: [54934/55810, loss: 6.0214]\n",
            "Epoch [5/15], batch: [54935/55810, loss: 6.5992]\n",
            "Epoch [5/15], batch: [54936/55810, loss: 7.9133]\n",
            "Epoch [5/15], batch: [54937/55810, loss: 5.4406]\n",
            "Epoch [5/15], batch: [54938/55810, loss: 7.4859]\n",
            "Epoch [5/15], batch: [54939/55810, loss: 5.6529]\n",
            "Epoch [5/15], batch: [54940/55810, loss: 7.5036]\n",
            "Epoch [5/15], batch: [54941/55810, loss: 8.0668]\n",
            "Epoch [5/15], batch: [54942/55810, loss: 6.7167]\n",
            "Epoch [5/15], batch: [54943/55810, loss: 8.5173]\n",
            "Epoch [5/15], batch: [54944/55810, loss: 8.0035]\n",
            "Epoch [5/15], batch: [54945/55810, loss: 7.6296]\n",
            "Epoch [5/15], batch: [54946/55810, loss: 7.6369]\n",
            "Epoch [5/15], batch: [54947/55810, loss: 8.3356]\n",
            "Epoch [5/15], batch: [54948/55810, loss: 6.6696]\n",
            "Epoch [5/15], batch: [54949/55810, loss: 6.6674]\n",
            "Epoch [5/15], batch: [54950/55810, loss: 8.7694]\n",
            "Epoch [5/15], batch: [54951/55810, loss: 5.6481]\n",
            "Epoch [5/15], batch: [54952/55810, loss: 5.9100]\n",
            "Epoch [5/15], batch: [54953/55810, loss: 7.1020]\n",
            "Epoch [5/15], batch: [54954/55810, loss: 7.7291]\n",
            "Epoch [5/15], batch: [54955/55810, loss: 7.1635]\n",
            "Epoch [5/15], batch: [54956/55810, loss: 7.6483]\n",
            "Epoch [5/15], batch: [54957/55810, loss: 7.5352]\n",
            "Epoch [5/15], batch: [54958/55810, loss: 7.4252]\n",
            "Epoch [5/15], batch: [54959/55810, loss: 8.3649]\n",
            "Epoch [5/15], batch: [54960/55810, loss: 5.1237]\n",
            "Epoch [5/15], batch: [54961/55810, loss: 6.1774]\n",
            "Epoch [5/15], batch: [54962/55810, loss: 5.6677]\n",
            "Epoch [5/15], batch: [54963/55810, loss: 6.3149]\n",
            "Epoch [5/15], batch: [54964/55810, loss: 5.3089]\n",
            "Epoch [5/15], batch: [54965/55810, loss: 3.7895]\n",
            "Epoch [5/15], batch: [54966/55810, loss: 6.8646]\n",
            "Epoch [5/15], batch: [54967/55810, loss: 6.7163]\n",
            "Epoch [5/15], batch: [54968/55810, loss: 4.8315]\n",
            "Epoch [5/15], batch: [54969/55810, loss: 7.2987]\n",
            "Epoch [5/15], batch: [54970/55810, loss: 4.5234]\n",
            "Epoch [5/15], batch: [54971/55810, loss: 4.9794]\n",
            "Epoch [5/15], batch: [54972/55810, loss: 7.6436]\n",
            "Epoch [5/15], batch: [54973/55810, loss: 5.7917]\n",
            "Epoch [5/15], batch: [54974/55810, loss: 5.7293]\n",
            "Epoch [5/15], batch: [54975/55810, loss: 4.5589]\n",
            "Epoch [5/15], batch: [54976/55810, loss: 6.1484]\n",
            "Epoch [5/15], batch: [54977/55810, loss: 5.9220]\n",
            "Epoch [5/15], batch: [54978/55810, loss: 5.2240]\n",
            "Epoch [5/15], batch: [54979/55810, loss: 5.2389]\n",
            "Epoch [5/15], batch: [54980/55810, loss: 5.5084]\n",
            "Epoch [5/15], batch: [54981/55810, loss: 7.1562]\n",
            "Epoch [5/15], batch: [54982/55810, loss: 6.4479]\n",
            "Epoch [5/15], batch: [54983/55810, loss: 6.5694]\n",
            "Epoch [5/15], batch: [54984/55810, loss: 6.0463]\n",
            "Epoch [5/15], batch: [54985/55810, loss: 5.5056]\n",
            "Epoch [5/15], batch: [54986/55810, loss: 7.2184]\n",
            "Epoch [5/15], batch: [54987/55810, loss: 3.8091]\n",
            "Epoch [5/15], batch: [54988/55810, loss: 5.6461]\n",
            "Epoch [5/15], batch: [54989/55810, loss: 6.2955]\n",
            "Epoch [5/15], batch: [54990/55810, loss: 5.6760]\n",
            "Epoch [5/15], batch: [54991/55810, loss: 5.9974]\n",
            "Epoch [5/15], batch: [54992/55810, loss: 7.4432]\n",
            "Epoch [5/15], batch: [54993/55810, loss: 6.0953]\n",
            "Epoch [5/15], batch: [54994/55810, loss: 8.0076]\n",
            "Epoch [5/15], batch: [54995/55810, loss: 6.1374]\n",
            "Epoch [5/15], batch: [54996/55810, loss: 6.2024]\n",
            "Epoch [5/15], batch: [54997/55810, loss: 5.9578]\n",
            "Epoch [5/15], batch: [54998/55810, loss: 7.4828]\n",
            "Epoch [5/15], batch: [54999/55810, loss: 7.5138]\n",
            "Epoch [5/15], batch: [55000/55810, loss: 5.5109]\n",
            "Epoch [5/15], batch: [55001/55810, loss: 5.4781]\n",
            "Epoch [5/15], batch: [55002/55810, loss: 5.9890]\n",
            "Epoch [5/15], batch: [55003/55810, loss: 7.9308]\n",
            "Epoch [5/15], batch: [55004/55810, loss: 5.8807]\n",
            "Epoch [5/15], batch: [55005/55810, loss: 5.9964]\n",
            "Epoch [5/15], batch: [55006/55810, loss: 7.0824]\n",
            "Epoch [5/15], batch: [55007/55810, loss: 5.3397]\n",
            "Epoch [5/15], batch: [55008/55810, loss: 5.9991]\n",
            "Epoch [5/15], batch: [55009/55810, loss: 7.9951]\n",
            "Epoch [5/15], batch: [55010/55810, loss: 6.8981]\n",
            "Epoch [5/15], batch: [55011/55810, loss: 6.4684]\n",
            "Epoch [5/15], batch: [55012/55810, loss: 7.1537]\n",
            "Epoch [5/15], batch: [55013/55810, loss: 7.7336]\n",
            "Epoch [5/15], batch: [55014/55810, loss: 6.9835]\n",
            "Epoch [5/15], batch: [55015/55810, loss: 6.8575]\n",
            "Epoch [5/15], batch: [55016/55810, loss: 6.1674]\n",
            "Epoch [5/15], batch: [55017/55810, loss: 5.7258]\n",
            "Epoch [5/15], batch: [55018/55810, loss: 7.1206]\n",
            "Epoch [5/15], batch: [55019/55810, loss: 8.0234]\n",
            "Epoch [5/15], batch: [55020/55810, loss: 7.0731]\n",
            "Epoch [5/15], batch: [55021/55810, loss: 5.5566]\n",
            "Epoch [5/15], batch: [55022/55810, loss: 5.6501]\n",
            "Epoch [5/15], batch: [55023/55810, loss: 5.6749]\n",
            "Epoch [5/15], batch: [55024/55810, loss: 8.5404]\n",
            "Epoch [5/15], batch: [55025/55810, loss: 6.4305]\n",
            "Epoch [5/15], batch: [55026/55810, loss: 6.9927]\n",
            "Epoch [5/15], batch: [55027/55810, loss: 6.4395]\n",
            "Epoch [5/15], batch: [55028/55810, loss: 5.9122]\n",
            "Epoch [5/15], batch: [55029/55810, loss: 7.2668]\n",
            "Epoch [5/15], batch: [55030/55810, loss: 5.1181]\n",
            "Epoch [5/15], batch: [55031/55810, loss: 5.6091]\n",
            "Epoch [5/15], batch: [55032/55810, loss: 5.8677]\n",
            "Epoch [5/15], batch: [55033/55810, loss: 6.1756]\n",
            "Epoch [5/15], batch: [55034/55810, loss: 7.2096]\n",
            "Epoch [5/15], batch: [55035/55810, loss: 6.2016]\n",
            "Epoch [5/15], batch: [55036/55810, loss: 8.3246]\n",
            "Epoch [5/15], batch: [55037/55810, loss: 8.5300]\n",
            "Epoch [5/15], batch: [55038/55810, loss: 5.1719]\n",
            "Epoch [5/15], batch: [55039/55810, loss: 6.5211]\n",
            "Epoch [5/15], batch: [55040/55810, loss: 6.8613]\n",
            "Epoch [5/15], batch: [55041/55810, loss: 5.0357]\n",
            "Epoch [5/15], batch: [55042/55810, loss: 5.1788]\n",
            "Epoch [5/15], batch: [55043/55810, loss: 6.8123]\n",
            "Epoch [5/15], batch: [55044/55810, loss: 7.0362]\n",
            "Epoch [5/15], batch: [55045/55810, loss: 7.0513]\n",
            "Epoch [5/15], batch: [55046/55810, loss: 7.1305]\n",
            "Epoch [5/15], batch: [55047/55810, loss: 6.5237]\n",
            "Epoch [5/15], batch: [55048/55810, loss: 8.2219]\n",
            "Epoch [5/15], batch: [55049/55810, loss: 7.5103]\n",
            "Epoch [5/15], batch: [55050/55810, loss: 6.9530]\n",
            "Epoch [5/15], batch: [55051/55810, loss: 4.4140]\n",
            "Epoch [5/15], batch: [55052/55810, loss: 5.9175]\n",
            "Epoch [5/15], batch: [55053/55810, loss: 4.9783]\n",
            "Epoch [5/15], batch: [55054/55810, loss: 6.4701]\n",
            "Epoch [5/15], batch: [55055/55810, loss: 7.5481]\n",
            "Epoch [5/15], batch: [55056/55810, loss: 7.0137]\n",
            "Epoch [5/15], batch: [55057/55810, loss: 6.5200]\n",
            "Epoch [5/15], batch: [55058/55810, loss: 6.3828]\n",
            "Epoch [5/15], batch: [55059/55810, loss: 5.0934]\n",
            "Epoch [5/15], batch: [55060/55810, loss: 4.8235]\n",
            "Epoch [5/15], batch: [55061/55810, loss: 7.0414]\n",
            "Epoch [5/15], batch: [55062/55810, loss: 6.3867]\n",
            "Epoch [5/15], batch: [55063/55810, loss: 6.3308]\n",
            "Epoch [5/15], batch: [55064/55810, loss: 6.5383]\n",
            "Epoch [5/15], batch: [55065/55810, loss: 8.4954]\n",
            "Epoch [5/15], batch: [55066/55810, loss: 7.2930]\n",
            "Epoch [5/15], batch: [55067/55810, loss: 6.0512]\n",
            "Epoch [5/15], batch: [55068/55810, loss: 7.3111]\n",
            "Epoch [5/15], batch: [55069/55810, loss: 5.5925]\n",
            "Epoch [5/15], batch: [55070/55810, loss: 6.5234]\n",
            "Epoch [5/15], batch: [55071/55810, loss: 5.5824]\n",
            "Epoch [5/15], batch: [55072/55810, loss: 6.4348]\n",
            "Epoch [5/15], batch: [55073/55810, loss: 5.8779]\n",
            "Epoch [5/15], batch: [55074/55810, loss: 8.7091]\n",
            "Epoch [5/15], batch: [55075/55810, loss: 8.4483]\n",
            "Epoch [5/15], batch: [55076/55810, loss: 7.4458]\n",
            "Epoch [5/15], batch: [55077/55810, loss: 7.1071]\n",
            "Epoch [5/15], batch: [55078/55810, loss: 6.2836]\n",
            "Epoch [5/15], batch: [55079/55810, loss: 5.7637]\n",
            "Epoch [5/15], batch: [55080/55810, loss: 6.8996]\n",
            "Epoch [5/15], batch: [55081/55810, loss: 7.8508]\n",
            "Epoch [5/15], batch: [55082/55810, loss: 7.8741]\n",
            "Epoch [5/15], batch: [55083/55810, loss: 6.7447]\n",
            "Epoch [5/15], batch: [55084/55810, loss: 4.7700]\n",
            "Epoch [5/15], batch: [55085/55810, loss: 6.1192]\n",
            "Epoch [5/15], batch: [55086/55810, loss: 7.7238]\n",
            "Epoch [5/15], batch: [55087/55810, loss: 7.0184]\n",
            "Epoch [5/15], batch: [55088/55810, loss: 6.5002]\n",
            "Epoch [5/15], batch: [55089/55810, loss: 7.0593]\n",
            "Epoch [5/15], batch: [55090/55810, loss: 7.1629]\n",
            "Epoch [5/15], batch: [55091/55810, loss: 6.6326]\n",
            "Epoch [5/15], batch: [55092/55810, loss: 7.5111]\n",
            "Epoch [5/15], batch: [55093/55810, loss: 7.0253]\n",
            "Epoch [5/15], batch: [55094/55810, loss: 6.9012]\n",
            "Epoch [5/15], batch: [55095/55810, loss: 7.9060]\n",
            "Epoch [5/15], batch: [55096/55810, loss: 6.8462]\n",
            "Epoch [5/15], batch: [55097/55810, loss: 6.9661]\n",
            "Epoch [5/15], batch: [55098/55810, loss: 6.3560]\n",
            "Epoch [5/15], batch: [55099/55810, loss: 7.8301]\n",
            "Epoch [5/15], batch: [55100/55810, loss: 8.0418]\n",
            "Epoch [5/15], batch: [55101/55810, loss: 7.1744]\n",
            "Epoch [5/15], batch: [55102/55810, loss: 6.3123]\n",
            "Epoch [5/15], batch: [55103/55810, loss: 8.2623]\n",
            "Epoch [5/15], batch: [55104/55810, loss: 6.4615]\n",
            "Epoch [5/15], batch: [55105/55810, loss: 7.9112]\n",
            "Epoch [5/15], batch: [55106/55810, loss: 6.4765]\n",
            "Epoch [5/15], batch: [55107/55810, loss: 7.4359]\n",
            "Epoch [5/15], batch: [55108/55810, loss: 8.7281]\n",
            "Epoch [5/15], batch: [55109/55810, loss: 6.3990]\n",
            "Epoch [5/15], batch: [55110/55810, loss: 6.9546]\n",
            "Epoch [5/15], batch: [55111/55810, loss: 7.3001]\n",
            "Epoch [5/15], batch: [55112/55810, loss: 7.8571]\n",
            "Epoch [5/15], batch: [55113/55810, loss: 6.8641]\n",
            "Epoch [5/15], batch: [55114/55810, loss: 7.0268]\n",
            "Epoch [5/15], batch: [55115/55810, loss: 5.8778]\n",
            "Epoch [5/15], batch: [55116/55810, loss: 5.9655]\n",
            "Epoch [5/15], batch: [55117/55810, loss: 7.0113]\n",
            "Epoch [5/15], batch: [55118/55810, loss: 5.9848]\n",
            "Epoch [5/15], batch: [55119/55810, loss: 5.1656]\n",
            "Epoch [5/15], batch: [55120/55810, loss: 7.4429]\n",
            "Epoch [5/15], batch: [55121/55810, loss: 7.6098]\n",
            "Epoch [5/15], batch: [55122/55810, loss: 6.3247]\n",
            "Epoch [5/15], batch: [55123/55810, loss: 7.8459]\n",
            "Epoch [5/15], batch: [55124/55810, loss: 7.3938]\n",
            "Epoch [5/15], batch: [55125/55810, loss: 6.5716]\n",
            "Epoch [5/15], batch: [55126/55810, loss: 8.1177]\n",
            "Epoch [5/15], batch: [55127/55810, loss: 7.9804]\n",
            "Epoch [5/15], batch: [55128/55810, loss: 8.5810]\n",
            "Epoch [5/15], batch: [55129/55810, loss: 6.9610]\n",
            "Epoch [5/15], batch: [55130/55810, loss: 7.4761]\n",
            "Epoch [5/15], batch: [55131/55810, loss: 6.4965]\n",
            "Epoch [5/15], batch: [55132/55810, loss: 8.8401]\n",
            "Epoch [5/15], batch: [55133/55810, loss: 7.6517]\n",
            "Epoch [5/15], batch: [55134/55810, loss: 7.5992]\n",
            "Epoch [5/15], batch: [55135/55810, loss: 7.7200]\n",
            "Epoch [5/15], batch: [55136/55810, loss: 4.6959]\n",
            "Epoch [5/15], batch: [55137/55810, loss: 6.7657]\n",
            "Epoch [5/15], batch: [55138/55810, loss: 6.6803]\n",
            "Epoch [5/15], batch: [55139/55810, loss: 7.1463]\n",
            "Epoch [5/15], batch: [55140/55810, loss: 6.0908]\n",
            "Epoch [5/15], batch: [55141/55810, loss: 7.9774]\n",
            "Epoch [5/15], batch: [55142/55810, loss: 7.8593]\n",
            "Epoch [5/15], batch: [55143/55810, loss: 6.2700]\n",
            "Epoch [5/15], batch: [55144/55810, loss: 7.3987]\n",
            "Epoch [5/15], batch: [55145/55810, loss: 6.6763]\n",
            "Epoch [5/15], batch: [55146/55810, loss: 5.0700]\n",
            "Epoch [5/15], batch: [55147/55810, loss: 6.4262]\n",
            "Epoch [5/15], batch: [55148/55810, loss: 6.5561]\n",
            "Epoch [5/15], batch: [55149/55810, loss: 6.1563]\n",
            "Epoch [5/15], batch: [55150/55810, loss: 7.3352]\n",
            "Epoch [5/15], batch: [55151/55810, loss: 3.9602]\n",
            "Epoch [5/15], batch: [55152/55810, loss: 6.2043]\n",
            "Epoch [5/15], batch: [55153/55810, loss: 5.9180]\n",
            "Epoch [5/15], batch: [55154/55810, loss: 7.4923]\n",
            "Epoch [5/15], batch: [55155/55810, loss: 6.8374]\n",
            "Epoch [5/15], batch: [55156/55810, loss: 6.6356]\n",
            "Epoch [5/15], batch: [55157/55810, loss: 7.0906]\n",
            "Epoch [5/15], batch: [55158/55810, loss: 6.0254]\n",
            "Epoch [5/15], batch: [55159/55810, loss: 6.4014]\n",
            "Epoch [5/15], batch: [55160/55810, loss: 6.3640]\n",
            "Epoch [5/15], batch: [55161/55810, loss: 6.3466]\n",
            "Epoch [5/15], batch: [55162/55810, loss: 6.9169]\n",
            "Epoch [5/15], batch: [55163/55810, loss: 5.4472]\n",
            "Epoch [5/15], batch: [55164/55810, loss: 8.7171]\n",
            "Epoch [5/15], batch: [55165/55810, loss: 6.8956]\n",
            "Epoch [5/15], batch: [55166/55810, loss: 5.6138]\n",
            "Epoch [5/15], batch: [55167/55810, loss: 6.1162]\n",
            "Epoch [5/15], batch: [55168/55810, loss: 7.7801]\n",
            "Epoch [5/15], batch: [55169/55810, loss: 7.2087]\n",
            "Epoch [5/15], batch: [55170/55810, loss: 7.3733]\n",
            "Epoch [5/15], batch: [55171/55810, loss: 7.6183]\n",
            "Epoch [5/15], batch: [55172/55810, loss: 5.8252]\n",
            "Epoch [5/15], batch: [55173/55810, loss: 6.4702]\n",
            "Epoch [5/15], batch: [55174/55810, loss: 7.3757]\n",
            "Epoch [5/15], batch: [55175/55810, loss: 6.8845]\n",
            "Epoch [5/15], batch: [55176/55810, loss: 6.4848]\n",
            "Epoch [5/15], batch: [55177/55810, loss: 6.9008]\n",
            "Epoch [5/15], batch: [55178/55810, loss: 7.2837]\n",
            "Epoch [5/15], batch: [55179/55810, loss: 6.8259]\n",
            "Epoch [5/15], batch: [55180/55810, loss: 4.3558]\n",
            "Epoch [5/15], batch: [55181/55810, loss: 7.2977]\n",
            "Epoch [5/15], batch: [55182/55810, loss: 5.8611]\n",
            "Epoch [5/15], batch: [55183/55810, loss: 6.2026]\n",
            "Epoch [5/15], batch: [55184/55810, loss: 6.9283]\n",
            "Epoch [5/15], batch: [55185/55810, loss: 6.9193]\n",
            "Epoch [5/15], batch: [55186/55810, loss: 6.4651]\n",
            "Epoch [5/15], batch: [55187/55810, loss: 6.7898]\n",
            "Epoch [5/15], batch: [55188/55810, loss: 5.8730]\n",
            "Epoch [5/15], batch: [55189/55810, loss: 6.8220]\n",
            "Epoch [5/15], batch: [55190/55810, loss: 7.3431]\n",
            "Epoch [5/15], batch: [55191/55810, loss: 6.1654]\n",
            "Epoch [5/15], batch: [55192/55810, loss: 6.9558]\n",
            "Epoch [5/15], batch: [55193/55810, loss: 6.8551]\n",
            "Epoch [5/15], batch: [55194/55810, loss: 5.8916]\n",
            "Epoch [5/15], batch: [55195/55810, loss: 6.6173]\n",
            "Epoch [5/15], batch: [55196/55810, loss: 6.1628]\n",
            "Epoch [5/15], batch: [55197/55810, loss: 7.2280]\n",
            "Epoch [5/15], batch: [55198/55810, loss: 7.4011]\n",
            "Epoch [5/15], batch: [55199/55810, loss: 6.9670]\n",
            "Epoch [5/15], batch: [55200/55810, loss: 7.2133]\n",
            "Epoch [5/15], batch: [55201/55810, loss: 5.3576]\n",
            "Epoch [5/15], batch: [55202/55810, loss: 5.0134]\n",
            "Epoch [5/15], batch: [55203/55810, loss: 6.4166]\n",
            "Epoch [5/15], batch: [55204/55810, loss: 6.7432]\n",
            "Epoch [5/15], batch: [55205/55810, loss: 7.6969]\n",
            "Epoch [5/15], batch: [55206/55810, loss: 5.9025]\n",
            "Epoch [5/15], batch: [55207/55810, loss: 6.1577]\n",
            "Epoch [5/15], batch: [55208/55810, loss: 6.3831]\n",
            "Epoch [5/15], batch: [55209/55810, loss: 5.3551]\n",
            "Epoch [5/15], batch: [55210/55810, loss: 5.6277]\n",
            "Epoch [5/15], batch: [55211/55810, loss: 8.1769]\n",
            "Epoch [5/15], batch: [55212/55810, loss: 7.2789]\n",
            "Epoch [5/15], batch: [55213/55810, loss: 6.4170]\n",
            "Epoch [5/15], batch: [55214/55810, loss: 8.2159]\n",
            "Epoch [5/15], batch: [55215/55810, loss: 7.0154]\n",
            "Epoch [5/15], batch: [55216/55810, loss: 7.3917]\n",
            "Epoch [5/15], batch: [55217/55810, loss: 6.9653]\n",
            "Epoch [5/15], batch: [55218/55810, loss: 5.1262]\n",
            "Epoch [5/15], batch: [55219/55810, loss: 6.0541]\n",
            "Epoch [5/15], batch: [55220/55810, loss: 5.4252]\n",
            "Epoch [5/15], batch: [55221/55810, loss: 5.6515]\n",
            "Epoch [5/15], batch: [55222/55810, loss: 7.2332]\n",
            "Epoch [5/15], batch: [55223/55810, loss: 5.8134]\n",
            "Epoch [5/15], batch: [55224/55810, loss: 3.9781]\n",
            "Epoch [5/15], batch: [55225/55810, loss: 7.1748]\n",
            "Epoch [5/15], batch: [55226/55810, loss: 5.8597]\n",
            "Epoch [5/15], batch: [55227/55810, loss: 7.8168]\n",
            "Epoch [5/15], batch: [55228/55810, loss: 7.6523]\n",
            "Epoch [5/15], batch: [55229/55810, loss: 7.0758]\n",
            "Epoch [5/15], batch: [55230/55810, loss: 7.4485]\n",
            "Epoch [5/15], batch: [55231/55810, loss: 6.4585]\n",
            "Epoch [5/15], batch: [55232/55810, loss: 6.5384]\n",
            "Epoch [5/15], batch: [55233/55810, loss: 5.6011]\n",
            "Epoch [5/15], batch: [55234/55810, loss: 7.1294]\n",
            "Epoch [5/15], batch: [55235/55810, loss: 5.0309]\n",
            "Epoch [5/15], batch: [55236/55810, loss: 6.6183]\n",
            "Epoch [5/15], batch: [55237/55810, loss: 6.8911]\n",
            "Epoch [5/15], batch: [55238/55810, loss: 6.1692]\n",
            "Epoch [5/15], batch: [55239/55810, loss: 7.0259]\n",
            "Epoch [5/15], batch: [55240/55810, loss: 5.6549]\n",
            "Epoch [5/15], batch: [55241/55810, loss: 7.2222]\n",
            "Epoch [5/15], batch: [55242/55810, loss: 6.5261]\n",
            "Epoch [5/15], batch: [55243/55810, loss: 6.8989]\n",
            "Epoch [5/15], batch: [55244/55810, loss: 8.0670]\n",
            "Epoch [5/15], batch: [55245/55810, loss: 7.1972]\n",
            "Epoch [5/15], batch: [55246/55810, loss: 6.3133]\n",
            "Epoch [5/15], batch: [55247/55810, loss: 6.4782]\n",
            "Epoch [5/15], batch: [55248/55810, loss: 7.9611]\n",
            "Epoch [5/15], batch: [55249/55810, loss: 3.7769]\n",
            "Epoch [5/15], batch: [55250/55810, loss: 4.8014]\n",
            "Epoch [5/15], batch: [55251/55810, loss: 7.3328]\n",
            "Epoch [5/15], batch: [55252/55810, loss: 7.6148]\n",
            "Epoch [5/15], batch: [55253/55810, loss: 6.0408]\n",
            "Epoch [5/15], batch: [55254/55810, loss: 5.1257]\n",
            "Epoch [5/15], batch: [55255/55810, loss: 6.2378]\n",
            "Epoch [5/15], batch: [55256/55810, loss: 5.6974]\n",
            "Epoch [5/15], batch: [55257/55810, loss: 5.0072]\n",
            "Epoch [5/15], batch: [55258/55810, loss: 7.0582]\n",
            "Epoch [5/15], batch: [55259/55810, loss: 6.3625]\n",
            "Epoch [5/15], batch: [55260/55810, loss: 8.6271]\n",
            "Epoch [5/15], batch: [55261/55810, loss: 7.9559]\n",
            "Epoch [5/15], batch: [55262/55810, loss: 4.7611]\n",
            "Epoch [5/15], batch: [55263/55810, loss: 4.7142]\n",
            "Epoch [5/15], batch: [55264/55810, loss: 6.4649]\n",
            "Epoch [5/15], batch: [55265/55810, loss: 4.8963]\n",
            "Epoch [5/15], batch: [55266/55810, loss: 7.6017]\n",
            "Epoch [5/15], batch: [55267/55810, loss: 6.4469]\n",
            "Epoch [5/15], batch: [55268/55810, loss: 6.9752]\n",
            "Epoch [5/15], batch: [55269/55810, loss: 4.1207]\n",
            "Epoch [5/15], batch: [55270/55810, loss: 6.3696]\n",
            "Epoch [5/15], batch: [55271/55810, loss: 4.5579]\n",
            "Epoch [5/15], batch: [55272/55810, loss: 7.3838]\n",
            "Epoch [5/15], batch: [55273/55810, loss: 5.4838]\n",
            "Epoch [5/15], batch: [55274/55810, loss: 6.2954]\n",
            "Epoch [5/15], batch: [55275/55810, loss: 6.4625]\n",
            "Epoch [5/15], batch: [55276/55810, loss: 6.7007]\n",
            "Epoch [5/15], batch: [55277/55810, loss: 6.0853]\n",
            "Epoch [5/15], batch: [55278/55810, loss: 5.7730]\n",
            "Epoch [5/15], batch: [55279/55810, loss: 7.0534]\n",
            "Epoch [5/15], batch: [55280/55810, loss: 5.3635]\n",
            "Epoch [5/15], batch: [55281/55810, loss: 5.4861]\n",
            "Epoch [5/15], batch: [55282/55810, loss: 5.6209]\n",
            "Epoch [5/15], batch: [55283/55810, loss: 5.1507]\n",
            "Epoch [5/15], batch: [55284/55810, loss: 6.6070]\n",
            "Epoch [5/15], batch: [55285/55810, loss: 4.7967]\n",
            "Epoch [5/15], batch: [55286/55810, loss: 4.3574]\n",
            "Epoch [5/15], batch: [55287/55810, loss: 5.8096]\n",
            "Epoch [5/15], batch: [55288/55810, loss: 5.3268]\n",
            "Epoch [5/15], batch: [55289/55810, loss: 4.9096]\n",
            "Epoch [5/15], batch: [55290/55810, loss: 4.5720]\n",
            "Epoch [5/15], batch: [55291/55810, loss: 7.8222]\n",
            "Epoch [5/15], batch: [55292/55810, loss: 5.3009]\n",
            "Epoch [5/15], batch: [55293/55810, loss: 7.8613]\n",
            "Epoch [5/15], batch: [55294/55810, loss: 7.9543]\n",
            "Epoch [5/15], batch: [55295/55810, loss: 7.5477]\n",
            "Epoch [5/15], batch: [55296/55810, loss: 7.5949]\n",
            "Epoch [5/15], batch: [55297/55810, loss: 7.7598]\n",
            "Epoch [5/15], batch: [55298/55810, loss: 4.6570]\n",
            "Epoch [5/15], batch: [55299/55810, loss: 7.7080]\n",
            "Epoch [5/15], batch: [55300/55810, loss: 7.8466]\n",
            "Epoch [5/15], batch: [55301/55810, loss: 5.3313]\n",
            "Epoch [5/15], batch: [55302/55810, loss: 7.3748]\n",
            "Epoch [5/15], batch: [55303/55810, loss: 7.3281]\n",
            "Epoch [5/15], batch: [55304/55810, loss: 7.3222]\n",
            "Epoch [5/15], batch: [55305/55810, loss: 7.3015]\n",
            "Epoch [5/15], batch: [55306/55810, loss: 5.8044]\n",
            "Epoch [5/15], batch: [55307/55810, loss: 6.9394]\n",
            "Epoch [5/15], batch: [55308/55810, loss: 7.9336]\n",
            "Epoch [5/15], batch: [55309/55810, loss: 6.2139]\n",
            "Epoch [5/15], batch: [55310/55810, loss: 7.1129]\n",
            "Epoch [5/15], batch: [55311/55810, loss: 6.5467]\n",
            "Epoch [5/15], batch: [55312/55810, loss: 6.8145]\n",
            "Epoch [5/15], batch: [55313/55810, loss: 6.7644]\n",
            "Epoch [5/15], batch: [55314/55810, loss: 6.2002]\n",
            "Epoch [5/15], batch: [55315/55810, loss: 5.5853]\n",
            "Epoch [5/15], batch: [55316/55810, loss: 5.2455]\n",
            "Epoch [5/15], batch: [55317/55810, loss: 7.5419]\n",
            "Epoch [5/15], batch: [55318/55810, loss: 6.9102]\n",
            "Epoch [5/15], batch: [55319/55810, loss: 5.5690]\n",
            "Epoch [5/15], batch: [55320/55810, loss: 6.7629]\n",
            "Epoch [5/15], batch: [55321/55810, loss: 6.8729]\n",
            "Epoch [5/15], batch: [55322/55810, loss: 7.3759]\n",
            "Epoch [5/15], batch: [55323/55810, loss: 6.7917]\n",
            "Epoch [5/15], batch: [55324/55810, loss: 4.7976]\n",
            "Epoch [5/15], batch: [55325/55810, loss: 7.5297]\n",
            "Epoch [5/15], batch: [55326/55810, loss: 6.5265]\n",
            "Epoch [5/15], batch: [55327/55810, loss: 5.5701]\n",
            "Epoch [5/15], batch: [55328/55810, loss: 4.9602]\n",
            "Epoch [5/15], batch: [55329/55810, loss: 6.2633]\n",
            "Epoch [5/15], batch: [55330/55810, loss: 4.8192]\n",
            "Epoch [5/15], batch: [55331/55810, loss: 6.7766]\n",
            "Epoch [5/15], batch: [55332/55810, loss: 5.6012]\n",
            "Epoch [5/15], batch: [55333/55810, loss: 6.5076]\n",
            "Epoch [5/15], batch: [55334/55810, loss: 7.7401]\n",
            "Epoch [5/15], batch: [55335/55810, loss: 6.2647]\n",
            "Epoch [5/15], batch: [55336/55810, loss: 6.6161]\n",
            "Epoch [5/15], batch: [55337/55810, loss: 6.6868]\n",
            "Epoch [5/15], batch: [55338/55810, loss: 5.5911]\n",
            "Epoch [5/15], batch: [55339/55810, loss: 3.2366]\n",
            "Epoch [5/15], batch: [55340/55810, loss: 7.1192]\n",
            "Epoch [5/15], batch: [55341/55810, loss: 7.1614]\n",
            "Epoch [5/15], batch: [55342/55810, loss: 5.1808]\n",
            "Epoch [5/15], batch: [55343/55810, loss: 5.2692]\n",
            "Epoch [5/15], batch: [55344/55810, loss: 6.0883]\n",
            "Epoch [5/15], batch: [55345/55810, loss: 5.3759]\n",
            "Epoch [5/15], batch: [55346/55810, loss: 6.4403]\n",
            "Epoch [5/15], batch: [55347/55810, loss: 5.5930]\n",
            "Epoch [5/15], batch: [55348/55810, loss: 4.6093]\n",
            "Epoch [5/15], batch: [55349/55810, loss: 4.7778]\n",
            "Epoch [5/15], batch: [55350/55810, loss: 3.6913]\n",
            "Epoch [5/15], batch: [55351/55810, loss: 5.0120]\n",
            "Epoch [5/15], batch: [55352/55810, loss: 5.5981]\n",
            "Epoch [5/15], batch: [55353/55810, loss: 6.9690]\n",
            "Epoch [5/15], batch: [55354/55810, loss: 6.5214]\n",
            "Epoch [5/15], batch: [55355/55810, loss: 7.1206]\n",
            "Epoch [5/15], batch: [55356/55810, loss: 5.9475]\n",
            "Epoch [5/15], batch: [55357/55810, loss: 6.0405]\n",
            "Epoch [5/15], batch: [55358/55810, loss: 7.0055]\n",
            "Epoch [5/15], batch: [55359/55810, loss: 4.9282]\n",
            "Epoch [5/15], batch: [55360/55810, loss: 7.3890]\n",
            "Epoch [5/15], batch: [55361/55810, loss: 7.3148]\n",
            "Epoch [5/15], batch: [55362/55810, loss: 6.2519]\n",
            "Epoch [5/15], batch: [55363/55810, loss: 6.6459]\n",
            "Epoch [5/15], batch: [55364/55810, loss: 6.2496]\n",
            "Epoch [5/15], batch: [55365/55810, loss: 5.9304]\n",
            "Epoch [5/15], batch: [55366/55810, loss: 6.9459]\n",
            "Epoch [5/15], batch: [55367/55810, loss: 6.6431]\n",
            "Epoch [5/15], batch: [55368/55810, loss: 5.3136]\n",
            "Epoch [5/15], batch: [55369/55810, loss: 8.5161]\n",
            "Epoch [5/15], batch: [55370/55810, loss: 5.4698]\n",
            "Epoch [5/15], batch: [55371/55810, loss: 5.5705]\n",
            "Epoch [5/15], batch: [55372/55810, loss: 6.6908]\n",
            "Epoch [5/15], batch: [55373/55810, loss: 6.9729]\n",
            "Epoch [5/15], batch: [55374/55810, loss: 6.9918]\n",
            "Epoch [5/15], batch: [55375/55810, loss: 8.9795]\n",
            "Epoch [5/15], batch: [55376/55810, loss: 7.4461]\n",
            "Epoch [5/15], batch: [55377/55810, loss: 8.7176]\n",
            "Epoch [5/15], batch: [55378/55810, loss: 6.5781]\n",
            "Epoch [5/15], batch: [55379/55810, loss: 8.0853]\n",
            "Epoch [5/15], batch: [55380/55810, loss: 8.5547]\n",
            "Epoch [5/15], batch: [55381/55810, loss: 6.7450]\n",
            "Epoch [5/15], batch: [55382/55810, loss: 6.2771]\n",
            "Epoch [5/15], batch: [55383/55810, loss: 6.9340]\n",
            "Epoch [5/15], batch: [55384/55810, loss: 6.2745]\n",
            "Epoch [5/15], batch: [55385/55810, loss: 6.1448]\n",
            "Epoch [5/15], batch: [55386/55810, loss: 6.8830]\n",
            "Epoch [5/15], batch: [55387/55810, loss: 7.5820]\n",
            "Epoch [5/15], batch: [55388/55810, loss: 6.6306]\n",
            "Epoch [5/15], batch: [55389/55810, loss: 6.3884]\n",
            "Epoch [5/15], batch: [55390/55810, loss: 6.6550]\n",
            "Epoch [5/15], batch: [55391/55810, loss: 6.1537]\n",
            "Epoch [5/15], batch: [55392/55810, loss: 5.9961]\n",
            "Epoch [5/15], batch: [55393/55810, loss: 6.9137]\n",
            "Epoch [5/15], batch: [55394/55810, loss: 7.9266]\n",
            "Epoch [5/15], batch: [55395/55810, loss: 6.4856]\n",
            "Epoch [5/15], batch: [55396/55810, loss: 7.0863]\n",
            "Epoch [5/15], batch: [55397/55810, loss: 7.8982]\n",
            "Epoch [5/15], batch: [55398/55810, loss: 7.8468]\n",
            "Epoch [5/15], batch: [55399/55810, loss: 6.3233]\n",
            "Epoch [5/15], batch: [55400/55810, loss: 6.0254]\n",
            "Epoch [5/15], batch: [55401/55810, loss: 6.7227]\n",
            "Epoch [5/15], batch: [55402/55810, loss: 5.8020]\n",
            "Epoch [5/15], batch: [55403/55810, loss: 6.4099]\n",
            "Epoch [5/15], batch: [55404/55810, loss: 5.4027]\n",
            "Epoch [5/15], batch: [55405/55810, loss: 6.5300]\n",
            "Epoch [5/15], batch: [55406/55810, loss: 6.1475]\n",
            "Epoch [5/15], batch: [55407/55810, loss: 8.3560]\n",
            "Epoch [5/15], batch: [55408/55810, loss: 7.6011]\n",
            "Epoch [5/15], batch: [55409/55810, loss: 7.2431]\n",
            "Epoch [5/15], batch: [55410/55810, loss: 7.8668]\n",
            "Epoch [5/15], batch: [55411/55810, loss: 6.4901]\n",
            "Epoch [5/15], batch: [55412/55810, loss: 5.2617]\n",
            "Epoch [5/15], batch: [55413/55810, loss: 5.5580]\n",
            "Epoch [5/15], batch: [55414/55810, loss: 6.7555]\n",
            "Epoch [5/15], batch: [55415/55810, loss: 3.9108]\n",
            "Epoch [5/15], batch: [55416/55810, loss: 6.2509]\n",
            "Epoch [5/15], batch: [55417/55810, loss: 5.8156]\n",
            "Epoch [5/15], batch: [55418/55810, loss: 6.0133]\n",
            "Epoch [5/15], batch: [55419/55810, loss: 8.3468]\n",
            "Epoch [5/15], batch: [55420/55810, loss: 7.6275]\n",
            "Epoch [5/15], batch: [55421/55810, loss: 6.9398]\n",
            "Epoch [5/15], batch: [55422/55810, loss: 6.6102]\n",
            "Epoch [5/15], batch: [55423/55810, loss: 6.6176]\n",
            "Epoch [5/15], batch: [55424/55810, loss: 6.0484]\n",
            "Epoch [5/15], batch: [55425/55810, loss: 5.4661]\n",
            "Epoch [5/15], batch: [55426/55810, loss: 6.4622]\n",
            "Epoch [5/15], batch: [55427/55810, loss: 6.3011]\n",
            "Epoch [5/15], batch: [55428/55810, loss: 6.3294]\n",
            "Epoch [5/15], batch: [55429/55810, loss: 5.9663]\n",
            "Epoch [5/15], batch: [55430/55810, loss: 6.0668]\n",
            "Epoch [5/15], batch: [55431/55810, loss: 6.1202]\n",
            "Epoch [5/15], batch: [55432/55810, loss: 5.9605]\n",
            "Epoch [5/15], batch: [55433/55810, loss: 6.6112]\n",
            "Epoch [5/15], batch: [55434/55810, loss: 7.0075]\n",
            "Epoch [5/15], batch: [55435/55810, loss: 4.6965]\n",
            "Epoch [5/15], batch: [55436/55810, loss: 6.0249]\n",
            "Epoch [5/15], batch: [55437/55810, loss: 5.3974]\n",
            "Epoch [5/15], batch: [55438/55810, loss: 5.8478]\n",
            "Epoch [5/15], batch: [55439/55810, loss: 6.1441]\n",
            "Epoch [5/15], batch: [55440/55810, loss: 5.1064]\n",
            "Epoch [5/15], batch: [55441/55810, loss: 6.5184]\n",
            "Epoch [5/15], batch: [55442/55810, loss: 6.4425]\n",
            "Epoch [5/15], batch: [55443/55810, loss: 4.5690]\n",
            "Epoch [5/15], batch: [55444/55810, loss: 4.7640]\n",
            "Epoch [5/15], batch: [55445/55810, loss: 6.6271]\n",
            "Epoch [5/15], batch: [55446/55810, loss: 7.8282]\n",
            "Epoch [5/15], batch: [55447/55810, loss: 6.5153]\n",
            "Epoch [5/15], batch: [55448/55810, loss: 5.3181]\n",
            "Epoch [5/15], batch: [55449/55810, loss: 6.9233]\n",
            "Epoch [5/15], batch: [55450/55810, loss: 5.8977]\n",
            "Epoch [5/15], batch: [55451/55810, loss: 5.7666]\n",
            "Epoch [5/15], batch: [55452/55810, loss: 5.2339]\n",
            "Epoch [5/15], batch: [55453/55810, loss: 7.1065]\n",
            "Epoch [5/15], batch: [55454/55810, loss: 7.4651]\n",
            "Epoch [5/15], batch: [55455/55810, loss: 8.3160]\n",
            "Epoch [5/15], batch: [55456/55810, loss: 7.4873]\n",
            "Epoch [5/15], batch: [55457/55810, loss: 5.9301]\n",
            "Epoch [5/15], batch: [55458/55810, loss: 5.6597]\n",
            "Epoch [5/15], batch: [55459/55810, loss: 5.4704]\n",
            "Epoch [5/15], batch: [55460/55810, loss: 6.8364]\n",
            "Epoch [5/15], batch: [55461/55810, loss: 6.9617]\n",
            "Epoch [5/15], batch: [55462/55810, loss: 6.1458]\n",
            "Epoch [5/15], batch: [55463/55810, loss: 7.5699]\n",
            "Epoch [5/15], batch: [55464/55810, loss: 7.1268]\n",
            "Epoch [5/15], batch: [55465/55810, loss: 6.0125]\n",
            "Epoch [5/15], batch: [55466/55810, loss: 5.8781]\n",
            "Epoch [5/15], batch: [55467/55810, loss: 7.4377]\n",
            "Epoch [5/15], batch: [55468/55810, loss: 6.6415]\n",
            "Epoch [5/15], batch: [55469/55810, loss: 7.5490]\n",
            "Epoch [5/15], batch: [55470/55810, loss: 5.6071]\n",
            "Epoch [5/15], batch: [55471/55810, loss: 7.4613]\n",
            "Epoch [5/15], batch: [55472/55810, loss: 7.2788]\n",
            "Epoch [5/15], batch: [55473/55810, loss: 6.6150]\n",
            "Epoch [5/15], batch: [55474/55810, loss: 5.9959]\n",
            "Epoch [5/15], batch: [55475/55810, loss: 7.0188]\n",
            "Epoch [5/15], batch: [55476/55810, loss: 6.0189]\n",
            "Epoch [5/15], batch: [55477/55810, loss: 7.8611]\n",
            "Epoch [5/15], batch: [55478/55810, loss: 4.8460]\n",
            "Epoch [5/15], batch: [55479/55810, loss: 7.4700]\n",
            "Epoch [5/15], batch: [55480/55810, loss: 4.7181]\n",
            "Epoch [5/15], batch: [55481/55810, loss: 5.4162]\n",
            "Epoch [5/15], batch: [55482/55810, loss: 5.5932]\n",
            "Epoch [5/15], batch: [55483/55810, loss: 6.6253]\n",
            "Epoch [5/15], batch: [55484/55810, loss: 6.3635]\n",
            "Epoch [5/15], batch: [55485/55810, loss: 6.1214]\n",
            "Epoch [5/15], batch: [55486/55810, loss: 5.3716]\n",
            "Epoch [5/15], batch: [55487/55810, loss: 4.8695]\n",
            "Epoch [5/15], batch: [55488/55810, loss: 5.8141]\n",
            "Epoch [5/15], batch: [55489/55810, loss: 7.0087]\n",
            "Epoch [5/15], batch: [55490/55810, loss: 6.0541]\n",
            "Epoch [5/15], batch: [55491/55810, loss: 6.5572]\n",
            "Epoch [5/15], batch: [55492/55810, loss: 4.9808]\n",
            "Epoch [5/15], batch: [55493/55810, loss: 6.3327]\n",
            "Epoch [5/15], batch: [55494/55810, loss: 6.6057]\n",
            "Epoch [5/15], batch: [55495/55810, loss: 6.4770]\n",
            "Epoch [5/15], batch: [55496/55810, loss: 6.7487]\n",
            "Epoch [5/15], batch: [55497/55810, loss: 5.6527]\n",
            "Epoch [5/15], batch: [55498/55810, loss: 5.8978]\n",
            "Epoch [5/15], batch: [55499/55810, loss: 4.7097]\n",
            "Epoch [5/15], batch: [55500/55810, loss: 5.2044]\n",
            "Epoch [5/15], batch: [55501/55810, loss: 6.2373]\n",
            "Epoch [5/15], batch: [55502/55810, loss: 7.7216]\n",
            "Epoch [5/15], batch: [55503/55810, loss: 6.5668]\n",
            "Epoch [5/15], batch: [55504/55810, loss: 7.6436]\n",
            "Epoch [5/15], batch: [55505/55810, loss: 7.0271]\n",
            "Epoch [5/15], batch: [55506/55810, loss: 7.3069]\n",
            "Epoch [5/15], batch: [55507/55810, loss: 7.1898]\n",
            "Epoch [5/15], batch: [55508/55810, loss: 6.6500]\n",
            "Epoch [5/15], batch: [55509/55810, loss: 7.2683]\n",
            "Epoch [5/15], batch: [55510/55810, loss: 6.8679]\n",
            "Epoch [5/15], batch: [55511/55810, loss: 7.4250]\n",
            "Epoch [5/15], batch: [55512/55810, loss: 6.6166]\n",
            "Epoch [5/15], batch: [55513/55810, loss: 5.9247]\n",
            "Epoch [5/15], batch: [55514/55810, loss: 6.8425]\n",
            "Epoch [5/15], batch: [55515/55810, loss: 6.8140]\n",
            "Epoch [5/15], batch: [55516/55810, loss: 7.4482]\n",
            "Epoch [5/15], batch: [55517/55810, loss: 6.8888]\n",
            "Epoch [5/15], batch: [55518/55810, loss: 4.8276]\n",
            "Epoch [5/15], batch: [55519/55810, loss: 5.9568]\n",
            "Epoch [5/15], batch: [55520/55810, loss: 4.9358]\n",
            "Epoch [5/15], batch: [55521/55810, loss: 7.1576]\n",
            "Epoch [5/15], batch: [55522/55810, loss: 7.3155]\n",
            "Epoch [5/15], batch: [55523/55810, loss: 7.5577]\n",
            "Epoch [5/15], batch: [55524/55810, loss: 5.6098]\n",
            "Epoch [5/15], batch: [55525/55810, loss: 6.8101]\n",
            "Epoch [5/15], batch: [55526/55810, loss: 8.0188]\n",
            "Epoch [5/15], batch: [55527/55810, loss: 6.9932]\n",
            "Epoch [5/15], batch: [55528/55810, loss: 6.8704]\n",
            "Epoch [5/15], batch: [55529/55810, loss: 7.1160]\n",
            "Epoch [5/15], batch: [55530/55810, loss: 6.5639]\n",
            "Epoch [5/15], batch: [55531/55810, loss: 6.5283]\n",
            "Epoch [5/15], batch: [55532/55810, loss: 8.5612]\n",
            "Epoch [5/15], batch: [55533/55810, loss: 7.8508]\n",
            "Epoch [5/15], batch: [55534/55810, loss: 7.8792]\n",
            "Epoch [5/15], batch: [55535/55810, loss: 5.7343]\n",
            "Epoch [5/15], batch: [55536/55810, loss: 7.1290]\n",
            "Epoch [5/15], batch: [55537/55810, loss: 6.5005]\n",
            "Epoch [5/15], batch: [55538/55810, loss: 6.6972]\n",
            "Epoch [5/15], batch: [55539/55810, loss: 6.0659]\n",
            "Epoch [5/15], batch: [55540/55810, loss: 4.8678]\n",
            "Epoch [5/15], batch: [55541/55810, loss: 8.0680]\n",
            "Epoch [5/15], batch: [55542/55810, loss: 8.1669]\n",
            "Epoch [5/15], batch: [55543/55810, loss: 6.7095]\n",
            "Epoch [5/15], batch: [55544/55810, loss: 7.6620]\n",
            "Epoch [5/15], batch: [55545/55810, loss: 5.3849]\n",
            "Epoch [5/15], batch: [55546/55810, loss: 7.3927]\n",
            "Epoch [5/15], batch: [55547/55810, loss: 6.6543]\n",
            "Epoch [5/15], batch: [55548/55810, loss: 5.3666]\n",
            "Epoch [5/15], batch: [55549/55810, loss: 5.9218]\n",
            "Epoch [5/15], batch: [55550/55810, loss: 5.7618]\n",
            "Epoch [5/15], batch: [55551/55810, loss: 7.5904]\n",
            "Epoch [5/15], batch: [55552/55810, loss: 6.8288]\n",
            "Epoch [5/15], batch: [55553/55810, loss: 5.2970]\n",
            "Epoch [5/15], batch: [55554/55810, loss: 3.9183]\n",
            "Epoch [5/15], batch: [55555/55810, loss: 6.0593]\n",
            "Epoch [5/15], batch: [55556/55810, loss: 3.1226]\n",
            "Epoch [5/15], batch: [55557/55810, loss: 5.0829]\n",
            "Epoch [5/15], batch: [55558/55810, loss: 6.4964]\n",
            "Epoch [5/15], batch: [55559/55810, loss: 5.6805]\n",
            "Epoch [5/15], batch: [55560/55810, loss: 7.2183]\n",
            "Epoch [5/15], batch: [55561/55810, loss: 5.6508]\n",
            "Epoch [5/15], batch: [55562/55810, loss: 6.9665]\n",
            "Epoch [5/15], batch: [55563/55810, loss: 5.7123]\n",
            "Epoch [5/15], batch: [55564/55810, loss: 6.4153]\n",
            "Epoch [5/15], batch: [55565/55810, loss: 5.8096]\n",
            "Epoch [5/15], batch: [55566/55810, loss: 7.0755]\n",
            "Epoch [5/15], batch: [55567/55810, loss: 5.1542]\n",
            "Epoch [5/15], batch: [55568/55810, loss: 6.8133]\n",
            "Epoch [5/15], batch: [55569/55810, loss: 5.9704]\n",
            "Epoch [5/15], batch: [55570/55810, loss: 7.2252]\n",
            "Epoch [5/15], batch: [55571/55810, loss: 7.0276]\n",
            "Epoch [5/15], batch: [55572/55810, loss: 6.3972]\n",
            "Epoch [5/15], batch: [55573/55810, loss: 7.3680]\n",
            "Epoch [5/15], batch: [55574/55810, loss: 6.1784]\n",
            "Epoch [5/15], batch: [55575/55810, loss: 5.3743]\n",
            "Epoch [5/15], batch: [55576/55810, loss: 6.9426]\n",
            "Epoch [5/15], batch: [55577/55810, loss: 7.5555]\n",
            "Epoch [5/15], batch: [55578/55810, loss: 7.8664]\n",
            "Epoch [5/15], batch: [55579/55810, loss: 6.4263]\n",
            "Epoch [5/15], batch: [55580/55810, loss: 5.4388]\n",
            "Epoch [5/15], batch: [55581/55810, loss: 7.1735]\n",
            "Epoch [5/15], batch: [55582/55810, loss: 8.3315]\n",
            "Epoch [5/15], batch: [55583/55810, loss: 5.8953]\n",
            "Epoch [5/15], batch: [55584/55810, loss: 7.7875]\n",
            "Epoch [5/15], batch: [55585/55810, loss: 7.1470]\n",
            "Epoch [5/15], batch: [55586/55810, loss: 5.2872]\n",
            "Epoch [5/15], batch: [55587/55810, loss: 6.9210]\n",
            "Epoch [5/15], batch: [55588/55810, loss: 6.2499]\n",
            "Epoch [5/15], batch: [55589/55810, loss: 6.7447]\n",
            "Epoch [5/15], batch: [55590/55810, loss: 4.8930]\n",
            "Epoch [5/15], batch: [55591/55810, loss: 5.1182]\n",
            "Epoch [5/15], batch: [55592/55810, loss: 5.0078]\n",
            "Epoch [5/15], batch: [55593/55810, loss: 5.4662]\n",
            "Epoch [5/15], batch: [55594/55810, loss: 6.8458]\n",
            "Epoch [5/15], batch: [55595/55810, loss: 5.5666]\n",
            "Epoch [5/15], batch: [55596/55810, loss: 5.4101]\n",
            "Epoch [5/15], batch: [55597/55810, loss: 4.8035]\n",
            "Epoch [5/15], batch: [55598/55810, loss: 4.2455]\n",
            "Epoch [5/15], batch: [55599/55810, loss: 4.1302]\n",
            "Epoch [5/15], batch: [55600/55810, loss: 7.1821]\n",
            "Epoch [5/15], batch: [55601/55810, loss: 7.6547]\n",
            "Epoch [5/15], batch: [55602/55810, loss: 6.3392]\n",
            "Epoch [5/15], batch: [55603/55810, loss: 8.2404]\n",
            "Epoch [5/15], batch: [55604/55810, loss: 8.0771]\n",
            "Epoch [5/15], batch: [55605/55810, loss: 5.8230]\n",
            "Epoch [5/15], batch: [55606/55810, loss: 6.9322]\n",
            "Epoch [5/15], batch: [55607/55810, loss: 6.4593]\n",
            "Epoch [5/15], batch: [55608/55810, loss: 7.6849]\n",
            "Epoch [5/15], batch: [55609/55810, loss: 6.6024]\n",
            "Epoch [5/15], batch: [55610/55810, loss: 6.9664]\n",
            "Epoch [5/15], batch: [55611/55810, loss: 6.7463]\n",
            "Epoch [5/15], batch: [55612/55810, loss: 6.3328]\n",
            "Epoch [5/15], batch: [55613/55810, loss: 4.5443]\n",
            "Epoch [5/15], batch: [55614/55810, loss: 4.8067]\n",
            "Epoch [5/15], batch: [55615/55810, loss: 7.0219]\n",
            "Epoch [5/15], batch: [55616/55810, loss: 6.0007]\n",
            "Epoch [5/15], batch: [55617/55810, loss: 6.5504]\n",
            "Epoch [5/15], batch: [55618/55810, loss: 5.4367]\n",
            "Epoch [5/15], batch: [55619/55810, loss: 5.7312]\n",
            "Epoch [5/15], batch: [55620/55810, loss: 5.2389]\n",
            "Epoch [5/15], batch: [55621/55810, loss: 5.4001]\n",
            "Epoch [5/15], batch: [55622/55810, loss: 6.7690]\n",
            "Epoch [5/15], batch: [55623/55810, loss: 6.1734]\n",
            "Epoch [5/15], batch: [55624/55810, loss: 4.8402]\n",
            "Epoch [5/15], batch: [55625/55810, loss: 7.3380]\n",
            "Epoch [5/15], batch: [55626/55810, loss: 7.5787]\n",
            "Epoch [5/15], batch: [55627/55810, loss: 6.9968]\n",
            "Epoch [5/15], batch: [55628/55810, loss: 7.1085]\n",
            "Epoch [5/15], batch: [55629/55810, loss: 6.5637]\n",
            "Epoch [5/15], batch: [55630/55810, loss: 6.3874]\n",
            "Epoch [5/15], batch: [55631/55810, loss: 5.4449]\n",
            "Epoch [5/15], batch: [55632/55810, loss: 5.6709]\n",
            "Epoch [5/15], batch: [55633/55810, loss: 6.5106]\n",
            "Epoch [5/15], batch: [55634/55810, loss: 6.5480]\n",
            "Epoch [5/15], batch: [55635/55810, loss: 6.0804]\n",
            "Epoch [5/15], batch: [55636/55810, loss: 6.1761]\n",
            "Epoch [5/15], batch: [55637/55810, loss: 4.2319]\n",
            "Epoch [5/15], batch: [55638/55810, loss: 6.4008]\n",
            "Epoch [5/15], batch: [55639/55810, loss: 5.9399]\n",
            "Epoch [5/15], batch: [55640/55810, loss: 8.4787]\n",
            "Epoch [5/15], batch: [55641/55810, loss: 6.2700]\n",
            "Epoch [5/15], batch: [55642/55810, loss: 6.2523]\n",
            "Epoch [5/15], batch: [55643/55810, loss: 7.6594]\n",
            "Epoch [5/15], batch: [55644/55810, loss: 5.6083]\n",
            "Epoch [5/15], batch: [55645/55810, loss: 5.1034]\n",
            "Epoch [5/15], batch: [55646/55810, loss: 6.8845]\n",
            "Epoch [5/15], batch: [55647/55810, loss: 6.7927]\n",
            "Epoch [5/15], batch: [55648/55810, loss: 7.8581]\n",
            "Epoch [5/15], batch: [55649/55810, loss: 7.0249]\n",
            "Epoch [5/15], batch: [55650/55810, loss: 5.3185]\n",
            "Epoch [5/15], batch: [55651/55810, loss: 9.1040]\n",
            "Epoch [5/15], batch: [55652/55810, loss: 9.4916]\n",
            "Epoch [5/15], batch: [55653/55810, loss: 7.5853]\n",
            "Epoch [5/15], batch: [55654/55810, loss: 7.2346]\n",
            "Epoch [5/15], batch: [55655/55810, loss: 7.4089]\n",
            "Epoch [5/15], batch: [55656/55810, loss: 7.3937]\n",
            "Epoch [5/15], batch: [55657/55810, loss: 8.3325]\n",
            "Epoch [5/15], batch: [55658/55810, loss: 6.6886]\n",
            "Epoch [5/15], batch: [55659/55810, loss: 7.9087]\n",
            "Epoch [5/15], batch: [55660/55810, loss: 7.4948]\n",
            "Epoch [5/15], batch: [55661/55810, loss: 4.8226]\n",
            "Epoch [5/15], batch: [55662/55810, loss: 6.1552]\n",
            "Epoch [5/15], batch: [55663/55810, loss: 6.3445]\n",
            "Epoch [5/15], batch: [55664/55810, loss: 4.5996]\n",
            "Epoch [5/15], batch: [55665/55810, loss: 5.9048]\n",
            "Epoch [5/15], batch: [55666/55810, loss: 5.8857]\n",
            "Epoch [5/15], batch: [55667/55810, loss: 6.1710]\n",
            "Epoch [5/15], batch: [55668/55810, loss: 5.3222]\n",
            "Epoch [5/15], batch: [55669/55810, loss: 5.0549]\n",
            "Epoch [5/15], batch: [55670/55810, loss: 6.5126]\n",
            "Epoch [5/15], batch: [55671/55810, loss: 6.5331]\n",
            "Epoch [5/15], batch: [55672/55810, loss: 5.8589]\n",
            "Epoch [5/15], batch: [55673/55810, loss: 7.3691]\n",
            "Epoch [5/15], batch: [55674/55810, loss: 5.4709]\n",
            "Epoch [5/15], batch: [55675/55810, loss: 6.2513]\n",
            "Epoch [5/15], batch: [55676/55810, loss: 6.5053]\n",
            "Epoch [5/15], batch: [55677/55810, loss: 5.5738]\n",
            "Epoch [5/15], batch: [55678/55810, loss: 5.8933]\n",
            "Epoch [5/15], batch: [55679/55810, loss: 5.4575]\n",
            "Epoch [5/15], batch: [55680/55810, loss: 5.5261]\n",
            "Epoch [5/15], batch: [55681/55810, loss: 5.7705]\n",
            "Epoch [5/15], batch: [55682/55810, loss: 5.6822]\n",
            "Epoch [5/15], batch: [55683/55810, loss: 6.1945]\n",
            "Epoch [5/15], batch: [55684/55810, loss: 7.4091]\n",
            "Epoch [5/15], batch: [55685/55810, loss: 4.9466]\n",
            "Epoch [5/15], batch: [55686/55810, loss: 7.6077]\n",
            "Epoch [5/15], batch: [55687/55810, loss: 4.8905]\n",
            "Epoch [5/15], batch: [55688/55810, loss: 4.8693]\n",
            "Epoch [5/15], batch: [55689/55810, loss: 3.2715]\n",
            "Epoch [5/15], batch: [55690/55810, loss: 6.0479]\n",
            "Epoch [5/15], batch: [55691/55810, loss: 5.1116]\n",
            "Epoch [5/15], batch: [55692/55810, loss: 6.0769]\n",
            "Epoch [5/15], batch: [55693/55810, loss: 7.0188]\n",
            "Epoch [5/15], batch: [55694/55810, loss: 8.0631]\n",
            "Epoch [5/15], batch: [55695/55810, loss: 6.7524]\n",
            "Epoch [5/15], batch: [55696/55810, loss: 4.9224]\n",
            "Epoch [5/15], batch: [55697/55810, loss: 5.2405]\n",
            "Epoch [5/15], batch: [55698/55810, loss: 6.9795]\n",
            "Epoch [5/15], batch: [55699/55810, loss: 6.5843]\n",
            "Epoch [5/15], batch: [55700/55810, loss: 4.7948]\n",
            "Epoch [5/15], batch: [55701/55810, loss: 6.1228]\n",
            "Epoch [5/15], batch: [55702/55810, loss: 6.6382]\n",
            "Epoch [5/15], batch: [55703/55810, loss: 6.8562]\n",
            "Epoch [5/15], batch: [55704/55810, loss: 4.1778]\n",
            "Epoch [5/15], batch: [55705/55810, loss: 5.4051]\n",
            "Epoch [5/15], batch: [55706/55810, loss: 6.0245]\n",
            "Epoch [5/15], batch: [55707/55810, loss: 7.0728]\n",
            "Epoch [5/15], batch: [55708/55810, loss: 5.7370]\n",
            "Epoch [5/15], batch: [55709/55810, loss: 6.5512]\n",
            "Epoch [5/15], batch: [55710/55810, loss: 7.2630]\n",
            "Epoch [5/15], batch: [55711/55810, loss: 7.9990]\n",
            "Epoch [5/15], batch: [55712/55810, loss: 7.5387]\n",
            "Epoch [5/15], batch: [55713/55810, loss: 6.5399]\n",
            "Epoch [5/15], batch: [55714/55810, loss: 6.6621]\n",
            "Epoch [5/15], batch: [55715/55810, loss: 5.0923]\n",
            "Epoch [5/15], batch: [55716/55810, loss: 6.8090]\n",
            "Epoch [5/15], batch: [55717/55810, loss: 6.0340]\n",
            "Epoch [5/15], batch: [55718/55810, loss: 6.3635]\n",
            "Epoch [5/15], batch: [55719/55810, loss: 6.3273]\n",
            "Epoch [5/15], batch: [55720/55810, loss: 4.9259]\n",
            "Epoch [5/15], batch: [55721/55810, loss: 6.5035]\n",
            "Epoch [5/15], batch: [55722/55810, loss: 7.5391]\n",
            "Epoch [5/15], batch: [55723/55810, loss: 6.0166]\n",
            "Epoch [5/15], batch: [55724/55810, loss: 6.4461]\n",
            "Epoch [5/15], batch: [55725/55810, loss: 6.4868]\n",
            "Epoch [5/15], batch: [55726/55810, loss: 8.4409]\n",
            "Epoch [5/15], batch: [55727/55810, loss: 6.6187]\n",
            "Epoch [5/15], batch: [55728/55810, loss: 8.9258]\n",
            "Epoch [5/15], batch: [55729/55810, loss: 6.5707]\n",
            "Epoch [5/15], batch: [55730/55810, loss: 5.1942]\n",
            "Epoch [5/15], batch: [55731/55810, loss: 6.4065]\n",
            "Epoch [5/15], batch: [55732/55810, loss: 3.5812]\n",
            "Epoch [5/15], batch: [55733/55810, loss: 5.9801]\n",
            "Epoch [5/15], batch: [55734/55810, loss: 3.9100]\n",
            "Epoch [5/15], batch: [55735/55810, loss: 5.0551]\n",
            "Epoch [5/15], batch: [55736/55810, loss: 6.7790]\n",
            "Epoch [5/15], batch: [55737/55810, loss: 6.2106]\n",
            "Epoch [5/15], batch: [55738/55810, loss: 8.1909]\n",
            "Epoch [5/15], batch: [55739/55810, loss: 5.6655]\n",
            "Epoch [5/15], batch: [55740/55810, loss: 7.6779]\n",
            "Epoch [5/15], batch: [55741/55810, loss: 6.1824]\n",
            "Epoch [5/15], batch: [55742/55810, loss: 5.6123]\n",
            "Epoch [5/15], batch: [55743/55810, loss: 8.5334]\n",
            "Epoch [5/15], batch: [55744/55810, loss: 6.4223]\n",
            "Epoch [5/15], batch: [55745/55810, loss: 8.1586]\n",
            "Epoch [5/15], batch: [55746/55810, loss: 4.9824]\n",
            "Epoch [5/15], batch: [55747/55810, loss: 8.2735]\n",
            "Epoch [5/15], batch: [55748/55810, loss: 6.6127]\n",
            "Epoch [5/15], batch: [55749/55810, loss: 6.1069]\n",
            "Epoch [5/15], batch: [55750/55810, loss: 7.6203]\n",
            "Epoch [5/15], batch: [55751/55810, loss: 5.5757]\n",
            "Epoch [5/15], batch: [55752/55810, loss: 7.9716]\n",
            "Epoch [5/15], batch: [55753/55810, loss: 7.0699]\n",
            "Epoch [5/15], batch: [55754/55810, loss: 6.5419]\n",
            "Epoch [5/15], batch: [55755/55810, loss: 7.5435]\n",
            "Epoch [5/15], batch: [55756/55810, loss: 6.9617]\n",
            "Epoch [5/15], batch: [55757/55810, loss: 6.0986]\n",
            "Epoch [5/15], batch: [55758/55810, loss: 5.8450]\n",
            "Epoch [5/15], batch: [55759/55810, loss: 7.3295]\n",
            "Epoch [5/15], batch: [55760/55810, loss: 6.2550]\n",
            "Epoch [5/15], batch: [55761/55810, loss: 5.4800]\n",
            "Epoch [5/15], batch: [55762/55810, loss: 6.6869]\n",
            "Epoch [5/15], batch: [55763/55810, loss: 6.2211]\n",
            "Epoch [5/15], batch: [55764/55810, loss: 6.8982]\n",
            "Epoch [5/15], batch: [55765/55810, loss: 5.9146]\n",
            "Epoch [5/15], batch: [55766/55810, loss: 6.9623]\n",
            "Epoch [5/15], batch: [55767/55810, loss: 5.9785]\n",
            "Epoch [5/15], batch: [55768/55810, loss: 6.9266]\n",
            "Epoch [5/15], batch: [55769/55810, loss: 5.8206]\n",
            "Epoch [5/15], batch: [55770/55810, loss: 6.8296]\n",
            "Epoch [5/15], batch: [55771/55810, loss: 6.9927]\n",
            "Epoch [5/15], batch: [55772/55810, loss: 6.2752]\n",
            "Epoch [5/15], batch: [55773/55810, loss: 5.5630]\n",
            "Epoch [5/15], batch: [55774/55810, loss: 4.6425]\n",
            "Epoch [5/15], batch: [55775/55810, loss: 5.6136]\n",
            "Epoch [5/15], batch: [55776/55810, loss: 7.6317]\n",
            "Epoch [5/15], batch: [55777/55810, loss: 7.3277]\n",
            "Epoch [5/15], batch: [55778/55810, loss: 4.7404]\n",
            "Epoch [5/15], batch: [55779/55810, loss: 7.5208]\n",
            "Epoch [5/15], batch: [55780/55810, loss: 4.8483]\n",
            "Epoch [5/15], batch: [55781/55810, loss: 5.6146]\n",
            "Epoch [5/15], batch: [55782/55810, loss: 6.1863]\n",
            "Epoch [5/15], batch: [55783/55810, loss: 7.7781]\n",
            "Epoch [5/15], batch: [55784/55810, loss: 6.8299]\n",
            "Epoch [5/15], batch: [55785/55810, loss: 6.3078]\n",
            "Epoch [5/15], batch: [55786/55810, loss: 5.8919]\n",
            "Epoch [5/15], batch: [55787/55810, loss: 7.5431]\n",
            "Epoch [5/15], batch: [55788/55810, loss: 7.0805]\n",
            "Epoch [5/15], batch: [55789/55810, loss: 5.4499]\n",
            "Epoch [5/15], batch: [55790/55810, loss: 6.1526]\n",
            "Epoch [5/15], batch: [55791/55810, loss: 6.9736]\n",
            "Epoch [5/15], batch: [55792/55810, loss: 7.7347]\n",
            "Epoch [5/15], batch: [55793/55810, loss: 6.4080]\n",
            "Epoch [5/15], batch: [55794/55810, loss: 6.2236]\n",
            "Epoch [5/15], batch: [55795/55810, loss: 6.3804]\n",
            "Epoch [5/15], batch: [55796/55810, loss: 8.1638]\n",
            "Epoch [5/15], batch: [55797/55810, loss: 6.8361]\n",
            "Epoch [5/15], batch: [55798/55810, loss: 7.9575]\n",
            "Epoch [5/15], batch: [55799/55810, loss: 6.7493]\n",
            "Epoch [5/15], batch: [55800/55810, loss: 6.4038]\n",
            "Epoch [5/15], batch: [55801/55810, loss: 7.6236]\n",
            "Epoch [5/15], batch: [55802/55810, loss: 6.1948]\n",
            "Epoch [5/15], batch: [55803/55810, loss: 5.7800]\n",
            "Epoch [5/15], batch: [55804/55810, loss: 5.1671]\n",
            "Epoch [5/15], batch: [55805/55810, loss: 6.4974]\n",
            "Epoch [5/15], batch: [55806/55810, loss: 8.0084]\n",
            "Epoch [5/15], batch: [55807/55810, loss: 7.4845]\n",
            "Epoch [5/15], batch: [55808/55810, loss: 5.3738]\n",
            "Epoch [5/15], batch: [55809/55810, loss: 5.7686]\n",
            "Epoch [5/15], batch: [55810/55810, loss: 5.0846]\n",
            "Epoch [6/15], batch: [1/55810, loss: 6.3381]\n",
            "Epoch [6/15], batch: [2/55810, loss: 6.5034]\n",
            "Epoch [6/15], batch: [3/55810, loss: 4.7827]\n",
            "Epoch [6/15], batch: [4/55810, loss: 5.7739]\n",
            "Epoch [6/15], batch: [5/55810, loss: 5.6519]\n",
            "Epoch [6/15], batch: [6/55810, loss: 4.1405]\n",
            "Epoch [6/15], batch: [7/55810, loss: 4.8057]\n",
            "Epoch [6/15], batch: [8/55810, loss: 5.5908]\n",
            "Epoch [6/15], batch: [9/55810, loss: 6.8540]\n",
            "Epoch [6/15], batch: [10/55810, loss: 7.2536]\n",
            "Epoch [6/15], batch: [11/55810, loss: 6.0776]\n",
            "Epoch [6/15], batch: [12/55810, loss: 5.8837]\n",
            "Epoch [6/15], batch: [13/55810, loss: 5.3874]\n",
            "Epoch [6/15], batch: [14/55810, loss: 6.5422]\n",
            "Epoch [6/15], batch: [15/55810, loss: 7.4064]\n",
            "Epoch [6/15], batch: [16/55810, loss: 6.9335]\n",
            "Epoch [6/15], batch: [17/55810, loss: 6.4220]\n",
            "Epoch [6/15], batch: [18/55810, loss: 7.3551]\n",
            "Epoch [6/15], batch: [19/55810, loss: 5.7961]\n",
            "Epoch [6/15], batch: [20/55810, loss: 7.6192]\n",
            "Epoch [6/15], batch: [21/55810, loss: 6.5077]\n",
            "Epoch [6/15], batch: [22/55810, loss: 5.5674]\n",
            "Epoch [6/15], batch: [23/55810, loss: 6.6212]\n",
            "Epoch [6/15], batch: [24/55810, loss: 7.0389]\n",
            "Epoch [6/15], batch: [25/55810, loss: 5.6538]\n",
            "Epoch [6/15], batch: [26/55810, loss: 7.6222]\n",
            "Epoch [6/15], batch: [27/55810, loss: 6.2093]\n",
            "Epoch [6/15], batch: [28/55810, loss: 6.7253]\n",
            "Epoch [6/15], batch: [29/55810, loss: 6.3872]\n",
            "Epoch [6/15], batch: [30/55810, loss: 5.5755]\n",
            "Epoch [6/15], batch: [31/55810, loss: 7.5323]\n",
            "Epoch [6/15], batch: [32/55810, loss: 8.0918]\n",
            "Epoch [6/15], batch: [33/55810, loss: 5.8852]\n",
            "Epoch [6/15], batch: [34/55810, loss: 6.8520]\n",
            "Epoch [6/15], batch: [35/55810, loss: 6.1313]\n",
            "Epoch [6/15], batch: [36/55810, loss: 7.4124]\n",
            "Epoch [6/15], batch: [37/55810, loss: 6.7901]\n",
            "Epoch [6/15], batch: [38/55810, loss: 4.8631]\n",
            "Epoch [6/15], batch: [39/55810, loss: 4.3314]\n",
            "Epoch [6/15], batch: [40/55810, loss: 6.8079]\n",
            "Epoch [6/15], batch: [41/55810, loss: 3.5704]\n",
            "Epoch [6/15], batch: [42/55810, loss: 4.9329]\n",
            "Epoch [6/15], batch: [43/55810, loss: 6.9360]\n",
            "Epoch [6/15], batch: [44/55810, loss: 7.5678]\n",
            "Epoch [6/15], batch: [45/55810, loss: 4.2830]\n",
            "Epoch [6/15], batch: [46/55810, loss: 6.4858]\n",
            "Epoch [6/15], batch: [47/55810, loss: 7.4292]\n",
            "Epoch [6/15], batch: [48/55810, loss: 5.9567]\n",
            "Epoch [6/15], batch: [49/55810, loss: 5.0917]\n",
            "Epoch [6/15], batch: [50/55810, loss: 6.5069]\n",
            "Epoch [6/15], batch: [51/55810, loss: 4.9999]\n",
            "Epoch [6/15], batch: [52/55810, loss: 7.7093]\n",
            "Epoch [6/15], batch: [53/55810, loss: 5.6859]\n",
            "Epoch [6/15], batch: [54/55810, loss: 6.2970]\n",
            "Epoch [6/15], batch: [55/55810, loss: 6.2642]\n",
            "Epoch [6/15], batch: [56/55810, loss: 4.4119]\n",
            "Epoch [6/15], batch: [57/55810, loss: 6.7261]\n",
            "Epoch [6/15], batch: [58/55810, loss: 4.7575]\n",
            "Epoch [6/15], batch: [59/55810, loss: 7.5914]\n",
            "Epoch [6/15], batch: [60/55810, loss: 5.9972]\n",
            "Epoch [6/15], batch: [61/55810, loss: 7.5544]\n",
            "Epoch [6/15], batch: [62/55810, loss: 6.7472]\n",
            "Epoch [6/15], batch: [63/55810, loss: 5.7307]\n",
            "Epoch [6/15], batch: [64/55810, loss: 5.9154]\n",
            "Epoch [6/15], batch: [65/55810, loss: 6.4476]\n",
            "Epoch [6/15], batch: [66/55810, loss: 6.4963]\n",
            "Epoch [6/15], batch: [67/55810, loss: 8.1062]\n",
            "Epoch [6/15], batch: [68/55810, loss: 6.7588]\n",
            "Epoch [6/15], batch: [69/55810, loss: 7.0600]\n",
            "Epoch [6/15], batch: [70/55810, loss: 6.0880]\n",
            "Epoch [6/15], batch: [71/55810, loss: 6.8499]\n",
            "Epoch [6/15], batch: [72/55810, loss: 5.6804]\n",
            "Epoch [6/15], batch: [73/55810, loss: 5.1899]\n",
            "Epoch [6/15], batch: [74/55810, loss: 7.5046]\n",
            "Epoch [6/15], batch: [75/55810, loss: 5.9362]\n",
            "Epoch [6/15], batch: [76/55810, loss: 6.7797]\n",
            "Epoch [6/15], batch: [77/55810, loss: 8.1130]\n",
            "Epoch [6/15], batch: [78/55810, loss: 4.3868]\n",
            "Epoch [6/15], batch: [79/55810, loss: 3.3223]\n",
            "Epoch [6/15], batch: [80/55810, loss: 6.5985]\n",
            "Epoch [6/15], batch: [81/55810, loss: 7.7432]\n",
            "Epoch [6/15], batch: [82/55810, loss: 6.9012]\n",
            "Epoch [6/15], batch: [83/55810, loss: 6.8162]\n",
            "Epoch [6/15], batch: [84/55810, loss: 5.1694]\n",
            "Epoch [6/15], batch: [85/55810, loss: 7.4912]\n",
            "Epoch [6/15], batch: [86/55810, loss: 6.1195]\n",
            "Epoch [6/15], batch: [87/55810, loss: 8.9485]\n",
            "Epoch [6/15], batch: [88/55810, loss: 8.6967]\n",
            "Epoch [6/15], batch: [89/55810, loss: 6.7009]\n",
            "Epoch [6/15], batch: [90/55810, loss: 7.3201]\n",
            "Epoch [6/15], batch: [91/55810, loss: 7.2179]\n",
            "Epoch [6/15], batch: [92/55810, loss: 8.7312]\n",
            "Epoch [6/15], batch: [93/55810, loss: 6.2150]\n",
            "Epoch [6/15], batch: [94/55810, loss: 7.5098]\n",
            "Epoch [6/15], batch: [95/55810, loss: 5.7363]\n",
            "Epoch [6/15], batch: [96/55810, loss: 5.3210]\n",
            "Epoch [6/15], batch: [97/55810, loss: 5.0348]\n",
            "Epoch [6/15], batch: [98/55810, loss: 4.9866]\n",
            "Epoch [6/15], batch: [99/55810, loss: 6.8157]\n",
            "Epoch [6/15], batch: [100/55810, loss: 7.8691]\n",
            "Epoch [6/15], batch: [101/55810, loss: 5.1549]\n",
            "Epoch [6/15], batch: [102/55810, loss: 5.5861]\n",
            "Epoch [6/15], batch: [103/55810, loss: 5.9552]\n",
            "Epoch [6/15], batch: [104/55810, loss: 6.4328]\n",
            "Epoch [6/15], batch: [105/55810, loss: 5.1131]\n",
            "Epoch [6/15], batch: [106/55810, loss: 5.4117]\n",
            "Epoch [6/15], batch: [107/55810, loss: 6.1265]\n",
            "Epoch [6/15], batch: [108/55810, loss: 6.2492]\n",
            "Epoch [6/15], batch: [109/55810, loss: 6.4583]\n",
            "Epoch [6/15], batch: [110/55810, loss: 6.7481]\n",
            "Epoch [6/15], batch: [111/55810, loss: 4.8721]\n",
            "Epoch [6/15], batch: [112/55810, loss: 5.9160]\n",
            "Epoch [6/15], batch: [113/55810, loss: 6.0648]\n",
            "Epoch [6/15], batch: [114/55810, loss: 5.9508]\n",
            "Epoch [6/15], batch: [115/55810, loss: 6.7619]\n",
            "Epoch [6/15], batch: [116/55810, loss: 7.4107]\n",
            "Epoch [6/15], batch: [117/55810, loss: 4.9481]\n",
            "Epoch [6/15], batch: [118/55810, loss: 5.0678]\n",
            "Epoch [6/15], batch: [119/55810, loss: 6.8045]\n",
            "Epoch [6/15], batch: [120/55810, loss: 6.4841]\n",
            "Epoch [6/15], batch: [121/55810, loss: 4.5295]\n",
            "Epoch [6/15], batch: [122/55810, loss: 6.7151]\n",
            "Epoch [6/15], batch: [123/55810, loss: 4.5837]\n",
            "Epoch [6/15], batch: [124/55810, loss: 7.0877]\n",
            "Epoch [6/15], batch: [125/55810, loss: 6.4831]\n",
            "Epoch [6/15], batch: [126/55810, loss: 6.0568]\n",
            "Epoch [6/15], batch: [127/55810, loss: 5.9602]\n",
            "Epoch [6/15], batch: [128/55810, loss: 7.2317]\n",
            "Epoch [6/15], batch: [129/55810, loss: 6.7262]\n",
            "Epoch [6/15], batch: [130/55810, loss: 7.3229]\n",
            "Epoch [6/15], batch: [131/55810, loss: 4.4787]\n",
            "Epoch [6/15], batch: [132/55810, loss: 6.9638]\n",
            "Epoch [6/15], batch: [133/55810, loss: 6.4106]\n",
            "Epoch [6/15], batch: [134/55810, loss: 7.6605]\n",
            "Epoch [6/15], batch: [135/55810, loss: 7.5200]\n",
            "Epoch [6/15], batch: [136/55810, loss: 5.8711]\n",
            "Epoch [6/15], batch: [137/55810, loss: 4.4154]\n",
            "Epoch [6/15], batch: [138/55810, loss: 6.2973]\n",
            "Epoch [6/15], batch: [139/55810, loss: 7.5898]\n",
            "Epoch [6/15], batch: [140/55810, loss: 6.6453]\n",
            "Epoch [6/15], batch: [141/55810, loss: 5.8948]\n",
            "Epoch [6/15], batch: [142/55810, loss: 6.6302]\n",
            "Epoch [6/15], batch: [143/55810, loss: 5.8653]\n",
            "Epoch [6/15], batch: [144/55810, loss: 6.7754]\n",
            "Epoch [6/15], batch: [145/55810, loss: 6.1918]\n",
            "Epoch [6/15], batch: [146/55810, loss: 7.3934]\n",
            "Epoch [6/15], batch: [147/55810, loss: 6.2353]\n",
            "Epoch [6/15], batch: [148/55810, loss: 5.8770]\n",
            "Epoch [6/15], batch: [149/55810, loss: 6.1414]\n",
            "Epoch [6/15], batch: [150/55810, loss: 7.0777]\n",
            "Epoch [6/15], batch: [151/55810, loss: 6.5038]\n",
            "Epoch [6/15], batch: [152/55810, loss: 7.5590]\n",
            "Epoch [6/15], batch: [153/55810, loss: 7.4448]\n",
            "Epoch [6/15], batch: [154/55810, loss: 7.0836]\n",
            "Epoch [6/15], batch: [155/55810, loss: 8.0458]\n",
            "Epoch [6/15], batch: [156/55810, loss: 7.5605]\n",
            "Epoch [6/15], batch: [157/55810, loss: 5.1598]\n",
            "Epoch [6/15], batch: [158/55810, loss: 4.0828]\n",
            "Epoch [6/15], batch: [159/55810, loss: 7.0639]\n",
            "Epoch [6/15], batch: [160/55810, loss: 6.7718]\n",
            "Epoch [6/15], batch: [161/55810, loss: 6.5385]\n",
            "Epoch [6/15], batch: [162/55810, loss: 6.0059]\n",
            "Epoch [6/15], batch: [163/55810, loss: 5.4776]\n",
            "Epoch [6/15], batch: [164/55810, loss: 6.2911]\n",
            "Epoch [6/15], batch: [165/55810, loss: 6.8925]\n",
            "Epoch [6/15], batch: [166/55810, loss: 5.5630]\n",
            "Epoch [6/15], batch: [167/55810, loss: 6.4276]\n",
            "Epoch [6/15], batch: [168/55810, loss: 5.8156]\n",
            "Epoch [6/15], batch: [169/55810, loss: 6.8934]\n",
            "Epoch [6/15], batch: [170/55810, loss: 8.0567]\n",
            "Epoch [6/15], batch: [171/55810, loss: 6.6739]\n",
            "Epoch [6/15], batch: [172/55810, loss: 8.3858]\n",
            "Epoch [6/15], batch: [173/55810, loss: 7.2653]\n",
            "Epoch [6/15], batch: [174/55810, loss: 5.8645]\n",
            "Epoch [6/15], batch: [175/55810, loss: 6.5718]\n",
            "Epoch [6/15], batch: [176/55810, loss: 6.9070]\n",
            "Epoch [6/15], batch: [177/55810, loss: 5.8927]\n",
            "Epoch [6/15], batch: [178/55810, loss: 7.4152]\n",
            "Epoch [6/15], batch: [179/55810, loss: 5.4998]\n",
            "Epoch [6/15], batch: [180/55810, loss: 7.1796]\n",
            "Epoch [6/15], batch: [181/55810, loss: 7.1335]\n",
            "Epoch [6/15], batch: [182/55810, loss: 6.4512]\n",
            "Epoch [6/15], batch: [183/55810, loss: 3.5797]\n",
            "Epoch [6/15], batch: [184/55810, loss: 4.7747]\n",
            "Epoch [6/15], batch: [185/55810, loss: 5.0583]\n",
            "Epoch [6/15], batch: [186/55810, loss: 6.9681]\n",
            "Epoch [6/15], batch: [187/55810, loss: 6.0138]\n",
            "Epoch [6/15], batch: [188/55810, loss: 7.7765]\n",
            "Epoch [6/15], batch: [189/55810, loss: 7.7666]\n",
            "Epoch [6/15], batch: [190/55810, loss: 7.6968]\n",
            "Epoch [6/15], batch: [191/55810, loss: 7.7814]\n",
            "Epoch [6/15], batch: [192/55810, loss: 8.6658]\n",
            "Epoch [6/15], batch: [193/55810, loss: 6.6597]\n",
            "Epoch [6/15], batch: [194/55810, loss: 8.0352]\n",
            "Epoch [6/15], batch: [195/55810, loss: 7.6022]\n",
            "Epoch [6/15], batch: [196/55810, loss: 7.3148]\n",
            "Epoch [6/15], batch: [197/55810, loss: 8.6418]\n",
            "Epoch [6/15], batch: [198/55810, loss: 7.5604]\n",
            "Epoch [6/15], batch: [199/55810, loss: 8.3517]\n",
            "Epoch [6/15], batch: [200/55810, loss: 6.2625]\n",
            "Epoch [6/15], batch: [201/55810, loss: 6.2482]\n",
            "Epoch [6/15], batch: [202/55810, loss: 7.6151]\n",
            "Epoch [6/15], batch: [203/55810, loss: 6.4683]\n",
            "Epoch [6/15], batch: [204/55810, loss: 6.9025]\n",
            "Epoch [6/15], batch: [205/55810, loss: 7.6243]\n",
            "Epoch [6/15], batch: [206/55810, loss: 7.0952]\n",
            "Epoch [6/15], batch: [207/55810, loss: 6.5100]\n",
            "Epoch [6/15], batch: [208/55810, loss: 7.1914]\n",
            "Epoch [6/15], batch: [209/55810, loss: 8.1406]\n",
            "Epoch [6/15], batch: [210/55810, loss: 5.4647]\n",
            "Epoch [6/15], batch: [211/55810, loss: 5.5697]\n",
            "Epoch [6/15], batch: [212/55810, loss: 7.7968]\n",
            "Epoch [6/15], batch: [213/55810, loss: 7.8828]\n",
            "Epoch [6/15], batch: [214/55810, loss: 8.4960]\n",
            "Epoch [6/15], batch: [215/55810, loss: 7.8913]\n",
            "Epoch [6/15], batch: [216/55810, loss: 7.2710]\n",
            "Epoch [6/15], batch: [217/55810, loss: 5.8613]\n",
            "Epoch [6/15], batch: [218/55810, loss: 7.9578]\n",
            "Epoch [6/15], batch: [219/55810, loss: 7.1208]\n",
            "Epoch [6/15], batch: [220/55810, loss: 7.7936]\n",
            "Epoch [6/15], batch: [221/55810, loss: 8.5432]\n",
            "Epoch [6/15], batch: [222/55810, loss: 7.7893]\n",
            "Epoch [6/15], batch: [223/55810, loss: 7.4257]\n",
            "Epoch [6/15], batch: [224/55810, loss: 6.7004]\n",
            "Epoch [6/15], batch: [225/55810, loss: 8.0412]\n",
            "Epoch [6/15], batch: [226/55810, loss: 6.6875]\n",
            "Epoch [6/15], batch: [227/55810, loss: 5.7101]\n",
            "Epoch [6/15], batch: [228/55810, loss: 7.0161]\n",
            "Epoch [6/15], batch: [229/55810, loss: 7.3004]\n",
            "Epoch [6/15], batch: [230/55810, loss: 7.0827]\n",
            "Epoch [6/15], batch: [231/55810, loss: 5.8019]\n",
            "Epoch [6/15], batch: [232/55810, loss: 5.6314]\n",
            "Epoch [6/15], batch: [233/55810, loss: 7.5607]\n",
            "Epoch [6/15], batch: [234/55810, loss: 5.9019]\n",
            "Epoch [6/15], batch: [235/55810, loss: 6.4071]\n",
            "Epoch [6/15], batch: [236/55810, loss: 7.7402]\n",
            "Epoch [6/15], batch: [237/55810, loss: 7.9717]\n",
            "Epoch [6/15], batch: [238/55810, loss: 5.5299]\n",
            "Epoch [6/15], batch: [239/55810, loss: 7.4748]\n",
            "Epoch [6/15], batch: [240/55810, loss: 7.5791]\n",
            "Epoch [6/15], batch: [241/55810, loss: 9.2633]\n",
            "Epoch [6/15], batch: [242/55810, loss: 6.2258]\n",
            "Epoch [6/15], batch: [243/55810, loss: 6.8446]\n",
            "Epoch [6/15], batch: [244/55810, loss: 8.3175]\n",
            "Epoch [6/15], batch: [245/55810, loss: 6.1315]\n",
            "Epoch [6/15], batch: [246/55810, loss: 7.4076]\n",
            "Epoch [6/15], batch: [247/55810, loss: 7.2066]\n",
            "Epoch [6/15], batch: [248/55810, loss: 7.3755]\n",
            "Epoch [6/15], batch: [249/55810, loss: 9.1661]\n",
            "Epoch [6/15], batch: [250/55810, loss: 8.1148]\n",
            "Epoch [6/15], batch: [251/55810, loss: 7.3431]\n",
            "Epoch [6/15], batch: [252/55810, loss: 6.7244]\n",
            "Epoch [6/15], batch: [253/55810, loss: 7.5101]\n",
            "Epoch [6/15], batch: [254/55810, loss: 8.6149]\n",
            "Epoch [6/15], batch: [255/55810, loss: 7.2164]\n",
            "Epoch [6/15], batch: [256/55810, loss: 7.3948]\n",
            "Epoch [6/15], batch: [257/55810, loss: 6.6341]\n",
            "Epoch [6/15], batch: [258/55810, loss: 6.4785]\n",
            "Epoch [6/15], batch: [259/55810, loss: 8.1526]\n",
            "Epoch [6/15], batch: [260/55810, loss: 7.3097]\n",
            "Epoch [6/15], batch: [261/55810, loss: 7.7004]\n",
            "Epoch [6/15], batch: [262/55810, loss: 8.0363]\n",
            "Epoch [6/15], batch: [263/55810, loss: 8.2159]\n",
            "Epoch [6/15], batch: [264/55810, loss: 8.9545]\n",
            "Epoch [6/15], batch: [265/55810, loss: 8.7742]\n",
            "Epoch [6/15], batch: [266/55810, loss: 6.4939]\n",
            "Epoch [6/15], batch: [267/55810, loss: 6.5208]\n",
            "Epoch [6/15], batch: [268/55810, loss: 7.1745]\n",
            "Epoch [6/15], batch: [269/55810, loss: 6.8794]\n",
            "Epoch [6/15], batch: [270/55810, loss: 7.8174]\n",
            "Epoch [6/15], batch: [271/55810, loss: 6.2014]\n",
            "Epoch [6/15], batch: [272/55810, loss: 7.2015]\n",
            "Epoch [6/15], batch: [273/55810, loss: 7.2026]\n",
            "Epoch [6/15], batch: [274/55810, loss: 7.3175]\n",
            "Epoch [6/15], batch: [275/55810, loss: 6.3664]\n",
            "Epoch [6/15], batch: [276/55810, loss: 5.3246]\n",
            "Epoch [6/15], batch: [277/55810, loss: 7.0422]\n",
            "Epoch [6/15], batch: [278/55810, loss: 5.5127]\n",
            "Epoch [6/15], batch: [279/55810, loss: 7.7487]\n",
            "Epoch [6/15], batch: [280/55810, loss: 7.2083]\n",
            "Epoch [6/15], batch: [281/55810, loss: 5.6771]\n",
            "Epoch [6/15], batch: [282/55810, loss: 6.8173]\n",
            "Epoch [6/15], batch: [283/55810, loss: 7.5340]\n",
            "Epoch [6/15], batch: [284/55810, loss: 5.4752]\n",
            "Epoch [6/15], batch: [285/55810, loss: 6.4333]\n",
            "Epoch [6/15], batch: [286/55810, loss: 5.7539]\n",
            "Epoch [6/15], batch: [287/55810, loss: 4.4206]\n",
            "Epoch [6/15], batch: [288/55810, loss: 6.8088]\n",
            "Epoch [6/15], batch: [289/55810, loss: 5.3354]\n",
            "Epoch [6/15], batch: [290/55810, loss: 7.2921]\n",
            "Epoch [6/15], batch: [291/55810, loss: 6.0395]\n",
            "Epoch [6/15], batch: [292/55810, loss: 7.2375]\n",
            "Epoch [6/15], batch: [293/55810, loss: 6.7511]\n",
            "Epoch [6/15], batch: [294/55810, loss: 8.3889]\n",
            "Epoch [6/15], batch: [295/55810, loss: 6.5332]\n",
            "Epoch [6/15], batch: [296/55810, loss: 7.5264]\n",
            "Epoch [6/15], batch: [297/55810, loss: 5.9734]\n",
            "Epoch [6/15], batch: [298/55810, loss: 6.2806]\n",
            "Epoch [6/15], batch: [299/55810, loss: 5.6078]\n",
            "Epoch [6/15], batch: [300/55810, loss: 6.7929]\n",
            "Epoch [6/15], batch: [301/55810, loss: 7.9431]\n",
            "Epoch [6/15], batch: [302/55810, loss: 6.9401]\n",
            "Epoch [6/15], batch: [303/55810, loss: 7.4635]\n",
            "Epoch [6/15], batch: [304/55810, loss: 7.4730]\n",
            "Epoch [6/15], batch: [305/55810, loss: 6.4540]\n",
            "Epoch [6/15], batch: [306/55810, loss: 5.9602]\n",
            "Epoch [6/15], batch: [307/55810, loss: 5.9299]\n",
            "Epoch [6/15], batch: [308/55810, loss: 6.5411]\n",
            "Epoch [6/15], batch: [309/55810, loss: 4.6966]\n",
            "Epoch [6/15], batch: [310/55810, loss: 6.5616]\n",
            "Epoch [6/15], batch: [311/55810, loss: 5.9799]\n",
            "Epoch [6/15], batch: [312/55810, loss: 4.5866]\n",
            "Epoch [6/15], batch: [313/55810, loss: 5.4027]\n",
            "Epoch [6/15], batch: [314/55810, loss: 6.2691]\n",
            "Epoch [6/15], batch: [315/55810, loss: 6.5670]\n",
            "Epoch [6/15], batch: [316/55810, loss: 5.5755]\n",
            "Epoch [6/15], batch: [317/55810, loss: 6.1326]\n",
            "Epoch [6/15], batch: [318/55810, loss: 6.5296]\n",
            "Epoch [6/15], batch: [319/55810, loss: 6.9961]\n",
            "Epoch [6/15], batch: [320/55810, loss: 5.9653]\n",
            "Epoch [6/15], batch: [321/55810, loss: 7.4845]\n",
            "Epoch [6/15], batch: [322/55810, loss: 6.8613]\n",
            "Epoch [6/15], batch: [323/55810, loss: 6.7911]\n",
            "Epoch [6/15], batch: [324/55810, loss: 6.3766]\n",
            "Epoch [6/15], batch: [325/55810, loss: 7.8927]\n",
            "Epoch [6/15], batch: [326/55810, loss: 6.1674]\n",
            "Epoch [6/15], batch: [327/55810, loss: 5.4439]\n",
            "Epoch [6/15], batch: [328/55810, loss: 7.6666]\n",
            "Epoch [6/15], batch: [329/55810, loss: 6.1619]\n",
            "Epoch [6/15], batch: [330/55810, loss: 6.5255]\n",
            "Epoch [6/15], batch: [331/55810, loss: 5.7595]\n",
            "Epoch [6/15], batch: [332/55810, loss: 6.1884]\n",
            "Epoch [6/15], batch: [333/55810, loss: 7.2923]\n",
            "Epoch [6/15], batch: [334/55810, loss: 6.5448]\n",
            "Epoch [6/15], batch: [335/55810, loss: 5.9029]\n",
            "Epoch [6/15], batch: [336/55810, loss: 5.2914]\n",
            "Epoch [6/15], batch: [337/55810, loss: 4.3369]\n",
            "Epoch [6/15], batch: [338/55810, loss: 5.4315]\n",
            "Epoch [6/15], batch: [339/55810, loss: 7.0524]\n",
            "Epoch [6/15], batch: [340/55810, loss: 6.5899]\n",
            "Epoch [6/15], batch: [341/55810, loss: 6.7067]\n",
            "Epoch [6/15], batch: [342/55810, loss: 7.4144]\n",
            "Epoch [6/15], batch: [343/55810, loss: 5.7491]\n",
            "Epoch [6/15], batch: [344/55810, loss: 6.6906]\n",
            "Epoch [6/15], batch: [345/55810, loss: 7.9489]\n",
            "Epoch [6/15], batch: [346/55810, loss: 5.4220]\n",
            "Epoch [6/15], batch: [347/55810, loss: 7.4196]\n",
            "Epoch [6/15], batch: [348/55810, loss: 7.6901]\n",
            "Epoch [6/15], batch: [349/55810, loss: 6.5178]\n",
            "Epoch [6/15], batch: [350/55810, loss: 4.8569]\n",
            "Epoch [6/15], batch: [351/55810, loss: 4.3570]\n",
            "Epoch [6/15], batch: [352/55810, loss: 6.9168]\n",
            "Epoch [6/15], batch: [353/55810, loss: 6.6385]\n",
            "Epoch [6/15], batch: [354/55810, loss: 4.5485]\n",
            "Epoch [6/15], batch: [355/55810, loss: 5.2397]\n",
            "Epoch [6/15], batch: [356/55810, loss: 5.7615]\n",
            "Epoch [6/15], batch: [357/55810, loss: 7.2842]\n",
            "Epoch [6/15], batch: [358/55810, loss: 6.9755]\n",
            "Epoch [6/15], batch: [359/55810, loss: 6.7900]\n",
            "Epoch [6/15], batch: [360/55810, loss: 4.2297]\n",
            "Epoch [6/15], batch: [361/55810, loss: 5.8373]\n",
            "Epoch [6/15], batch: [362/55810, loss: 4.7574]\n",
            "Epoch [6/15], batch: [363/55810, loss: 6.0280]\n",
            "Epoch [6/15], batch: [364/55810, loss: 6.4326]\n",
            "Epoch [6/15], batch: [365/55810, loss: 8.2185]\n",
            "Epoch [6/15], batch: [366/55810, loss: 7.2041]\n",
            "Epoch [6/15], batch: [367/55810, loss: 7.3919]\n",
            "Epoch [6/15], batch: [368/55810, loss: 7.8189]\n",
            "Epoch [6/15], batch: [369/55810, loss: 7.9856]\n",
            "Epoch [6/15], batch: [370/55810, loss: 7.3787]\n",
            "Epoch [6/15], batch: [371/55810, loss: 6.4786]\n",
            "Epoch [6/15], batch: [372/55810, loss: 7.7978]\n",
            "Epoch [6/15], batch: [373/55810, loss: 5.7976]\n",
            "Epoch [6/15], batch: [374/55810, loss: 7.9862]\n",
            "Epoch [6/15], batch: [375/55810, loss: 5.2242]\n",
            "Epoch [6/15], batch: [376/55810, loss: 6.9256]\n",
            "Epoch [6/15], batch: [377/55810, loss: 6.1279]\n",
            "Epoch [6/15], batch: [378/55810, loss: 6.6391]\n",
            "Epoch [6/15], batch: [379/55810, loss: 8.2067]\n",
            "Epoch [6/15], batch: [380/55810, loss: 6.4592]\n",
            "Epoch [6/15], batch: [381/55810, loss: 6.8596]\n",
            "Epoch [6/15], batch: [382/55810, loss: 8.3580]\n",
            "Epoch [6/15], batch: [383/55810, loss: 6.1376]\n",
            "Epoch [6/15], batch: [384/55810, loss: 7.3356]\n",
            "Epoch [6/15], batch: [385/55810, loss: 7.2925]\n",
            "Epoch [6/15], batch: [386/55810, loss: 7.4865]\n",
            "Epoch [6/15], batch: [387/55810, loss: 9.3599]\n",
            "Epoch [6/15], batch: [388/55810, loss: 6.0522]\n",
            "Epoch [6/15], batch: [389/55810, loss: 6.5985]\n",
            "Epoch [6/15], batch: [390/55810, loss: 5.3750]\n",
            "Epoch [6/15], batch: [391/55810, loss: 4.5053]\n",
            "Epoch [6/15], batch: [392/55810, loss: 6.5244]\n",
            "Epoch [6/15], batch: [393/55810, loss: 7.0369]\n",
            "Epoch [6/15], batch: [394/55810, loss: 6.1872]\n",
            "Epoch [6/15], batch: [395/55810, loss: 6.3756]\n",
            "Epoch [6/15], batch: [396/55810, loss: 5.5578]\n",
            "Epoch [6/15], batch: [397/55810, loss: 3.8680]\n",
            "Epoch [6/15], batch: [398/55810, loss: 5.5797]\n",
            "Epoch [6/15], batch: [399/55810, loss: 5.8959]\n",
            "Epoch [6/15], batch: [400/55810, loss: 6.1124]\n",
            "Epoch [6/15], batch: [401/55810, loss: 6.6754]\n",
            "Epoch [6/15], batch: [402/55810, loss: 7.1958]\n",
            "Epoch [6/15], batch: [403/55810, loss: 7.8561]\n",
            "Epoch [6/15], batch: [404/55810, loss: 6.6417]\n",
            "Epoch [6/15], batch: [405/55810, loss: 8.1669]\n",
            "Epoch [6/15], batch: [406/55810, loss: 6.6104]\n",
            "Epoch [6/15], batch: [407/55810, loss: 5.7351]\n",
            "Epoch [6/15], batch: [408/55810, loss: 5.9508]\n",
            "Epoch [6/15], batch: [409/55810, loss: 5.4947]\n",
            "Epoch [6/15], batch: [410/55810, loss: 5.9159]\n",
            "Epoch [6/15], batch: [411/55810, loss: 8.3551]\n",
            "Epoch [6/15], batch: [412/55810, loss: 7.5204]\n",
            "Epoch [6/15], batch: [413/55810, loss: 7.7882]\n",
            "Epoch [6/15], batch: [414/55810, loss: 7.1545]\n",
            "Epoch [6/15], batch: [415/55810, loss: 6.8364]\n",
            "Epoch [6/15], batch: [416/55810, loss: 7.2042]\n",
            "Epoch [6/15], batch: [417/55810, loss: 7.2844]\n",
            "Epoch [6/15], batch: [418/55810, loss: 7.7738]\n",
            "Epoch [6/15], batch: [419/55810, loss: 9.0813]\n",
            "Epoch [6/15], batch: [420/55810, loss: 7.3669]\n",
            "Epoch [6/15], batch: [421/55810, loss: 5.8769]\n",
            "Epoch [6/15], batch: [422/55810, loss: 5.9449]\n",
            "Epoch [6/15], batch: [423/55810, loss: 6.3997]\n",
            "Epoch [6/15], batch: [424/55810, loss: 5.3325]\n",
            "Epoch [6/15], batch: [425/55810, loss: 6.8403]\n",
            "Epoch [6/15], batch: [426/55810, loss: 5.2674]\n",
            "Epoch [6/15], batch: [427/55810, loss: 8.2801]\n",
            "Epoch [6/15], batch: [428/55810, loss: 5.5493]\n",
            "Epoch [6/15], batch: [429/55810, loss: 5.3812]\n",
            "Epoch [6/15], batch: [430/55810, loss: 4.6173]\n",
            "Epoch [6/15], batch: [431/55810, loss: 7.8285]\n",
            "Epoch [6/15], batch: [432/55810, loss: 5.9971]\n",
            "Epoch [6/15], batch: [433/55810, loss: 6.4974]\n",
            "Epoch [6/15], batch: [434/55810, loss: 7.6476]\n",
            "Epoch [6/15], batch: [435/55810, loss: 6.5399]\n",
            "Epoch [6/15], batch: [436/55810, loss: 6.2026]\n",
            "Epoch [6/15], batch: [437/55810, loss: 6.8853]\n",
            "Epoch [6/15], batch: [438/55810, loss: 5.6389]\n",
            "Epoch [6/15], batch: [439/55810, loss: 6.3563]\n",
            "Epoch [6/15], batch: [440/55810, loss: 5.8876]\n",
            "Epoch [6/15], batch: [441/55810, loss: 5.9137]\n",
            "Epoch [6/15], batch: [442/55810, loss: 5.3374]\n",
            "Epoch [6/15], batch: [443/55810, loss: 7.3643]\n",
            "Epoch [6/15], batch: [444/55810, loss: 6.5694]\n",
            "Epoch [6/15], batch: [445/55810, loss: 5.8423]\n",
            "Epoch [6/15], batch: [446/55810, loss: 6.1880]\n",
            "Epoch [6/15], batch: [447/55810, loss: 5.3985]\n",
            "Epoch [6/15], batch: [448/55810, loss: 6.6635]\n",
            "Epoch [6/15], batch: [449/55810, loss: 5.2326]\n",
            "Epoch [6/15], batch: [450/55810, loss: 7.7315]\n",
            "Epoch [6/15], batch: [451/55810, loss: 7.3823]\n",
            "Epoch [6/15], batch: [452/55810, loss: 7.7915]\n",
            "Epoch [6/15], batch: [453/55810, loss: 5.1463]\n",
            "Epoch [6/15], batch: [454/55810, loss: 5.6308]\n",
            "Epoch [6/15], batch: [455/55810, loss: 5.8789]\n",
            "Epoch [6/15], batch: [456/55810, loss: 6.6681]\n",
            "Epoch [6/15], batch: [457/55810, loss: 6.3112]\n",
            "Epoch [6/15], batch: [458/55810, loss: 6.2617]\n",
            "Epoch [6/15], batch: [459/55810, loss: 6.1096]\n",
            "Epoch [6/15], batch: [460/55810, loss: 6.0539]\n",
            "Epoch [6/15], batch: [461/55810, loss: 7.0929]\n",
            "Epoch [6/15], batch: [462/55810, loss: 6.8958]\n",
            "Epoch [6/15], batch: [463/55810, loss: 6.2285]\n",
            "Epoch [6/15], batch: [464/55810, loss: 6.6023]\n",
            "Epoch [6/15], batch: [465/55810, loss: 7.1094]\n",
            "Epoch [6/15], batch: [466/55810, loss: 6.3415]\n",
            "Epoch [6/15], batch: [467/55810, loss: 7.9837]\n",
            "Epoch [6/15], batch: [468/55810, loss: 7.5215]\n",
            "Epoch [6/15], batch: [469/55810, loss: 6.4352]\n",
            "Epoch [6/15], batch: [470/55810, loss: 7.0858]\n",
            "Epoch [6/15], batch: [471/55810, loss: 8.3916]\n",
            "Epoch [6/15], batch: [472/55810, loss: 7.3829]\n",
            "Epoch [6/15], batch: [473/55810, loss: 7.6977]\n",
            "Epoch [6/15], batch: [474/55810, loss: 6.6552]\n",
            "Epoch [6/15], batch: [475/55810, loss: 7.5709]\n",
            "Epoch [6/15], batch: [476/55810, loss: 8.8501]\n",
            "Epoch [6/15], batch: [477/55810, loss: 6.9217]\n",
            "Epoch [6/15], batch: [478/55810, loss: 7.1914]\n",
            "Epoch [6/15], batch: [479/55810, loss: 5.8833]\n",
            "Epoch [6/15], batch: [480/55810, loss: 4.5782]\n",
            "Epoch [6/15], batch: [481/55810, loss: 5.3983]\n",
            "Epoch [6/15], batch: [482/55810, loss: 6.6201]\n",
            "Epoch [6/15], batch: [483/55810, loss: 8.1080]\n",
            "Epoch [6/15], batch: [484/55810, loss: 6.6866]\n",
            "Epoch [6/15], batch: [485/55810, loss: 7.5934]\n",
            "Epoch [6/15], batch: [486/55810, loss: 6.6798]\n",
            "Epoch [6/15], batch: [487/55810, loss: 6.3520]\n",
            "Epoch [6/15], batch: [488/55810, loss: 6.1204]\n",
            "Epoch [6/15], batch: [489/55810, loss: 6.3787]\n",
            "Epoch [6/15], batch: [490/55810, loss: 7.2294]\n",
            "Epoch [6/15], batch: [491/55810, loss: 6.8496]\n",
            "Epoch [6/15], batch: [492/55810, loss: 7.6966]\n",
            "Epoch [6/15], batch: [493/55810, loss: 6.6011]\n",
            "Epoch [6/15], batch: [494/55810, loss: 6.6694]\n",
            "Epoch [6/15], batch: [495/55810, loss: 7.0503]\n",
            "Epoch [6/15], batch: [496/55810, loss: 5.3366]\n",
            "Epoch [6/15], batch: [497/55810, loss: 7.1314]\n",
            "Epoch [6/15], batch: [498/55810, loss: 6.4536]\n",
            "Epoch [6/15], batch: [499/55810, loss: 6.7972]\n",
            "Epoch [6/15], batch: [500/55810, loss: 5.7054]\n",
            "Epoch [6/15], batch: [501/55810, loss: 5.8738]\n",
            "Epoch [6/15], batch: [502/55810, loss: 7.6746]\n",
            "Epoch [6/15], batch: [503/55810, loss: 7.3546]\n",
            "Epoch [6/15], batch: [504/55810, loss: 6.3996]\n",
            "Epoch [6/15], batch: [505/55810, loss: 6.5502]\n",
            "Epoch [6/15], batch: [506/55810, loss: 7.8343]\n",
            "Epoch [6/15], batch: [507/55810, loss: 7.1711]\n",
            "Epoch [6/15], batch: [508/55810, loss: 7.5468]\n",
            "Epoch [6/15], batch: [509/55810, loss: 6.9146]\n",
            "Epoch [6/15], batch: [510/55810, loss: 6.5561]\n",
            "Epoch [6/15], batch: [511/55810, loss: 5.7594]\n",
            "Epoch [6/15], batch: [512/55810, loss: 6.4630]\n",
            "Epoch [6/15], batch: [513/55810, loss: 5.1963]\n",
            "Epoch [6/15], batch: [514/55810, loss: 7.4158]\n",
            "Epoch [6/15], batch: [515/55810, loss: 6.3137]\n",
            "Epoch [6/15], batch: [516/55810, loss: 6.0899]\n",
            "Epoch [6/15], batch: [517/55810, loss: 4.2205]\n",
            "Epoch [6/15], batch: [518/55810, loss: 5.5873]\n",
            "Epoch [6/15], batch: [519/55810, loss: 6.6955]\n",
            "Epoch [6/15], batch: [520/55810, loss: 6.2525]\n",
            "Epoch [6/15], batch: [521/55810, loss: 7.1395]\n",
            "Epoch [6/15], batch: [522/55810, loss: 6.4848]\n",
            "Epoch [6/15], batch: [523/55810, loss: 7.0741]\n",
            "Epoch [6/15], batch: [524/55810, loss: 7.3864]\n",
            "Epoch [6/15], batch: [525/55810, loss: 5.3591]\n",
            "Epoch [6/15], batch: [526/55810, loss: 6.7099]\n",
            "Epoch [6/15], batch: [527/55810, loss: 7.2877]\n",
            "Epoch [6/15], batch: [528/55810, loss: 5.5637]\n",
            "Epoch [6/15], batch: [529/55810, loss: 5.4544]\n",
            "Epoch [6/15], batch: [530/55810, loss: 6.1676]\n",
            "Epoch [6/15], batch: [531/55810, loss: 6.8426]\n",
            "Epoch [6/15], batch: [532/55810, loss: 5.7852]\n",
            "Epoch [6/15], batch: [533/55810, loss: 4.9724]\n",
            "Epoch [6/15], batch: [534/55810, loss: 6.9941]\n",
            "Epoch [6/15], batch: [535/55810, loss: 6.5575]\n",
            "Epoch [6/15], batch: [536/55810, loss: 6.7387]\n",
            "Epoch [6/15], batch: [537/55810, loss: 6.5314]\n",
            "Epoch [6/15], batch: [538/55810, loss: 5.9430]\n",
            "Epoch [6/15], batch: [539/55810, loss: 6.0171]\n",
            "Epoch [6/15], batch: [540/55810, loss: 6.6899]\n",
            "Epoch [6/15], batch: [541/55810, loss: 6.5910]\n",
            "Epoch [6/15], batch: [542/55810, loss: 7.5815]\n",
            "Epoch [6/15], batch: [543/55810, loss: 5.0378]\n",
            "Epoch [6/15], batch: [544/55810, loss: 6.4860]\n",
            "Epoch [6/15], batch: [545/55810, loss: 6.3655]\n",
            "Epoch [6/15], batch: [546/55810, loss: 4.7594]\n",
            "Epoch [6/15], batch: [547/55810, loss: 5.9982]\n",
            "Epoch [6/15], batch: [548/55810, loss: 5.8067]\n",
            "Epoch [6/15], batch: [549/55810, loss: 6.3625]\n",
            "Epoch [6/15], batch: [550/55810, loss: 5.7357]\n",
            "Epoch [6/15], batch: [551/55810, loss: 6.8724]\n",
            "Epoch [6/15], batch: [552/55810, loss: 5.2524]\n",
            "Epoch [6/15], batch: [553/55810, loss: 4.2212]\n",
            "Epoch [6/15], batch: [554/55810, loss: 5.5973]\n",
            "Epoch [6/15], batch: [555/55810, loss: 5.8361]\n",
            "Epoch [6/15], batch: [556/55810, loss: 6.5755]\n",
            "Epoch [6/15], batch: [557/55810, loss: 7.4112]\n",
            "Epoch [6/15], batch: [558/55810, loss: 5.5963]\n",
            "Epoch [6/15], batch: [559/55810, loss: 4.6598]\n",
            "Epoch [6/15], batch: [560/55810, loss: 7.0864]\n",
            "Epoch [6/15], batch: [561/55810, loss: 6.4066]\n",
            "Epoch [6/15], batch: [562/55810, loss: 6.6678]\n",
            "Epoch [6/15], batch: [563/55810, loss: 5.0754]\n",
            "Epoch [6/15], batch: [564/55810, loss: 5.2781]\n",
            "Epoch [6/15], batch: [565/55810, loss: 6.6383]\n",
            "Epoch [6/15], batch: [566/55810, loss: 8.8932]\n",
            "Epoch [6/15], batch: [567/55810, loss: 6.7200]\n",
            "Epoch [6/15], batch: [568/55810, loss: 6.2905]\n",
            "Epoch [6/15], batch: [569/55810, loss: 6.9458]\n",
            "Epoch [6/15], batch: [570/55810, loss: 7.6024]\n",
            "Epoch [6/15], batch: [571/55810, loss: 6.5955]\n",
            "Epoch [6/15], batch: [572/55810, loss: 6.9241]\n",
            "Epoch [6/15], batch: [573/55810, loss: 6.4231]\n",
            "Epoch [6/15], batch: [574/55810, loss: 6.9842]\n",
            "Epoch [6/15], batch: [575/55810, loss: 5.7703]\n",
            "Epoch [6/15], batch: [576/55810, loss: 5.7097]\n",
            "Epoch [6/15], batch: [577/55810, loss: 5.6677]\n",
            "Epoch [6/15], batch: [578/55810, loss: 5.3243]\n",
            "Epoch [6/15], batch: [579/55810, loss: 5.6397]\n",
            "Epoch [6/15], batch: [580/55810, loss: 7.2030]\n",
            "Epoch [6/15], batch: [581/55810, loss: 6.4972]\n",
            "Epoch [6/15], batch: [582/55810, loss: 6.6755]\n",
            "Epoch [6/15], batch: [583/55810, loss: 5.9592]\n",
            "Epoch [6/15], batch: [584/55810, loss: 7.0737]\n",
            "Epoch [6/15], batch: [585/55810, loss: 7.2809]\n",
            "Epoch [6/15], batch: [586/55810, loss: 8.0680]\n",
            "Epoch [6/15], batch: [587/55810, loss: 7.6809]\n",
            "Epoch [6/15], batch: [588/55810, loss: 6.5421]\n",
            "Epoch [6/15], batch: [589/55810, loss: 6.5363]\n",
            "Epoch [6/15], batch: [590/55810, loss: 6.6946]\n",
            "Epoch [6/15], batch: [591/55810, loss: 7.0965]\n",
            "Epoch [6/15], batch: [592/55810, loss: 7.6687]\n",
            "Epoch [6/15], batch: [593/55810, loss: 6.1388]\n",
            "Epoch [6/15], batch: [594/55810, loss: 7.2945]\n",
            "Epoch [6/15], batch: [595/55810, loss: 5.1927]\n",
            "Epoch [6/15], batch: [596/55810, loss: 6.6729]\n",
            "Epoch [6/15], batch: [597/55810, loss: 6.0245]\n",
            "Epoch [6/15], batch: [598/55810, loss: 6.2992]\n",
            "Epoch [6/15], batch: [599/55810, loss: 5.8868]\n",
            "Epoch [6/15], batch: [600/55810, loss: 5.7790]\n",
            "Epoch [6/15], batch: [601/55810, loss: 7.2357]\n",
            "Epoch [6/15], batch: [602/55810, loss: 6.6382]\n",
            "Epoch [6/15], batch: [603/55810, loss: 6.3971]\n",
            "Epoch [6/15], batch: [604/55810, loss: 5.5680]\n",
            "Epoch [6/15], batch: [605/55810, loss: 5.6750]\n",
            "Epoch [6/15], batch: [606/55810, loss: 4.7301]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW5.ptvocab.txt','r') as data:\n",
        "      print(data.read())\n",
        "      data.seek(0)\n",
        "      vocab = eval(data.read())\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/CBOW5.ptword_to_ix.txt','r') as data:\n",
        "\n",
        "      data.seek(0)\n",
        "      word_to_ix = eval(data.read())\n",
        "      print(word_to_ix)\n",
        "get_closest_word('hotel')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1IlFTRu13ai",
        "outputId": "aa05323d-4c7c-4cf1-ceda-36fcb8086311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wak', 6.224923610687256),\n",
              " ('masterpieces', 6.45383358001709),\n",
              " ('muy', 6.520915985107422),\n",
              " ('agno', 6.52716588973999),\n",
              " ('reaked', 6.556485652923584)]"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['hotel', 'room', 'copley', 'did', 'stay', 'trian' 'beautiful', 'ineffective', 'cracked']\n",
        "for word in words:\n",
        "  print(get_closest_word(word))"
      ],
      "metadata": {
        "id": "wrHteqA3cN7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 For the hotel reviews dataset, choose 3 nouns, 3 verbs, and 3 adjectives. (CBOW2 and optionally for CBOW5)\n",
        "Make sure that some nouns/verbs/adjectives occur frequently in the corpus and that others are rare. For each of the 9 chosen words, retrieve the 5 closest words according to your trained CBOW2 model.    \n",
        "\n",
        "ðŸ—’â“ List them in your report (at the end of this notebook) and comment on the performance of your model: do the neighbours the model provides make sense? Discuss.   \n"
      ],
      "metadata": {
        "id": "ulqFt2nc_Oq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Train CBOW2 with a context width of 2 (in both directions) for the Sci-Fi story dataset"
      ],
      "metadata": {
        "id": "AXmEddYd-FSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "050OoM_rx4qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('scifi_reduced.txt', 'r') as file:\n",
        "    text = file.read().split(\".\")\n",
        "\n",
        "print(text[:10])\n"
      ],
      "metadata": {
        "id": "2U1S_5Hx-Jku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8702de0e-7bdf-414a-9116-ab9a1aba75af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' A chat with the editor  i #  science fiction magazine called IF', ' The title was selected after much thought because of its brevity and on the theory it is indicative of the field and will be easy to remember', \" The tentative title that just morning and couldn't remember it until we'd had a cup of coffee, it was summarily discarded\", ' A great deal of thought and effort lias gone into the formation of this magazine', ' We have had the aid of several very talented and generous people, for which we are most grateful', ' Much is due them for their warmhearted assistance', ' And now that the bulk of the formative work is done, we will try to maintain IF as one of the finest books on the market', '  t a great public demand for our magazine', ' In short, why will you buy IF? We cannot, in honesty, say we will publish at all times the best science fiction in the field', ' That would not be true']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sci_df = pd.DataFrame(text, columns=['Sentence'])\n",
        "sci_df.head()"
      ],
      "metadata": {
        "id": "WwFTBToC0ylA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "dd67e648-f9fd-4f9e-d22e-125e0b77df72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Sentence\n",
              "0   A chat with the editor  i #  science fiction ...\n",
              "1   The title was selected after much thought bec...\n",
              "2   The tentative title that just morning and cou...\n",
              "3   A great deal of thought and effort lias gone ...\n",
              "4   We have had the aid of several very talented ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-767013b8-1943-43b0-b2a7-2319fde8958f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A chat with the editor  i #  science fiction ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The title was selected after much thought bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The tentative title that just morning and cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A great deal of thought and effort lias gone ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We have had the aid of several very talented ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-767013b8-1943-43b0-b2a7-2319fde8958f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-767013b8-1943-43b0-b2a7-2319fde8958f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-767013b8-1943-43b0-b2a7-2319fde8958f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-35085c85-2c1d-437b-8416-6579bb187da8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35085c85-2c1d-437b-8416-6579bb187da8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-35085c85-2c1d-437b-8416-6579bb187da8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sci_df"
            }
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "train_loader, test_loader, vocab, word_to_ix = create_dataloader(sci_df, 'Sentence')"
      ],
      "metadata": {
        "id": "u7fDja7i1obc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbea080d-a069-4291-cab9-d2a8ada0bc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([90, 4])\n",
            "torch.Size([90, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 128\n",
        "model = CBOW(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "num_epochs = 3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "model.to(device)\n",
        "trainig_loop(model, train_loader, optimizer, loss_func, num_epochs, device)\n",
        "\n",
        "Modelname = \"CBOW2_scifi.pt\"\n",
        "PATH  =f\"/content/drive/MyDrive/Colab Notebooks/{Modelname}\"\n",
        "\n",
        "# Save\n",
        "torch.save(model, PATH)"
      ],
      "metadata": {
        "id": "uYW7XqTL1-sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3fa89e49-8d9f-4d1a-dbe9-5317019b35f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Epoch [1/10], batch: [1/6, loss: 11.6258]\n",
            "Epoch [1/10], batch: [2/6, loss: 11.5951]\n",
            "Epoch [1/10], batch: [3/6, loss: 11.5470]\n",
            "Epoch [1/10], batch: [4/6, loss: 11.6658]\n",
            "Epoch [1/10], batch: [5/6, loss: 11.5902]\n",
            "Epoch [1/10], batch: [6/6, loss: 11.7499]\n",
            "Epoch [2/10], batch: [1/6, loss: 11.4614]\n",
            "Epoch [2/10], batch: [2/6, loss: 11.3817]\n",
            "Epoch [2/10], batch: [3/6, loss: 11.3307]\n",
            "Epoch [2/10], batch: [4/6, loss: 11.4092]\n",
            "Epoch [2/10], batch: [5/6, loss: 11.3747]\n",
            "Epoch [2/10], batch: [6/6, loss: 11.4353]\n",
            "Epoch [3/10], batch: [1/6, loss: 11.1751]\n",
            "Epoch [3/10], batch: [2/6, loss: 10.9996]\n",
            "Epoch [3/10], batch: [3/6, loss: 10.9563]\n",
            "Epoch [3/10], batch: [4/6, loss: 10.9686]\n",
            "Epoch [3/10], batch: [5/6, loss: 11.0097]\n",
            "Epoch [3/10], batch: [6/6, loss: 10.9091]\n",
            "Epoch [4/10], batch: [1/6, loss: 10.7966]\n",
            "Epoch [4/10], batch: [2/6, loss: 10.4642]\n",
            "Epoch [4/10], batch: [3/6, loss: 10.4513]\n",
            "Epoch [4/10], batch: [4/6, loss: 10.3524]\n",
            "Epoch [4/10], batch: [5/6, loss: 10.5156]\n",
            "Epoch [4/10], batch: [6/6, loss: 10.1593]\n",
            "Epoch [5/10], batch: [1/6, loss: 10.3141]\n",
            "Epoch [5/10], batch: [2/6, loss: 9.7099]\n",
            "Epoch [5/10], batch: [3/6, loss: 9.7908]\n",
            "Epoch [5/10], batch: [4/6, loss: 9.4697]\n",
            "Epoch [5/10], batch: [5/6, loss: 9.8502]\n",
            "Epoch [5/10], batch: [6/6, loss: 9.0362]\n",
            "Epoch [6/10], batch: [1/6, loss: 9.6716]\n",
            "Epoch [6/10], batch: [2/6, loss: 8.5780]\n",
            "Epoch [6/10], batch: [3/6, loss: 8.8790]\n",
            "Epoch [6/10], batch: [4/6, loss: 8.1142]\n",
            "Epoch [6/10], batch: [5/6, loss: 8.9077]\n",
            "Epoch [6/10], batch: [6/6, loss: 7.2413]\n",
            "Epoch [7/10], batch: [1/6, loss: 8.7698]\n",
            "Epoch [7/10], batch: [2/6, loss: 6.8127]\n",
            "Epoch [7/10], batch: [3/6, loss: 7.5682]\n",
            "Epoch [7/10], batch: [4/6, loss: 6.1965]\n",
            "Epoch [7/10], batch: [5/6, loss: 7.6053]\n",
            "Epoch [7/10], batch: [6/6, loss: 5.2112]\n",
            "Epoch [8/10], batch: [1/6, loss: 7.6772]\n",
            "Epoch [8/10], batch: [2/6, loss: 5.0543]\n",
            "Epoch [8/10], batch: [3/6, loss: 6.1099]\n",
            "Epoch [8/10], batch: [4/6, loss: 4.7843]\n",
            "Epoch [8/10], batch: [5/6, loss: 6.2389]\n",
            "Epoch [8/10], batch: [6/6, loss: 3.7032]\n",
            "Epoch [9/10], batch: [1/6, loss: 6.8666]\n",
            "Epoch [9/10], batch: [2/6, loss: 3.8389]\n",
            "Epoch [9/10], batch: [3/6, loss: 4.3366]\n",
            "Epoch [9/10], batch: [4/6, loss: 3.5896]\n",
            "Epoch [9/10], batch: [5/6, loss: 4.6023]\n",
            "Epoch [9/10], batch: [6/6, loss: 1.4551]\n",
            "Epoch [10/10], batch: [1/6, loss: 5.1434]\n",
            "Epoch [10/10], batch: [2/6, loss: 2.1957]\n",
            "Epoch [10/10], batch: [3/6, loss: 2.5574]\n",
            "Epoch [10/10], batch: [4/6, loss: 2.2145]\n",
            "Epoch [10/10], batch: [5/6, loss: 2.8774]\n",
            "Epoch [10/10], batch: [6/6, loss: 0.1769]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_closest_word('alien')"
      ],
      "metadata": {
        "id": "9IZ4t24G1_Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0040a7b5-51b2-44f1-f81f-45e6a8df3524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('soory', 6.2498250007629395),\n",
              " ('destandard', 6.413937091827393),\n",
              " ('gels', 6.474977493286133),\n",
              " ('philosophically', 6.499139785766602),\n",
              " ('jcould', 6.626455307006836)]"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q-N9jEOW9pJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sci_words = ['']"
      ],
      "metadata": {
        "id": "PRhW1MSU_byy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Repeat 2.1 for SciFi Dataset\n",
        "\n",
        "ðŸ—’â“ List your findings for SciFi Dataset as well, similarly to 2.1"
      ],
      "metadata": {
        "id": "njWN32I2ADn9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3Uyn_VpAWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 ðŸ—’â“ How does the quality of the hotel review-based embeddings compare with the Sci-fi-based embeddings? Elaborate."
      ],
      "metadata": {
        "id": "E2yqWM5oAhzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oQrywIKgA1Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Choose 2 words and retrieve their 5 closest neighbours according to hotel review-based embeddings and the Sci-fi-based embeddings.\n",
        "\n",
        "ðŸ—’â“ Do they have different neighbours? If yes, can you reason why?"
      ],
      "metadata": {
        "id": "Ivwn8YCgA4GX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4J_xUKzgA3gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 ðŸ—’â“ What are the differences between CBOW2 and CBOW5 ? Can you \"describe\" them?    "
      ],
      "metadata": {
        "id": "DD09lU02Cw7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L9SZlUPM4mZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report\n",
        "The lab report should contain a detailed description of the approaches you have used to solve this exercise. Please also include results.\n",
        "\n",
        "Answers for the questions marked ðŸ—’â“ goes here as well"
      ],
      "metadata": {
        "id": "RdgWLLHJC24h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eLWqDG2GFV69"
      }
    }
  ]
}